{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6c19ac-423f-4e03-9453-b74de83ebccb",
   "metadata": {},
   "source": [
    "\n",
    "# Processing and analyzing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd9f12-1cd0-48ca-b439-686e151137f5",
   "metadata": {},
   "source": [
    "This notebook provides an introduction for analyzing urban data. It will cover ...\n",
    "\n",
    " - Exploring, filtering, and sorting data\n",
    " - Cleaning data and removing missing data\n",
    " - Creating new columns from existing data\n",
    " - Joining data from multiple tables\n",
    " - Computing descriptive statistics (sum, mean, etc.)\n",
    " - Aggregating data via cross-tabulations and pivot tables\n",
    "\n",
    "To do this, we'll primarily be using [pandas](https://pandas.pydata.org/), a Python library for analyzing and organizing tabular data. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069cb15e",
   "metadata": {},
   "source": [
    "When using external libraries like `pandas`, we need to *install* the library before we can *import* and then use it. In Jupyter Notebook, we can do this by running `!pip install [name of library]`.\n",
    "\n",
    "If you don't already have `pip` installed on your computer, follow [these instructions](https://pip.pypa.io/en/stable/installation/). If you're still having trouble, try Googling your specific questions or asking a chatbot for step-by-step instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23431636",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db44c14",
   "metadata": {},
   "source": [
    "Next, let's import the `pandas` library using the `pd` alias. An *alias* in Python is an alternate name for a library that's typically shorter and easier to reference in the code later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1123b26f-d663-411d-a5e6-aa6e6433a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a12f05f",
   "metadata": {},
   "source": [
    "`pandas` is probably the most common library for working with both big and small datasets in Python, and is the basis for working with more analytical packages (e.g. `numpy`, `scipy`, `scikit-learn`) and analyzing geographic data (e.g. `geopandas`). For each section, we'll also link to relevant documentation for doing similar tasks in Microsoft Excel and Google Sheets.\n",
    "\n",
    "Here are download links to this notebook and example datasets.\n",
    "\n",
    "- <a href=\"data-analytics-and-processing.ipynb\" download>Download Notebook</a>  \n",
    "- <a href=\"data/cities.csv\" download>cities.csv</a>  \n",
    "- <a href=\"data/cities.csv\" download>capitals.csv</a>  \n",
    "- <a href=\"data/cities.csv\" download>new_york_cities.csv</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8de5d-68cf-440f-842c-7126d22dcf47",
   "metadata": {},
   "source": [
    "## Tables & DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879c7e2-7e2a-4874-a051-485635b8a5b5",
   "metadata": {},
   "source": [
    "A very common way of structuring and analyzing a dataset is in a 2-dimensional table format, where rows are individual records or observations, while columns are attributes (often called variables) about those observations. For example, rows could each be a city and columns could indicate the population for different time periods. Data stored in spreadsheets often take this format.\n",
    "\n",
    "In `pandas`, a `DataFrame` is a tabular data structure similar to a spreadsheet, where data is organized in rows and columns. Columns can contain different kinds of data, such as numbers, strings, dates, and so on. When we load data in `pandas`, we typically load it into the structure of a `DataFrame`.\n",
    "\n",
    "Let's first take a look at a small dataset, Canadian municipalities and their population in 2021 and 2016, based on Census data. In Statistics Canada lingo, these are called [Census Subdivisions](https://www12.statcan.gc.ca/census-recensement/2021/ref/dict/az/Definition-eng.cfm?ID=geo012). This dataset only includes municipalities with a population greater than 25,000 in 2021.\n",
    "\n",
    "The main method for loading csv data is to use the [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function, but pandas can also read and write [many other](https://pandas.pydata.org/pandas-docs/stable/reference/io.html) data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9113af00-ba82-4adc-83d5-812caf09d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d44668-ce3e-46c0-a360-67370e3b2d61",
   "metadata": {},
   "source": [
    "Great! Now our data is stored in the variable `df` in the structure of a `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78884e-83d4-420d-b493-96698da5e54c",
   "metadata": {},
   "source": [
    "## Viewing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57fe6c-44d5-470e-8148-316542c82d80",
   "metadata": {},
   "source": [
    "In spreadsheet software, when we open a data file, we will see the top rows of the data right away.\n",
    "\n",
    "In `pandas`, we can simply type the name of the `DataFrame`, in this case `df`, in the cell to view it. By default, it will print the top and bottom rows in the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ce3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f837ec4",
   "metadata": {},
   "source": [
    "We can specify which rows we want to view.\n",
    "\n",
    "Let's explore what this data frame looks like. Adding the function `.head(N)` or `.tail(N)` prints the top or bottom `N` rows of the DataFrame. The following prints the first 10 rows.\n",
    "\n",
    "**Try to edit this to print the bottom 10 rows or a different number of rows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1124a-1e2b-4bf1-a621-194704815820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7146c67-bb6c-4c83-86cf-64114cd14ef6",
   "metadata": {},
   "source": [
    "Notice that each column has a unique name. We can view the data of this column alone by using that name, and see what unique values exist using `.unique()`. \n",
    "\n",
    "**Try viewing the data of another column. Beware of upper and lower case -- exact names matter!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82ad67-e44e-4a8d-ae25-5d5bd73a11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Prov/terr'].head(10)  # Top 10 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75d9c6-ad34-463d-94dc-824d82ad52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Prov/terr'].unique()  # Unique values for the *full* dataset - what happens if you do df['Prov/terr'].head(10).unique()?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780687b7-6a07-4c82-ab4d-499ea7644955",
   "metadata": {},
   "source": [
    "## Filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4150f657-e5df-45fb-aeb2-4df917e92c87",
   "metadata": {},
   "source": [
    "We often want to look at only a portion of our data that fit some set of conditions (e.g. all cities in a province that have a population more than 100,000). This is often called filtering, querying, or subsetting a dataset.\n",
    "\n",
    "Let's try to do some filtering in `pandas` with our data of Canadian cities. Check out these links for filtering and sorting in spreadsheet software:\n",
    "\n",
    "- [Filtering in Excel](https://support.microsoft.com/en-us/office/filter-data-in-a-range-or-table-01832226-31b5-4568-8806-38c37dcc180e)\n",
    "- [Filtering in Google Sheets](https://support.google.com/docs/answer/3540681?hl=en&co=GENIE.Platform%3DDesktop)\n",
    "\n",
    "We can use the columns to identify data that we might want to filter by. The line below shows data only for Ontario, **but see if you can filter for another province or territory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d291a78-7d41-4e76-bddc-4f3c72484642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Prov/terr'] == 'Ont.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a6b762-3681-4fa4-84e6-af97f69a351e",
   "metadata": {},
   "source": [
    "Pandas allows us to use other similar mathematical concepts filter for data. Previously, we asked for all data in Ontario. \n",
    "\n",
    "**Now, filter for all cities which had a population of at least 100,000 in 2021**.\n",
    "\n",
    "HINT: in Python, \"greater than or equals to\" (i.e., \"at least\") is represented using the syntax `>=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2dabf8-a5fe-42c5-bd8b-a8c053564eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1271b2a-ca8a-4702-b317-c3f5faca1e8d",
   "metadata": {},
   "source": [
    "Pandas also allows us to combine filtering conditions. \n",
    "\n",
    "**Use the template below to select for all cities in Ontario with a population of over 100,000 in 2021**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d8d8d-56c3-41f2-bc0d-fb83d7dde541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62836a7c-939d-434d-85b0-97185599fdf5",
   "metadata": {},
   "source": [
    "Now let's count how many cities actually meet these conditions. Run the line below to see how many cities there are in this data set in Ontario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb688d2-4188-4fe9-ac30-88029e5efbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Prov/terr'] == 'Ont.'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01e04a-09fe-475d-a9fd-f94c7423a558",
   "metadata": {},
   "source": [
    "The function `.count()` tells us how much data there is for each column - but if we wanted to just see one column, we could also filter for that individual column using `df[COL_NAME]`. \n",
    "\n",
    "**Try a different condition and count the amount of data for it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ef3b2-01d5-4897-a7d4-dff404e39b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c172cfc",
   "metadata": {},
   "source": [
    "## Sorting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93f7a4-0f72-455b-81fe-9771f441a21f",
   "metadata": {},
   "source": [
    "You might have noticed that these cities are in alphabetical order - what if we wanted to see them in the order of population? In pandas, we do this using the [sort_values](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) function. The default is to sort in ascending order, so we set this to be `False` (i.e. descending) so the most populous cities are at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287a54b-b162-4fd5-9b84-7100f148e2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(by='Population, 2021', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced26d8-96d8-4696-b036-695cb30fa4b6",
   "metadata": {},
   "source": [
    "Let's put some in this together now. \n",
    "\n",
    "**Filter the data to show all cities which are in the province of Quebec with at least a population of 50,000 in 2016, and then try to sort the cities by their 2016 population**.\n",
    "\n",
    "HINT: You can do this in two steps (which is more readable) by storing the data that you filter into a variable called `df_filtered`, then running the command to sort the values on `df_filtered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478284c0-0d8e-4c32-b640-2997bef67b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c55ef5b",
   "metadata": {},
   "source": [
    "## Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a6d1c",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "Once we have processed and analyzed our data, we may want to save it for future use or share it with others. `pandas` makes it easy to export data to various formats, such as CSV or Excel files.\n",
    "\n",
    "Below, we demonstrate how to export a `DataFrame` to both CSV and Excel formats. This is particularly useful for sharing results or viewing the data in other tools like spreadsheet software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df_filtered.to_csv('df_filtered.csv', index=False)\n",
    "\n",
    "# Save to Excel\n",
    "df_filtered.to_excel('df_filtered.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209adfa-e756-43e7-81c6-62ea737f7e75",
   "metadata": {},
   "source": [
    "## Updating and renaming columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518fa8c-e311-40c9-8a32-591b861bf70e",
   "metadata": {},
   "source": [
    "Often, the data we have might not be in the condition want it to be in. Some data might be missing, and other data might have odd naming conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52910d02-be57-4224-b4d1-83db8f434c7c",
   "metadata": {},
   "source": [
    "A simple example is that we might want all city names to be lowercase - which is what the code below does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a6d07-f1c7-4967-98d8-89838a4801c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name'] = df['Name'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba8963-ae7f-4789-b6af-f04b2a67e733",
   "metadata": {},
   "source": [
    "`pandas` has a number of methods like `str.lower()` to alter data (see the [full API](https://pandas.pydata.org/docs/reference/index.html)). But the important thing to note here is that we directly modified the existing values of a column. We might not always want to do this, but often it is a good way of saving memory and shows that data frames are not just static forms but modifiable.\n",
    "\n",
    "Likewise, we might want better names for the columns we have. **Take a look at the [API for `.rename()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) and modify the data frame `df` such that we rename the column `Name` to `City`**. Pandas has more methods than we could ever remember - so learning to navigate the API is a crucial part of using the library.\n",
    "\n",
    "*HINT: Take a look at the first example*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48bde4-b516-4031-83f6-f2faecc0e366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "855b3ed6-8a66-44e7-a19b-d261dce9287e",
   "metadata": {},
   "source": [
    "## Handling missing data\n",
    "\n",
    "Unfortunately, it is pretty common that dataset we work with will be missing data values for some rows and columns. This can create complications when we want to produce summary statistics or visualizations. There are different strategies for dealing with this (i.e., imputing data), but the easiest is just to remove them. But first come out let's check how much data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107e955-fdb9-458e-a4a2-ed614b8f9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5cb78-cd3b-4703-aa65-ba5cb80133b3",
   "metadata": {},
   "source": [
    "It seems that each column has a couple of data points missing. Let's take a look at which rows these occur in. Similar to how we created a condition to filter for certain data, the code below creates a condition to filter for rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e69d4-22ce-47b6-9f8c-8f983e1e4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaaa12e-f456-4f4a-b6d9-99c1ce709daf",
   "metadata": {},
   "source": [
    "You can see that some rows are missing multiple values, While others are just missing one. We can remove rows which have missing data using the function [dropna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) and assign it to `df` so we're working with complete data only going forward. **Try to modify the code below to drop rows whose empty values are in one of the two population columns - that is, if the name or province is missing, we want to keep that row still**. Look at the API to figure this out, specifically the argument `subset` for `.dropna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cd644-4e56-4c2c-8479-68de04c28a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e063a2-bbbe-4ad2-86c0-558c42e36145",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbcc21-4498-4deb-8ca7-186feb069870",
   "metadata": {},
   "source": [
    "Great. Now let's reset to our original data frame and exclude any rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9e83d-59b2-47f9-ae25-56cc4632732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cities.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccd8ae",
   "metadata": {},
   "source": [
    "Instead of dropping (i.e. removing) rows with missing values, we can instead replace them with specific values with [fillna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html). For example, `df.fillna(0)` would replace all missing data values with a `0`. This wouldn't make sense in our data of Canadian cities, but can be useful in other contexts.\n",
    "\n",
    "Similarly we can promgramatically find and replace data in any other column or across our dataset. For example, if we wanted to rename `'Y.T.'` to `'Yukon'` we would run the [replace](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html#pandas.DataFrame.replace) function as so `df = df.replace('Y.T.', 'Yukon')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c60a6-4ead-428f-af0e-c292cafb5fd3",
   "metadata": {},
   "source": [
    "### Creating new columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63928bd6-400c-46ca-962d-4c9a3f71206c",
   "metadata": {},
   "source": [
    "We can add or delete columns as needed. Let's first add a column which shows the change in population between 2021 and 2016 and then sort by the cities that lost the most people. We can calculate that via a simple subtraction as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7bffb-dc82-49bb-b10d-a21382d192d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pop_change\"] = df[\"Population, 2021\"] - df[\"Population, 2016\"]\n",
    "df.sort_values(\"pop_change\", ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc6576-c636-42d6-9863-228ae2e97c84",
   "metadata": {},
   "source": [
    "Pandas supports mathematical equations between columns, just like the subtraction we did above. **Create a new column called `pct_pop_change` that computes the percentage change in population between 2016 and 2021, and sort by the cities with the greatest increase**.\n",
    "\n",
    "HINT: the way to compute percent change is `100 * (Y - X) / X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9074c1d-aac6-4eb5-9449-127c2210723d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71fc401d-1689-4678-9d2a-045efb9922a7",
   "metadata": {},
   "source": [
    "Now let's clear these new columns out using [drop](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) to return to what we had originally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['pop_change', 'pct_pop_change'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150d4ce-4546-4494-9364-3caaf5e1425b",
   "metadata": {},
   "source": [
    "### Concatenating and joining tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dbe24b-9d75-4cbb-b6d1-14c6db22c80c",
   "metadata": {},
   "source": [
    "Much of the time, we are working with multiple sets of data which may overlap in key ways. Perhaps we want to include measures of income in cities, or look at voting patterns - this may require us to combine multiple data frames so we can analyze them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574b291-dae6-40d3-9825-11ced56e5a1d",
   "metadata": {},
   "source": [
    "Pandas methods to combine data frames are quite similar to that of database operations - if you've worked with SQL before, this will come very easily to you. There's an [extensive tutorial](https://pandas.pydata.org/docs/user_guide/merging.html#concat) on this topic, but we'll focus on simple cases of `pd.concat()` and `pd.merge()`. If you are curious about how to do this in spreadsheet software like [Excel, check out this tutorial](https://www.exceldemy.com/merge-two-tables-in-excel/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b531b-30a2-4a1c-8728-c59692367478",
   "metadata": {},
   "source": [
    "First, we can use `pd.concat()` to stack DataFrames vertically when new data has the same columns but additional rows (e.g., adding cities from another region). This is like adding new entries to a database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb34fc-71a8-4f27-a4e4-8997d6fd7891",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ny_cities = pd.read_csv(\"./data/new_york_cities.csv\")\n",
    "combined = pd.concat([df, df_ny_cities], ignore_index=True)\n",
    "print(\"Original rows:\", len(df), \"| After append:\", len(combined))\n",
    "display(combined.tail(5))  # Show new rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129299d-7e7b-4a10-8c6c-50e0987a94e9",
   "metadata": {},
   "source": [
    "Second, we can use `pd_merge()` (or `pd.concat(axis=1)`) to combine DataFrames side-by-side when they share a key column (e.g., city names). Here, we’ll a column denoting whether the city is a provincial capital by matching city names.\n",
    "\n",
    "Let's first load this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f7305-2d5e-4a31-9d64-7b47819a23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_capitals = pd.read_csv('./data/capitals.csv')\n",
    "df_capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3047f02-f5e4-40a7-bf52-f9a4e3ecc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_capitals = pd.merge(\n",
    "    df,  # Original data\n",
    "    df_capitals,  # New data\n",
    "    on=\"Name\",  # The same column \n",
    "    how=\"left\"  # Keep all data from the \"left\", ie. original\n",
    ")\n",
    "\n",
    "# Set non-capitals to False\n",
    "df_with_capitals[\"Is_Capital\"] = df_with_capitals[\"Is_Capital\"].astype('boolean').fillna(False)\n",
    "\n",
    "# Verify: Show capitals and counts\n",
    "print(f\"Found {df_with_capitals['Is_Capital'].sum()} capitals:\")\n",
    "df_with_capitals[df_with_capitals[\"Is_Capital\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5127c9-6ca9-4945-a70c-abd8e92cb25c",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803d353-b8fc-43de-9980-aa8de8ff510e",
   "metadata": {},
   "source": [
    "The data we have is only as good as we understand what's going on. There's some basic methods in `pandas` we can use to get an idea of what the data says.\n",
    "\n",
    "The most basic function you can use to get an idea of what's going on is `.describe()`. It shows you how much data there is, and a number of summary statistics. \n",
    "\n",
    "**Modify the code below to report summary statistics for cities in Quebec only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93bcfd5-c554-4297-9e03-45dee67e0c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6fc5b-e7c8-4b0d-afc3-2f3f9f567662",
   "metadata": {},
   "source": [
    "Instead of picking out an examining a subset of the data one by one, we can use the function `.groupby()`. Given a column name, it will group rows which have the same value. In the example below, that means grouping every row which has the same province name. We can then apply a function to this (or multiple functions using `.agg()`) to examine different aspects of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c52919-3d19-4143-b50e-d95966ddae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by province and calculate total population\n",
    "province_pop = df.groupby('Prov/terr')['Population, 2021'].sum()\n",
    "print(\"Total population by province:\")\n",
    "province_pop.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ef590-48e6-413f-9009-d9ae201fa67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregation statistics\n",
    "stats = df.groupby('Prov/terr')['Population, 2021'].agg(['count', 'mean', 'max', 'sum'])\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235c7f5-85a4-4563-9be0-8f1a8fe3da99",
   "metadata": {},
   "source": [
    "Below, we've added a column which shows the percent growth for each city. **Use `.groupby('Prov/terr')`  and use the function `.median()` (just as we used `.sum()` above) to observe the median growth rate per province or territory**. Make sure to use `sort_values()` and set ascending to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafec74a-ffa8-4c19-8e85-a378f7195cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Percent_Growth'] = 100 * (df['Population, 2021'] - df['Population, 2016']) / df['Population, 2016'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238e17f-59b3-444c-94ed-ede0336feaf8",
   "metadata": {},
   "source": [
    "## Cross tabulations and pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3b575",
   "metadata": {},
   "source": [
    "A *cross tabulation*, also called a *frequency table* or a *contingency table*, is a table that shows the summarizes two categorical variables by displaying the number of occurrences in each pair of categories.\n",
    "\n",
    "Let's show an example by counting the number of cities in each province by a categorization of city size.\n",
    "\n",
    "We will need two tools to do this:\n",
    " - [cut](https://pandas.pydata.org/docs/reference/api/pandas.cut.html), which bins continuous numbers (like population) into categories (e.g., \"Small\"/\"Medium\"/\"Large\"), turning numbers into meaningful groups.\n",
    " - [crosstab](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html), which counts how often these groups appear together—like how many \"Medium\" cities exist per province—revealing patterns that raw numbers hide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f378e8-5684-4f4b-9a65-6b2ca7e38e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a size category column\n",
    "df['Size'] = pd.cut(df['Population, 2021'],\n",
    "                    bins=[0, 50000, 200000, float('inf')],\n",
    "                    labels=['Small', 'Medium', 'Large'])\n",
    "\n",
    "# Cross-tab: Province vs. Size\n",
    "size_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\n",
    "size_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f84a1-1726-4a63-b558-997a060f594d",
   "metadata": {},
   "source": [
    "Recall that we just created the column `'Percent_Growth'` as well. **Use these two functions to create different bins for different levels of growth and cross tabulate them**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d2709-9f86-4c1f-9e47-095788b59a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e9152af",
   "metadata": {},
   "source": [
    "If you've worked with Excel or Google Sheets, this is very similar to doing a [Pivot Table](https://support.microsoft.com/en-us/office/create-a-pivottable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576). \n",
    "\n",
    "Pivot tables are able to summarize data across several categories, and for different types of summaries (e.g. `sum`, `mean`, `median`, etc.)\n",
    "\n",
    "The [pivot_table](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html) function in `pandas` to aggregate data, and has more options than the [crosstab](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html) function. Both are quite similar though. Here's an example where we are using `pivot_table` to sum the population in each province by the 3 size groups. \n",
    "\n",
    "**Try to edit this to compute the mean or median city `Percent_Growth`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ed39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(data = df, values = ['Population, 2021'], index = ['Prov/terr'], columns = ['Size'], aggfunc = 'sum', observed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3d65b",
   "metadata": {},
   "source": [
    "There are often multiple ways to achieve similar results. In the previous section we looked at the `groupby` function to summarize data by one column, while above we used `crosstab` or `pivot_table`. However, we could also use the `groupby` function for this purpose. Check out the example below. \n",
    "\n",
    "You should notice that it has created a long-table formate rather than a wide-table format. Both formats can be useful, wide-table formats are easier for viewing data for only 2 categories, while long-table formats are often used for inputting data into modelling or visualization libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Prov/terr', 'Size'], observed=False)['Population, 2021'].agg(['sum'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
