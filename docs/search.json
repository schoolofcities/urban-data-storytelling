[
  {
    "objectID": "outline-example.html",
    "href": "outline-example.html",
    "title": "<img src='https://raw.githubusercontent.com/schoolofcities/gta-immigration/refs/heads/main/src/assets/top-logo-full.svg' style='height:auto;width:250px;margin-left: -20px;'></img><br>Urban Data Storytelling üìäüìàüèôÔ∏è",
    "section": "",
    "text": "Data storytelling (V1, V2)\nAssessment: - Describe your goals, stakeholders, narrative ideas, list of potential data, etc.\nMandatory readings: - intro to data storytelling (video) - intro to urban data (video and notebook) - measuring the city (notebook) - data ethics / literacy (videos) Optional readings (if they want a background in coding) - intro to python and jupyter (notebook)\nData analysis (V3)\nAssessment: - List each dataset you have downloaded/have access to, and briefly describe why it‚Äôs important for your work - Analyze and explore your data to provide at least 3 summaries that super important to your research and story. Describe what the data tells you. These can include, but are not limited to,summary statistics (means, percents, etc.), summary tables (e.g.¬†cross-tabs, pivot tables, etc.), correlation statistics or summary trend from a regression model, or other results from a statistical analysis\nMandatory readings: - data analysis and processing (notebook) - spatial data and gis (notebook) Optional readings (pick and choose depending on which relate to your project): - statistics fundementals (notebook) - intro to census data (notebook) - intro to openstreetmap data (notebook) - REV - spatial data in python (notebook) - spatial data processing (notebook)\nData visualization (V4, V5)\nAssessment: - Create at least 3 maps and/or charts of your data that help communicate patterns and help tell your story. Try to make at least 1 non-spatial chart and at least 1 map/spatial data visualization.\nMandatory readings: - data visualization (notebook) - maps and spatial data visualization (notebook) Optional readings (pick and choose depending on which relate to your project): - exploratory data viz in Python (notebook) - choropleth maps (notebook) - proportional symbol maps (notebook) - mapping density (notebook) (might add 1 or 2 more in this section depending on time)\nTo do? - stats page - list other examples of hypoth tests? - excel/sheets data viz without code - density mapping - multiviarate methods - overview of what, with links?"
  },
  {
    "objectID": "urban-data-analytics/intro-geopandas/intro-geopandas.html#loading-exploring-and-geometric-data-types",
    "href": "urban-data-analytics/intro-geopandas/intro-geopandas.html#loading-exploring-and-geometric-data-types",
    "title": "Introduction to Geopandas",
    "section": "Loading, Exploring, and Geometric Data Types",
    "text": "Loading, Exploring, and Geometric Data Types\nGeospatial data represents real-world features using three primary geometric types: - Points: Single (x,y) coordinates for discrete locations like transit stops or landmarks. - Lines: Connected sequences of points forming paths, such as roads or rivers. - Polygons: Closed shapes defining areas like census tracts or property boundaries.\n\ntransit_stops = gpd.read_file(\"data/ttc_stops.geojson\")\ntransit_routes = gpd.read_file(\"data/ttc_routes.geojson\")\ncensus_tracts = gpd.read_file(\"data/toronto_census_tract_2021.shp\")\ncensus_data = pd.read_csv(\"data/census_tract_data_sample.csv\")\n\nIn geopandas, we typically load data with a more agnostic read_file() function. For this workshop, we‚Äôre going to use four sources of data: - transit_stops: each of the stops for the TTC - transit_routes: each of the lines for the TTC - census_tracts: The geometry of Canadian census tracts within Toronto - census_data: the data of Canadian census tracts within Toronto\nLet‚Äôs take a look at the first layer, the transit stops. We have two columns with text, and a third with geometry data.\n\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nNAME\ngeometry\n\n\n\n\n0\nAvenue\nEglinton Crosstown LRT\nPOINT (-79.40851 43.70460)\n\n\n1\nForest Hill\nEglinton Crosstown LRT\nPOINT (-79.42556 43.70102)\n\n\n2\nLeaside\nEglinton Crosstown LRT\nPOINT (-79.37715 43.71105)\n\n\n3\nSloane\nEglinton Crosstown LRT\nPOINT (-79.31352 43.72597)\n\n\n4\nBirchmount\nEglinton Crosstown LRT\nPOINT (-79.27791 43.73006)\n\n\n...\n...\n...\n...\n\n\n101\nVaughan Metropolitan Centre\nToronto-York Spadina Subway Extension\nPOINT (-79.52727 43.79351)\n\n\n102\nSheppard-Yonge\nSheppard Subway\nPOINT (-79.41092 43.76151)\n\n\n103\nSpadina\nBloor-Danforth Subway\nPOINT (-79.40397 43.66728)\n\n\n104\nSt. George\nBloor-Danforth Subway\nPOINT (-79.39930 43.66827)\n\n\n105\nBloor-Yonge\nBloor-Danforth Subway\nPOINT (-79.38572 43.67100)\n\n\n\n\n106 rows √ó 3 columns\n\n\n\nThe data can be manipulated like a regular pandas data frame. For example, if we want to filter out all stations on the ‚ÄúEglinton Crosstown LRT‚Äù line, since it hasn‚Äôt been completed yet (at the time of writing), we can do so as follows:\n\ntransit_stops = transit_stops.loc[transit_stops[\"NAME\"] != \"Eglinton Crosstown LRT\"]\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nNAME\ngeometry\n\n\n\n\n25\nKipling\nBloor-Danforth Subway\nPOINT (-79.53628 43.63694)\n\n\n26\nIslington\nBloor-Danforth Subway\nPOINT (-79.52459 43.64532)\n\n\n27\nRoyal York\nBloor-Danforth Subway\nPOINT (-79.51129 43.64811)\n\n\n28\nOld Mill\nBloor-Danforth Subway\nPOINT (-79.49509 43.65007)\n\n\n29\nJane\nBloor-Danforth Subway\nPOINT (-79.48446 43.64979)\n\n\n...\n...\n...\n...\n\n\n101\nVaughan Metropolitan Centre\nToronto-York Spadina Subway Extension\nPOINT (-79.52727 43.79351)\n\n\n102\nSheppard-Yonge\nSheppard Subway\nPOINT (-79.41092 43.76151)\n\n\n103\nSpadina\nBloor-Danforth Subway\nPOINT (-79.40397 43.66728)\n\n\n104\nSt. George\nBloor-Danforth Subway\nPOINT (-79.39930 43.66827)\n\n\n105\nBloor-Yonge\nBloor-Danforth Subway\nPOINT (-79.38572 43.67100)\n\n\n\n\n81 rows √ó 3 columns\n\n\n\nWe can do the same for the transit lines data. Here the data are coded as a MULTILINESTRING, essentially a combination of lines that combine into one object. There are also MULTIPOLYGON geometry types\n\ntransit_routes = transit_routes.loc[transit_routes[\"NAME\"] != \"Eglinton Crosstown LRT\"]\ntransit_routes\n\n\n\n\n\n\n\n\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nExisting\nSubway\nSheppard Subway\nMULTILINESTRING ((-79.41092 43.76151, -79.4096...\n\n\n1\nExisting\nSubway\nYonge-University-Spadina Subway\nMULTILINESTRING ((-79.46247 43.75043, -79.4621...\n\n\n2\nExisting\nSubway\nSpadina Subway Extension\nMULTILINESTRING ((-79.52727 43.79351, -79.5261...\n\n\n4\nExisting\nSubway\nScarborough RT\nMULTILINESTRING ((-79.26453 43.73226, -79.2632...\n\n\n5\nExisting\nSubway\nBloor Subway\nMULTILINESTRING ((-79.26453 43.73226, -79.2669...\n\n\n\n\n\n\n\nBefore we go on, it‚Äôs important to have an idea of the metadata of geometric files that we work with. There‚Äôs two key parts to this. - CRS (Coordinate Reference System): The crs attribute defines a geodataset‚Äôs spatial ‚Äúcoordinate system‚Äù (e.g., latitude/longitude, meters-based projections). We can use it to ensure layers align‚Äîfor example, combining Toronto census tracts (EPSG:3347) with a Web Mercator basemap (EPSG:3857). - Total Bounds: The total_bounds attribute returns the min/max coordinates (xmin, ymin, xmax, ymax) of your data‚Äôs extent. It‚Äôs useful for setting map zoom levels or clipping other datasets to the same area‚Äîlike focusing a transit map on Toronto‚Äôs downtown core.\n\ntransit_stops.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ntransit_stops.total_bounds\n\narray([-79.53627668,  43.63693527, -79.25160004,  43.79350523])"
  },
  {
    "objectID": "urban-data-analytics/intro-geopandas/intro-geopandas.html#basic-static-plots",
    "href": "urban-data-analytics/intro-geopandas/intro-geopandas.html#basic-static-plots",
    "title": "Introduction to Geopandas",
    "section": "Basic Static Plots",
    "text": "Basic Static Plots\nWe explore geometry simply by plotting using .plot(). We can do this for any row, or the entire GeoDataFrame\n\ntransit_stops.plot()\n\n\n\n\n\n\n\n\nThis is the default plot, but we can tweak the colours, add multiple layers, and change some of the layout options using matplotlib, probably the most commonly used map. Here‚Äôs a very simple schematic of rapid transit in Toronto (circa 2021)\n\nfig, ax = plt.subplots(ncols = 1, figsize=(3, 3))\n\ntransit_stops.plot(\n    color=\"Black\",\n    markersize = 6,\n    ax = ax\n)\n\ntransit_routes.plot(\n    linewidth = 1,\n    color=\"Black\",\n    ax = ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nLet‚Äôs take a look at the census tract data. Here we have polygon geometries.\n\ncensus_tracts\n\n\n\n\n\n\n\n\nid\nctuid\ndguid\nctname\nlandarea\npruid\ngeometry\n\n\n\n\n0\n487\n5350128.04\n2021S05075350128.04\n0128.04\n0.1620\n35\nPOLYGON ((629437.750 4839364.950, 629247.561 4...\n\n\n1\n502\n5350363.06\n2021S05075350363.06\n0363.06\n0.8210\n35\nPOLYGON ((640741.738 4848050.419, 640723.345 4...\n\n\n2\n506\n5350363.07\n2021S05075350363.07\n0363.07\n2.2422\n35\nPOLYGON ((642782.718 4849973.938, 642781.180 4...\n\n\n3\n508\n5350378.23\n2021S05075350378.23\n0378.23\n1.5314\n35\nPOLYGON ((639248.900 4849901.332, 639248.900 4...\n\n\n4\n509\n5350378.24\n2021S05075350378.24\n0378.24\n2.5129\n35\nPOLYGON ((639952.255 4850407.204, 639952.255 4...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n580\n5861\n5350210.04\n2021S05075350210.04\n0210.04\n0.4751\n35\nMULTIPOLYGON (((623047.314 4831182.748, 623047...\n\n\n581\n5862\n5350062.03\n2021S05075350062.03\n0062.03\n0.4638\n35\nPOLYGON ((629776.795 4835352.843, 629766.377 4...\n\n\n582\n5863\n5350062.04\n2021S05075350062.04\n0062.04\n0.1215\n35\nPOLYGON ((630319.668 4835517.832, 630149.660 4...\n\n\n583\n5864\n5350017.01\n2021S05075350017.01\n0017.01\n0.8026\n35\nPOLYGON ((633075.947 4834744.346, 633089.159 4...\n\n\n584\n5865\n5350017.02\n2021S05075350017.02\n0017.02\n0.5681\n35\nPOLYGON ((631852.697 4833570.180, 631843.913 4...\n\n\n\n\n585 rows √ó 7 columns\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\ncensus_tracts.plot(\n    edgecolor=\"Black\",\n    color=\"White\",\n    linewidth=0.5,\n    ax = ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nLet‚Äôs try to make a choropleth map. We‚Äôll have to join in the tabular data in census_data\n\ncensus_tracts_data = census_tracts.merge(census_data, how='left', on='ctuid')\n\n\ncensus_tracts_data[\"population density\"] = census_tracts_data[\"population_2021\"] / census_tracts_data[\"landarea\"]\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\ncensus_tracts_data.to_crs(4326).plot(\n    column = \"population density\",\n    edgecolor=\"White\",\n    cmap = 'YlOrRd', \n    k = 7,\n    scheme = \"Quantiles\", \n    linewidth=0.5,\n    legend = True,\n    zorder=1,\n    legend_kwds = {\n        \"loc\": \"lower right\",\n        \"fontsize\": 7,\n        \"title\": \"Population Density\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    },\n    ax=ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nWe can choose a different variable and make a map of median income. Notice that we‚Äôve also switched the color parameter cmap; just as in matplotlib generally, we can style the map as we please.\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\ncensus_tracts_data.plot(\n    column = \"median_aftertax_hhld_income_2020\",\n    edgecolor=\"Black\",\n    cmap = 'coolwarm_r', \n    k = 4,\n    scheme = \"Quantiles\", \n    linewidth=0.5,\n    legend = True,\n    legend_kwds = {\n        \"loc\": \"lower right\",\n        \"fontsize\": 7,\n        \"title\": \"Median Income\\n(after tax)\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    },\n    ax=ax\n).set_axis_off()"
  },
  {
    "objectID": "urban-data-analytics/intro-geopandas/intro-geopandas.html#interactive-exploration",
    "href": "urban-data-analytics/intro-geopandas/intro-geopandas.html#interactive-exploration",
    "title": "Introduction to Geopandas",
    "section": "Interactive Exploration",
    "text": "Interactive Exploration\nGeoPandas‚Äô explore() function generates an interactive Leaflet map (like Google Maps) from your geodata. We can use it to better understand the data we are working with and how it might be viewed from the user side on a web application (e.g., Svelte).\nYou‚Äôll need to install a couple libraries in order for this to work - matplotlib, folium, and mapclassify. This can be done in the environment that you‚Äôre working in with a command like pip install folium matplotlib mapclassify.\n\ntransit_stops.explore(\n    column='LOCATION_N',  # Popup labels\n    tiles=\"CartoDB Positron\", \n    marker_kwds={\"radius\": 5}\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "urban-data-analytics/intro-geopandas/intro-geopandas.html#layering-multiple-datasets",
    "href": "urban-data-analytics/intro-geopandas/intro-geopandas.html#layering-multiple-datasets",
    "title": "Introduction to Geopandas",
    "section": "Layering Multiple Datasets",
    "text": "Layering Multiple Datasets\nLayering lets you combine different geographic datasets (like roads on top of neighborhoods) to reveal spatial relationships. In GeoPandas, each plot() call adds a new visual layer, with zorder controlling which appears on top. These plots are highly customizable, as we see below.\n\n# Initialize the canvas (all layers will share this axis)\nfig, ax = plt.subplots(figsize=(7, 7))  \n\n# Layer 1: Transit routes (black lines on top, zorder=3)\ntransit_routes.to_crs(4326).plot(\n    linewidth=1,        \n    color=\"Black\",      \n    ax=ax,              # Draw on our shared axis\n    zorder=3            # Top layer (overlaps others)\n)\n\n# Layer 2: Transit stops (black dots below routes, zorder=2)\ntransit_stops.to_crs(4326).plot(\n    color=\"Black\",      \n    markersize=16,      \n    ax=ax,              # Same shared axis\n    zorder=2            # Middle layer (above tracts)\n)\n\n# Layer 3: Census tracts (colored by density, zorder=1)\ncensus_tracts_data.to_crs(4326).plot(\n    column=\"population density\",  \n    edgecolor=\"White\",            \n    cmap='YlOrRd',                \n    k=7,                          \n    scheme=\"Quantiles\",           \n    linewidth=0.5,                \n    legend=True,                  \n    zorder=1,                     # Bottom layer\n    legend_kwds={\n        \"loc\": \"lower right\",     \n        \"fontsize\": 7,            \n        \"title\": \"Population Density\",  \n        \"alignment\": \"left\",      \n        \"reverse\": True           \n    },\n    ax=ax\n).set_axis_off()  # Hide distracting x/y axes"
  },
  {
    "objectID": "urban-data-analytics/measuring-the-city/measuring-the-city.html",
    "href": "urban-data-analytics/measuring-the-city/measuring-the-city.html",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "Karen Chapple, Julia Greenberg, Jeff Allen\nThis page covers 1) common metrics and indicators used in urban analysis, 2) spatial units and measures of aggregation often used for urban analysis, and 3) limitations and biases for working with urban datasets to keep in mind when working with urban data.",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "urban-data-analytics/measuring-the-city/measuring-the-city.html#common-metrics-and-indicators-for-urban-analyses",
    "href": "urban-data-analytics/measuring-the-city/measuring-the-city.html#common-metrics-and-indicators-for-urban-analyses",
    "title": "Measuring the city: metrics and indicators",
    "section": "Common metrics and indicators for urban analyses",
    "text": "Common metrics and indicators for urban analyses\nThe table below lists examples of variables/metrics that are used in urban analyses, grouped by topic.\n\n\n\nTopic\nCommon metrics\n\n\n\n\nPeople / socioeconomics\n- Population density (e.g., population per square kilometer)- Population change over time- Average household size- Median household income- % population from a given racial-ethnic category (e.g., % Asian)- % population from a given educational level (e.g., % with a high school degree)- # of crimes by census tract- % votes for a given party by riding (e.g.¬†in Toronto)- Displacement risk- Social vulnerability index- Inequality metrics (e.g., Gini index - US & Canada)- Dissimilarity index of racial segregation\n\n\nHousing\n- % owners vs.¬†renters- % single family homes vs.¬†multifamily homes- Median rent- Median home sale price per sq. ft.- Vacancy rate- Housing cost burden (share of income spent on housing costs)- Core housing need (housing suitability plus cost burden)- # of units built or permits issued over time or in a given neighborhood (e.g., Canadian ADU analysis; market-rate housing in the Bay Area)- # of dedicated affordable housing units\n\n\nLand use\n- % change in land cover or forest area (Toronto example)- % of land zoned single-family vs.¬†multifamily (e.g., zoning policy changes; residential zoning in Canadian cities, National Zoning Atlas)- Walk score- Entropy index for land use\n\n\nEconomics / employment\n- Job density overall or for a given industry sector- Location quotient (definition)- % of jobs from a given occupation- % of jobs from a given industry sector- % of workers commuting in/out of a given boundary (commuting patterns)- Venture capital investment by city- Sales by sector downtown- Downtown recovery post-pandemic (e.g., trends; Urban Activity Atlas)\n\n\nTransportation\n- % of people who commute via public transit vs.¬†car (commute mode)- % population within 1-km of a rail station or population near a given transit station (public transit accessibility)- E-bike trip distance- % or number of bike share trips by neighborhood or dock station (e.g.¬†Bike share trips in Toronto)- Density of traffic violations by street or by intersection (e.g.¬†Traffic violations)- # of public electric vehicle charging stations by neighborhood (US/Canada Alternative Fueling Station Data)- Pedestrian counts\n\n\nEnvironment\n- Prevalence of PM 2.5 by census tract (e.g.¬†Emissions/air pollution)- Average temperature by census tract (e.g., heat exposure in Toronto)- % population within 0.5-km from a public park (park accessibility)- % of land at risk of flooding in a given city or neighborhood (e.g., Toronto; FEMA; Canada)- % tree canopy coverage by census tract (e.g.¬†Toronto, U.S. data)- # of census tracts that are Environmental Justice communities\n\n\nHealth\n- Average life expectancy- Prevalence of chronic diseases by census tract (e.g., diabetes, asthma)- # of gun deaths over time",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "urban-data-analytics/measuring-the-city/measuring-the-city.html#common-spatial-units-and-measures-of-aggregation",
    "href": "urban-data-analytics/measuring-the-city/measuring-the-city.html#common-spatial-units-and-measures-of-aggregation",
    "title": "Measuring the city: metrics and indicators",
    "section": "Common spatial units and measures of aggregation",
    "text": "Common spatial units and measures of aggregation\nUrban data is often linked to specific places. This is often called spatial or geographic data.\nWhen analyzing spatial data, our analysis is often at the level of specific spatial units or unit of aggregations. Sometimes our data is directly collected at these units, while sometimes it is useful to aggregate large datasets to these units to help analyses and visualizations.\nWhen working with urban data, it‚Äôs essential to consider how geographic boundaries are defined. Standard units like census tracts or zip codes may not reflect actual community identities and are subject to change over time.\nBelow are spatial units and types of encoding that are often used for collecting and analyzing urban data.\nIn our notebook on spatial data and GIS, we go into details on how different spatial data is structured, and how we can begin to view, explore, and analyze different data in GIS.\n\nAdministrative or political boundaries\nPolitical boundaries that delineate jurisdictions for different levels of government, from national down to local levels. Countries, provinces or states, counties, municipalities, electoral districts or city wards, and within cities, neighbourhood planning areas, are all examples of commonly used spatial units.\n\n\n\nFederal electoral district in Saskatoon\n\n\n\n\nCensus geographies\nNational censuses aggregate data to a variety of spatial units ranging in size, many are the same as administrative and political boundaries, as well as many smaller-geography boundaries that are super useful for urban- and neighbourhood-scale maps and analyses. Census tracts (usually in the range of 2,500 and 8,000 persons) and Dissemination Areas (400 to 700 persons) are two scales that are often used. Check out (Statistics Canada documentation, U.S. Census documentation) or see our notebook on Canadian census data for more information.\n\n\n\nCommon census boundaries in Toronto\n\n\n\n\nGrids\nA grid is repetitive tesselation spread across the surface of a map. Grids are used in spatial analysis when existing boundaries are unavailable, unsuitable, or when evenly sized, uniform areas are required. Geohashes, which uniquely identify specific regions according to their latitude and longitude everywhere on Earth, are one type of commonly used grid.\n\n\n\nGeohashes in Qu√©bec City (source)\n\n\nGrids do not have to be 4-sided. Triangular and hexagonal grids are often used for some studies. Hexagon‚Äôs are often recommended since they are the regular polygon with the most sides (i.e.¬†can closest represent a circle), that can tesselate without any gaps.\n\n\n\nScreenshot of a map showing density of activity in Glasgow\n\n\n\n\nStreets\nWhile streets are often added to maps to provide geographic context, streets can also be their own unit of analysis. For example, traffic flow could be measured on street segments throughout a city. In the image below, streets are coloured by how many parking tickets that they have.\n\n\n\nParking tickets in Toronto (source)\n\n\n\n\nAddresses\nSome urban data is measured or collected at the address level. For example, address of businesses, non-profits, or community facilities. To map them and compare with other spatial data, addresses are often geocoded, where their names are converted into geographic coordinates (latitude and longitude). See our Spatial data and GIS tutorial for more.\n\n\n\nGeocoded addresses of businesses in Mississauga from the Canadian Urban Institutes Measuring Main Streets project",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "urban-data-analytics/measuring-the-city/measuring-the-city.html#biases-and-limitations-of-spatial-data",
    "href": "urban-data-analytics/measuring-the-city/measuring-the-city.html#biases-and-limitations-of-spatial-data",
    "title": "Measuring the city: metrics and indicators",
    "section": "Biases and limitations of spatial data",
    "text": "Biases and limitations of spatial data\nWhen collecting and analyzing data, it is important to verify the quality of the dataset. Some data is incomplete or has missing values, which can bias the results, especially if data from certain categories is missing disproportionately. For example, if income data is missing more often for lower-income individuals, the results may overestimate average income and under-represent vulnerable populations.\nThese are a few important sources of bias or limitations when working with spatial data that are super important to be aware of when working with data linked to places.\n\nSelf-reporting bias which is when individuals report inaccurate information about themselves in a survey. This can be intentional (e.g., under-reporting income or over-reporting education) or unintentional (e.g., forgetting details). This can lead to biases in the final dataset and any subsequent analysis.\nEcological fallacy is the phenomenon of drawing conclusions about individuals based on the group they belong to. For example, one might infer that everyone in a census tract with an overall high median income is wealthy. Although the median income is high, there may be low income residents who live in the tract who are not close to the median.\nEdge effects in spatial analysis refer to the limitations or distortions that occur at the boundaries of a study area. They can bias results or reduce accuracy, especially when spatial patterns or processes extend beyond the area being analyzed. For example, let‚Äôs say you were mapping access to healthy food in a city. Your map may show that one corner of your city does not have a grocery store, leading to a conclusion of it being a food dessert. But if you didn‚Äôt consider grocery stores just outside the edge or boundary of your city adjacent to this corner, this may not be the case.\nModifiable areal unit problem (MAUP) is another source of bias when working with spatial data. It is a form of statistical bias that results from the fact that changing the scale or shape of aggregation units leads to different results. Gerrymandering is a classic example of intentional MAUP to obtain specific voting outcomes. Check out the graphic below, we can see the results of how different spatial units are arranged would impact the overall results of an election. Overall, it is important to think critically when working with different spatial units to avoid misrepresenting data or cherry-picking results.\n\n\n\n\nGerrymandering example",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html",
    "title": "Programming with Python and computational notebooks",
    "section": "",
    "text": "Julia Greenberg\nThis notebook introduces the Python programming language and Jupyter notebooks. It will cover:",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#glossary",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#glossary",
    "title": "Programming with Python and computational notebooks",
    "section": "Glossary",
    "text": "Glossary\nThe glossary below lists key terms related to programming and Python as well as their simple definitions. We‚Äôll go into detail about each one later in this section, but feel free to refer to this table whenever you need a quick reminder of what any of these terms mean.\n\n\n\n\n\n\n\nTerm\nDefinition/Example\n\n\n\n\nCoding/programming\nThe act of giving a computer written instructions for a task or set of tasks using a ‚Äúlanguage‚Äù like Python, R, Javascript, SQL, etc.\n\n\nPython\nA widely used programming language\n\n\nScript\nPlain text file with code (a Python script has the .py file extension)\n\n\nTerminal\nApplication for using the command line\n\n\nCommand line\nInterface for inputting commands to computer\n\n\nIntegrated development environment (IDE)\nAn environment to write code in (also known as a ‚Äúcode editor‚Äù)\n\n\nVisual Studio Code\nPopular code editor (IDE)\n\n\nComputational notebook\nCoding environment (e.g., Jupyter Notebook) that includes code, explanations, and outputs all in one place\n\n\nJupyter Notebook\nPopular computational notebook application\n\n\nComment\nWritten explanation of code that is not implemented (in Python, lines that start with #)\n\n\nVariable\nLabeled entity that stores information (e.g., numbers or text)\n\n\nList\nCollection of elements which can be accessed by their position\n\n\nDictionary\nObject that stores information in ‚Äúkey‚Äù/‚Äúvalue‚Äù pairs\n\n\nIf statement\nCode that uses conditional logic\n\n\nFor loop\nCode that repeats for each item in a list or range\n\n\nWhile loop\nCode that repeats as long as a condition is true\n\n\nFunction\nReusable block of code that performs a task\n\n\nLibrary\nCollection of related functions that can be imported in your code",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#what-is-codingprogramming",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#what-is-codingprogramming",
    "title": "Programming with Python and computational notebooks",
    "section": "What is coding/programming?",
    "text": "What is coding/programming?\nCoding, or computer programming, is the act of giving a computer written instructions for a task or set of tasks. According to Wikipedia, it ‚Äúinvolves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages.‚Äù\nProgramming languages, just like Python and R and JavaScript can be thought of similarly to languages that people speak and write, like Spanish or Arabic ‚Äì each language has its own set of rules and different syntax, but (most) ideas can be translated from one language to another. Instead of using language to communicate with other people, you write code to give directions to a computer. In data analysis, these directions could be to summarize data in a table, create a chart or a map, or generate a statistical model, for example.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-programming",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-programming",
    "title": "Programming with Python and computational notebooks",
    "section": "Why learn programming?",
    "text": "Why learn programming?\nSpreadsheet software, like Excel, Google Sheets, and LibreOffice calc can be great for viewing and exploring data, as well as quick analyses, they can be super limited for serious data analysis. They struggle with large datasets, lacks robust tools for cleaning and transforming complex data, have very limited options for spatial/geographic data, and makes reproducibility nearly impossible. Plus, it‚Äôs easy to introduce silent errors with formulas or copy-pasting. For anything beyond basic summaries or charts, scripting languages like Python are far more powerful, reliable, and scalable.\n(If you are not interested in learning Python, many of the other notebooks in this online textbook that show Python examples, include links to equivalent examples in QGIS or Excel).",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-python",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-python",
    "title": "Programming with Python and computational notebooks",
    "section": "Why learn Python?",
    "text": "Why learn Python?\nPython is a widely used programming language, especially in urban and spatial analysis, because it‚Äôs easy to learn relative to other languages and has a strong ecosystem of libraries for working with data, maps, and models. Plus, it‚Äôs open source and has a huge community of people that use and develop it, which means lots of learning resources and support. While we‚Äôll mostly be looking at Python, the same concepts are applicable across many other languages and software.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#download-python",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#download-python",
    "title": "Programming with Python and computational notebooks",
    "section": "Download Python",
    "text": "Download Python\nBefore you can run any Python code, download the most recent version of Python from this link if you haven‚Äôt already. Make sure to choose the correct version for your computer (Windows vs.¬†macOS), and follow the instructions to complete the download.\n\n\n\nScreenshot of Python download website",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#how-do-i-run-python-code-using-the-terminal",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#how-do-i-run-python-code-using-the-terminal",
    "title": "Programming with Python and computational notebooks",
    "section": "How do I run Python code using the terminal?",
    "text": "How do I run Python code using the terminal?\nPython lets you write instructions in a plain text file, often called a script, which the Python interpreter on your computer reads and runs line by line. You don‚Äôt need to ‚Äúcompile‚Äù your code like in some other languages‚Äîyou can just write it, save it as a .py file, and run it directly.\nFor example, create a file on your computer called my-script.py, a very simple script which simply prints ‚ÄúHello, world!‚Äù. You can do this by opening a basic text editor (e.g., TextEdit on macOS or Notepad on Windows), writing (only) the following code, and saving the file as my-script.py in a directory (folder) that you‚Äôll remember. It might make sense to create a new folder on your computer for the work you do in this workshop so that it‚Äôs all in one place. Make sure to use that file extension instead of saving it as a .txt text file.\nprint(\"Hello, world!\")\nNext, run this code using the command line in your terminal. The terminal is an application that provides access to the command line, which is the space where you type instructions for your computer to execute. However, the terms ‚Äúcommand line‚Äù and ‚Äúterminal‚Äù are often used interchangeably, and are also called the shell, console, prompt, or various other names.\nYou can use the terminal/command line to do things like run programs and manipulate files and folders stored on your computer. The command line has its own language and syntax, kind of like how Python is its own language, and some of the commands are slightly different for different operating systems. In other words, they might change depending on whether you have a Mac or a Windows computer.\nTo open the terminal on a Mac, follow these instructions. To open the terminal on a Windows computer, follow these instructions. This link contains a more in-depth explanation of how the terminal works, and this link explains how to run a Python script in the terminal. If you‚Äôre still having trouble, Google your questions (or use ChatGPT!) to troubleshoot.\nFirst navigate to the folder where you saved the my-script.py file using the cd command, which changes the directory. For example, if you saved the file in a folder called urban-data, type cd urban-data and hit enter. Make sure you‚Äôre navigating to the folder according to the full filepath‚Äîthis is important if it‚Äôs not in your root folder. To see what files are located in this directory, use the ls command.\nSee a list of Windows commands here and Mac commands here.\n(base) your-computer-name ~ % cd urban-data \n(base) your-computer-name urban-data % ls\nmy-script.py\nGood‚Äîthe my-script.py is in the urban-data folder. Once you‚Äôre in the right folder, type the following: python my-script.py, and then hit enter. This should result in the computer printing out Hello, world! on the next line, like below:\nyour-computer-name:~$ python my-script.py \nHello, world!\nThis is a very simple example. Usually, you‚Äôll want your code to do more than just print out a statement. However, if you‚Äôre running .py scripts in the terminal, it can be hard to see what your code is doing. For example, you won‚Äôt be able to see a plot that you create in Python using just the command line. In the following section, we‚Äôll explain how you can use other tools and applications to code in a more interactive way.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#tools-for-writing-code",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#tools-for-writing-code",
    "title": "Programming with Python and computational notebooks",
    "section": "Tools for writing code",
    "text": "Tools for writing code\nWhen you write something in English or another language, you need to choose an environment to write in. For example, if you‚Äôre writing an email, you might use Gmail or Outlook. If you‚Äôre writing a report, you might use Google Docs or Microsoft Word. Or maybe you prefer writing on parchment with a quill pen!\nWhen coding, you also have to choose an environment to write in. This is typically called a ‚Äúcode editor‚Äù or integrated development environment (IDE). The simplest editor would simply be the notepad or text editor on your computer. But there are many IDEs to choose from that make coding much easier. Some of the most popular ones for Python are Visual Studio Code (‚ÄúVS Code‚Äù) and PyCharm.\nA main benefit for using a code editor is that it highlights different parts of your code in different colours or styles, making it much easier to read.\n\n\n\nScreenshot of part of a Python script in VS Code",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#computational-notebooks",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#computational-notebooks",
    "title": "Programming with Python and computational notebooks",
    "section": "Computational notebooks",
    "text": "Computational notebooks\nThere are many different ways to execute code, or tell the computer to perform the instructions you‚Äôve written. If you write your code in a file that‚Äôs saved with the .py file extension, you can run it all at once, and the computer will follow all of your instructions, one line at a time. This works well for scripts that are complete and do not need to be run in separate chunks.\nIf you are exploring, analyzing, or visualizing data, it is sometimes easier to work in a computational notebook. Computational notebooks are coding environments that not only allow you to write code, but also let you write explanations and show the outputs of your analysis, very similar to pre-digital formats.\n\n\n\nNotebook by Galileo\n\n\nJupyter Notebooks (named after Galileo‚Äôs work!) are often used for data analysis in Python. When using a Jupyter Notebook, you can run code chunk-by-chunk and see the output right below each chunk. For example, you can write a chunk of code that manipulates a dataframe and then look at the first few rows of the dataframe right below the code. The page you are reading now was written in a Jupyter notebook! :)\n\n\n\nExample of a Jupyter notebook that is plotting a map",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#how-do-i-run-code-using-a-jupyter-notebook-in-vs-code",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#how-do-i-run-code-using-a-jupyter-notebook-in-vs-code",
    "title": "Programming with Python and computational notebooks",
    "section": "How do I run code using a Jupyter Notebook in VS Code?",
    "text": "How do I run code using a Jupyter Notebook in VS Code?\nWhile you can choose to work with notebooks via Jupyter Lab (a web-based environment for Jupyter Notebooks) or other code editors like Sublime Text, we recommend using Jupyter Notebooks in VS Code via the Jupyter Notebook ‚Äúextension‚Äù. In other words, you‚Äôre using a Jupyter Notebook format (which has the .ipynb file extension, just like how basic Python scripts have the .py extension) in the VS Code IDE.\nTo get started in VS Code, first download the appropriate version for your computer from this link. Once you‚Äôve finished setting it up, open the desktop application and open a new folder by clicking on the ‚ÄúExplorer‚Äù tab at the top left, clicking ‚ÄúOpen Folder‚Äù, and navigating to the directory that you want to work in. This essentially pins the folder you‚Äôre working in to the left of the screen so that you can easily access all the files within it.\n\n\n\nOpening a folder in VS Code\n\n\nFor example, if you open the urban-data folder from earlier, you should see the my-script.py file within it.\n\n\n\nOpening a .py file in VS Code\n\n\nTo run this .py file directly in VS Code, click the arrow button at the top right of the window and a terminal should show up with the result:\n\n\n\nExecuting a .py file in VS Code\n\n\nTo create a Jupyter Notebook, start by creating a new file in your folder with the .ipynb extension instead of the .py extension. Next, select a Python environment to work in by clicking on the ‚ÄúSelect Kernel‚Äù button at the top right, ‚ÄúPython Environments‚Äù, and then the version of Python that you‚Äôve installed. This essentially tells VS Code which programming language you want it to use to execute your code.\n\n\n\nSelecting a Python environment\n\n\nTo write and run code in the notebook, use ‚Äúcells‚Äù that contain chunks of information. For example, write the code from earlier (print(\"Hello, world!\")) in the first cell, and then click the arrow to the left of the cell to run/execute it:\n\n\n\nRunning a chunk of code in Jupyter Notebook\n\n\nWhen you run an individual cell, you are telling the computer to follow the instructions you‚Äôve provided in only that cell, ignoring any other cells that are in the notebook. Be careful of the order in which you ‚Äúexecute‚Äù the cells. For example, if you run cell B before running cell A, it doesn‚Äôt matter if cell B is located below cell A ‚Äì the computer will still follow the instructions in B first and A second.\nTo create a new cell, click on the + Code button at the top left:\n\n\n\nCreating a new cell in Jupyter Notebook\n\n\nFor more information about how to use Jupyter Notebook in VS Code (including useful keyboard shortcuts!), see this link.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#python-basics",
    "href": "urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#python-basics",
    "title": "Programming with Python and computational notebooks",
    "section": "Python basics",
    "text": "Python basics\nNow that you have Jupyter Notebook set up, let‚Äôs code! Below, we‚Äôll cover some of the basic building blocks of Python.\nOpen up a fresh .ipynb file and you can get started building bits of code for each of the topics below\n\nVariables\nA variable is like a labeled entity that stores information (numbers, text, etc.). You can create a variable, give it a name, and assign it a value.\nNote that in the below code, some of the lines are written with a # at the beginning - these are comments. Putting a # in front of a line of code tells the computer not to execute it. You should use comments often to explain what your code is doing, either for someone else who might need to understand it or for your future self.\n\n# Assign a value to a variable\ncity = \"Toronto\"\npopulation = 100\n\nIn the above cell, we created two variables, one called city and another called population. The city variable is a string because it is a sequence of characters. The computer knows this is a string because we enclosed the text, Toronto, in quotes. Single or double quotes both work here.\nThe population variable is an integer because it is a numeric value without decimals. You can see the data type with type([name of object]) like below:\n\ntype(population)\n\nint\n\n\nIf we print the variable, it will show us the variable‚Äôs value:\n\nprint(population)\n\n100\n\n\nWe can also re-assign variables, which will change their value. Now when we print the value of population, it will show 101 instead of 100:\n\n# Re-assign `population` variable with a new value\npopulation = 101\nprint(population)\n\n101\n\n\nRemember that the computer interprets your code in the order you run the cells, not in the order of the cells in the notebook. For example, if you ran the above cell that assigns a value of 101 to the population variable before running the cell that assigns the value of 100 to population, the computer would first store the value of 101 before re-assigning it the value of 100.\n\n\nSimple math\nPython can do simple math, like a calculator:\n\n4 + 3\n\n7\n\n\n\n10/3\n\n3.3333333333333335\n\n\nYou can also use the math module to access more advanced functions, like taking the square root. To use this module, you have to import it first:\n\nimport math # import module\nmath.sqrt(25)\n\n5.0\n\n\n\n\nLists\nA list is a collection of elements which can be accessed by their position. Python uses something called zero-based indexing, which means the first element of a sequence has an index (position) of 0 instead of 1.\nIn the below example, transport_modes is a variable whose type is a list.\n\n# Assign list to variable called 'transport_modes'\ntransport_modes = [\"train\", \"car\", \"bus\"]\ntype(transport_modes)\n\nlist\n\n\n\n# Access first item in list\nprint(transport_modes[0])\n\ntrain\n\n\n\n# Access second item in list\nprint(transport_modes[1])\n\ncar\n\n\nItems can be appended to lists:\n\n# Add \"streetcar\" to the list\ntransport_modes.append(\"streetcar\")\nprint(transport_modes)\n\n['train', 'car', 'bus', 'streetcar']\n\n\nWe can check the length of the new list to see how many elements it has:\n\nlen(transport_modes)\n\n4\n\n\nLearn more about lists here.\n\n\nDictionaries\nA dictionary is a type of object that stores information in pairs: each ‚Äúentry‚Äù in the dictionary has both a key and a value. In the example below, housing_unit is a dictionary that contains characteristics ‚Äì specifically the address and age ‚Äì of a housing unit.\n\nhousing_unit = {\"address\": \"605 Main St\", \"age\": 70}\n\nWe can access the value associated with the address of the housing unit:\n\nprint(housing_unit[\"address\"])\n\n605 Main St\n\n\nWe can also add a new key-value pair to the dictionary that represents, in this case, the housing unit type:\n\nhousing_unit[\"type\"] = \"condominium\"\nprint(housing_unit)\n\n{'address': '605 Main St', 'age': 70, 'type': 'condominium'}\n\n\nLearn more about dictionaries here.\n\n\nIf statements\nIf statements let your code make decisions. You check a condition (e.g., whether income &lt; 30,000), and run different code depending on whether it‚Äôs true or false.\n\nincome = 50000\n\nif income &lt; 30000:\n    print(\"You're considered low-income.\")\nelse:\n    print(\"You're not considered low-income.\")\n\nYou're not considered low-income.\n\n\nLearn more about if statements here.\n\n\nFor loops\nA for loop repeats code for each item in a list or range. For example:\n\n# Reminder of what items are in the 'transport_modes' list:\ntransport_modes\n\n['train', 'car', 'bus', 'streetcar']\n\n\n\nfor transport_mode in transport_modes:\n    print(transport_mode)\n\ntrain\ncar\nbus\nstreetcar\n\n\nLearn more about for loops here.\n\n\nWhile loops\nA while loop repeats code as long as a condition is true. In the below example, we start with 0 and keep adding 1 until we get to 3, after which we stop counting:\n\ncount = 0\nwhile count &lt;= 3:\n    print(\"Counting:\", count)\n    count += 1\n\nCounting: 0\nCounting: 1\nCounting: 2\nCounting: 3\n\n\nLearn more about while loops here.\n\n\nFunctions\nA function is a reusable block of code that performs a task. You ‚Äúdefine‚Äù it (write the code that performs the task) once and ‚Äúcall‚Äù it (run that pre-defined code) whenever you want. In the below example, we define the function zoning so that when it is called, it prints ‚ÄúThis parcel is zoned as [land_use]!‚Äù where land_use is an argument (also known as a parameter) that‚Äôs passed into the function.\n\n# Define the function\ndef zoning(land_use):\n    print(f\"This parcel is zoned as {land_use}.\")\n\n# Call the function\nzoning(\"commercial\")\n\nThis parcel is zoned as commercial.\n\n\nNot all functions need arguments. For example:\n\n# Define the function\ndef largest_cities():\n    print(\"Toronto, Montreal, and Calgary are the largest cities in Canada by population.\")\n\n# Call the function\nlargest_cities()\n\nToronto, Montreal, and Calgary are the largest cities in Canada by population.\n\n\nSome functions have more than one argument. For example:\n\n# Define the function\ndef add_numbers(a, b):\n    c = a + b\n    print(c)\n\n# Call the function\nadd_numbers(8, 7)\n\n15\n\n\nWhile all of the example functions listed above result in something being printed out, most functions do more than that. For example, a single function can filter a dataset based on a set of values, manipulate the resulting dataset, and create a plot.\nLearn more about functions here.\n\n\nLibraries\nLibraries are collections of related functions that do things like analyze data, draw charts, or work with maps.\nMany libraries like math for a variety of mathematical operations, os for interacting with files on your computer, and random for generating random numbers, typically come automatically installed with Python.\nThere are many popular freely available libraries available‚Äîlike pandas for working with data tables, matplotlib for plotting, and geopandas for spatial data‚Äîthat can help you do a lot with just a few lines of code.\nTo include functions that are part of a library, we use import at the top of our script. For example, let‚Äôs import a function for a random number generator, and use it to create our own d20 dice rolling function.\n\nimport random\n\n# Function to roll a 20-sided dice\ndef roll_d20():\n    return random.randint(1, 20)\n\n# Simulate rolling the dice\nroll = roll_d20()\nprint(roll)\n\n14\n\n\nWhen using external libraries like pandas, we need to install the library before we can import and then use it. In Jupyter Notebook, we can do this by running !pip install [name of library]. If you are running Python in your command line, you can simply run pip install [name of library]\nIf you don‚Äôt already have pip installed on your computer, follow these instructions. If you‚Äôre still having trouble, try Googling your specific questions or asking a chatbot for step-by-step instructions!",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html",
    "href": "urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html",
    "title": "Data ethics and literacy",
    "section": "",
    "text": "Karen Chapple, Evelyne St.¬†Louis, Michelle Zhang\nThis section will include 1) key questions and concepts in data literacy, 2) ethical data practices, and 3) centering equity in data.",
    "crumbs": [
      "Urban Data Analytics",
      "Data ethics and literacy"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html#data-literacy",
    "href": "urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html#data-literacy",
    "title": "Data ethics and literacy",
    "section": "Data literacy",
    "text": "Data literacy\n\n\n \n\n\nData literacy begins with applying a critical lens and asking the right questions about data. Before using a dataset, you should always ask yourself: (1) who collected the data, (2) why did they collect the data, (3) what does the data say, (4) what does it leave out, and (5) how can it be used to support your goals?\nNext, it is important to consider the limitations of the scale, or geography, used for analysis. When analyzing communities, census tracts are often considered the gold standard. However, they come with sampling limitations, especially at smaller scales. Not everyone responds to the census, so a lot of data is imputed. Aggregating data can also lead to misleading generalizations about neighbourhoods that don‚Äôt ring true from one block to another. It‚Äôs important to consider context and local definitions of the neighbourhood, since that can drastically change the story your data tells.\nFinally, effective and trustworthy communication is the final step in building data literacy. Visualizations should be clear, credible, and honest about the limitations of the data. Avoid distracting visuals, ‚Äúchartjunk‚Äù, misleading precision, and vague labels. Instead, aim for clarity and purpose in your design. How you frame your data also matters. Shifting from deficit-based narratives to asset-based ones can powerfully reframe discussions and impact how your analysis is received.",
    "crumbs": [
      "Urban Data Analytics",
      "Data ethics and literacy"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html#data-ethics",
    "href": "urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html#data-ethics",
    "title": "Data ethics and literacy",
    "section": "Data ethics",
    "text": "Data ethics\n\n\n \n\n\nUsing data ethically starts with recognizing the people behind the numbers. Behind every dataset are many people: data generators (that‚Äôs often you and me), data collectors, and data utilizers. Each group brings different intentions and potential biases, which can shape how data is framed and interpreted. Each time you encounter a new dataset, take a moment to reflect on who collected the data, who is represented in the data, who benefits, and who might be harmed.\nOther best practices for the ethical use of data include protecting privacy, preventing reidentification, ensuring transparency, and making methods replicable. Following clear codes of conduct helps reduce the risk of unintended harm. Frameworks like the Locus Charter offer specific guidance for spatial data, emphasizing the importance of minimizing intrusion and safeguarding vulnerable populations. Another great resource to explore is these Ten Simple Rules for Responsible Big Data Research.",
    "crumbs": [
      "Urban Data Analytics",
      "Data ethics and literacy"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html#centering-equity-in-data",
    "href": "urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html#centering-equity-in-data",
    "title": "Data ethics and literacy",
    "section": "Centering equity in data",
    "text": "Centering equity in data\nCentering equity in data means challenging deficit-based narratives and telling stories that reflect systemic injustices and community strengths. For example, mapping redlining alongside current patterns of displacement highlights how historical discrimination continues to shape neighborhoods. Reframing data around what was lost - like generational wealth due to exclusion - can deepen understanding and fuel advocacy. However, be aware of the way data is organized. For example, dimensions of group and individual identity can‚Äôt be examined in isolation from each other. Understanding intersectionality reveals forces of oppression and privilege and how they work differently across income, race, gender, ability, sexuality, and immigrant status.",
    "crumbs": [
      "Urban Data Analytics",
      "Data ethics and literacy"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html#additional-readings",
    "href": "urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html#additional-readings",
    "title": "Data ethics and literacy",
    "section": "Additional readings",
    "text": "Additional readings\nThe videos and content above represent a brief introduction to the topics of data ethics and centering equity in data. To learn more or dive in deeper, Wwe encourage you to check out the following additional readings:\nSchwabish, J. (2018). Form and Function: Let Your Audience‚Äôs Needs Drive Your Visualization Choices. The Urban Institute, Data@Urban Medium. Retrieved from:\nSchwabish, J., & Feng, A. (2020). Applying Racial Equity Awareness in Data Visualization.\nSchwabish, J. & Feng, A. (2021). Do No Harm Guide: Applying Racial Equity Awareness in Data Visualization. The Urban Institute. Retrieved from:\nBelow are the citations and resources mentioned in the video:\nD‚Äôignazio, C. & Klen, L. F. (2023). Data feminism. MIT Press.\nEthical Geo (2021). Locus Charter. Retrieved from:\nTufte, E. R. (1983). The Visual Display of Quantitative Information. Graphics Press.\nChapple, K., & Thomas, T., and Zuk, M. (2021). Urban Displacement Project website. Berkeley, CA: Urban Displacement Project. Retrieved from: https://www.urbandisplacement.org/\nZook, M., Barocas, S., Boyd, D., Crawford, K., Keller, E., Gangadharan, S. P., ‚Ä¶ & Pasquale, F. (2017). Ten simple rules for responsible big data research. PLoS computational biology, 13(3), e1005399.",
    "crumbs": [
      "Urban Data Analytics",
      "Data ethics and literacy"
    ]
  },
  {
    "objectID": "urban-data-analytics/canadian-census-data/canadian-census-data.html",
    "href": "urban-data-analytics/canadian-census-data/canadian-census-data.html",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "Jeff Allen\nNational censuses, like the Canadian and U.S. censuses, are very common data sources analyzing demographic and socio-economic data pertaining to specific places.\nStatistics Canada conducts a national census of the population every five years, asking a range of demographic and socio-economic questions. The results paint a demographic portrait of the country at the time period the census was conducted.\nThe most recent census at the time of writing was in 2021. Lots of census data are publicly available for download, across the following topics:\nMost data are pre-aggregated to a variety of geographic boundaries (e.g.¬†provinces, cities, neighbourhoods, blocks, etc.), which allow for finding a variety of demographic and socio-economic statistics for specific places as well as for making a range of maps.\nFor example, here‚Äôs a map of population density in the Greater Toronto Area (GTA) and the census block level, clearly showing where people are clustered throughout the region.\nThis notebook covers:",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "urban-data-analytics/canadian-census-data/canadian-census-data.html#overview-of-the-canadian-census",
    "href": "urban-data-analytics/canadian-census-data/canadian-census-data.html#overview-of-the-canadian-census",
    "title": "Overview of Canadian census data",
    "section": "Overview of the Canadian census",
    "text": "Overview of the Canadian census\nThere are two parts to the census, the short-form survey and the long-form survey. The short-form survey asks a set of basic household and demographic questions (e.g.¬†address, age, marital status, etc.) and is sent to all households in Canada. The long-from survey is sent to 25% of households in Canada. It asks additional questions pertaining to a broader range of demographic, social, and economic topics (e.g.¬†religion, education, journey to work, etc.). Statistics Canada also augments collected census survey data by joining in data from other administrative sources, including income data collected by the Canadian Revenue Agency (CRA).\nCensus data are collected primarily on a household-by-household basis (one adult member in each household usually fills out the census on behalf of everyone in the household). Data of individual responses from the census are often called census ‚Äúmicro-data‚Äù. Because of personal identification concerns, this data is only accessible by accredited researchers. (However, note that a public use microdata file called the PUMF is available. It is a random sample of the overall population, with several of the identifying variables removed, such as home addresses and postal code).",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "urban-data-analytics/canadian-census-data/canadian-census-data.html#finding-census-data",
    "href": "urban-data-analytics/canadian-census-data/canadian-census-data.html#finding-census-data",
    "title": "Overview of Canadian census data",
    "section": "Finding census data",
    "text": "Finding census data\nSummaries (i.e.¬†aggregations) of census data to a range of geographic areas are publicly available to view online or download. These are super useful for understanding the demographics of a place. For example, the total population in a province, the number of people who speak Spanish in Toronto, or the average income in a specific neighbourhood.\nThe Census Profile tables on Statistics Canada‚Äôs website allow for searching for census data for specific variables and geographic areas. For example, here‚Äôs an output of ‚ÄúKnowledge of Official Languages‚Äù in Ontario.\n\n\n\nTable of knowledge of language in Ontario\n\n\nWhen working with census data, it‚Äôs often advisable to use the Census Dictionary, the main reference guidebook, to understand what different data variables and geographies in the census represent. For example, here‚Äôs the entry for Knowledge of official languages.\nCensus profile data is typically limited to single categories totals (e.g.¬†number of people who speak French by gender), as shown in the table above. However, if you are interested in cross-tabulations, that is, summaries across multiple categories (e.g.¬†number of people who have knowledge of French who also speak French at work, e.g.¬†total number low-income residents who live in different types of housing), then there are a variety of Data Tables available for this purpose.\nIf neither the Census Profile or Data Tables fit your purpose, there is also a Public Use Microdata File (PUMF). This is a non-aggregated (i.e.¬†each row is a disaggregated individual-level response) dataset covering a sample of the Canadian population. This data can be queried and cross-tabulated across any number of categories included. For privacy reasons, the data only include larger geographic linkages (e.g.¬†provinces, large metro areas), and are only a sample of the overall census.",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "urban-data-analytics/canadian-census-data/canadian-census-data.html#census-geography",
    "href": "urban-data-analytics/canadian-census-data/canadian-census-data.html#census-geography",
    "title": "Overview of Canadian census data",
    "section": "Census geography",
    "text": "Census geography\nThere are a number of geographic boundaries available with associated census data, ranging in scale from city blocks to the entire country. Below is an example of commonly used boundaries for urban-scale maps and analysis.\n\n\n\nCommon census boundaries in Toronto\n\n\nEach polygon on this map has associated publicly available summary census data. Joining this tabular data to these spatial boundaries allows for making a wide range of maps showing the distribution of demographics and socio-economic variables\nYou can bulk download census data for a number of geographic levels and formats from the Statistics Canada website. These downloads are essentially copies of the Census Profile data, but for all regions noted in each row.\nOne issue to be aware of is that census boundaries can change over time each time a census is conducted. Doing a longitudinal analysis of spatial census data often requires using a technique like areal interpolation, in which data are joined to a common set of spatial units prior to analyses.\n\n\n\nExample of census tract boundaries changing in Victoria",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "urban-data-analytics/canadian-census-data/canadian-census-data.html#making-maps-with-censusmapper",
    "href": "urban-data-analytics/canadian-census-data/canadian-census-data.html#making-maps-with-censusmapper",
    "title": "Overview of Canadian census data",
    "section": "Making maps with CensusMapper",
    "text": "Making maps with CensusMapper\nCensusMapper is a website for exploring and downloading census data across Canada. When we first land on the website, it defaults to a map of Population Density in Vancouver and it shares a number of preset options for making maps.\nIf we want to search for a specific census variable, we can click Make a Map in the top right, and then select the year (e.g.¬†2021). Here we can search and explore all available data. By using the search icon at the top-left to search for a specific geography, or by clicking the inset Canada map (top right of the map), we can navigate elsewhere in the country. Just through a few clicks, I was able to create this map of knowledge of Portuguese in Toronto. Try making your own map!\n\n\n\nPortuguese in Toronto on CensusMapper\n\n\nWe can also use CensusMapper to download census data for specified geographic boundaries. To do so, click on API on the top right. First select the census year. Variable Selection is used for searching for and selecting the variables (i.e.¬†attribute data such as knowledge of a particular language) to download. Region Selection is the geographic area that we want download data for (e.g.¬†for Toronto). In the Overview panel, we can view what we‚Äôve selected as well as pick the geographic aggregation level (e.g.¬†Census Tracts, Dissemination Areas, etc.). Once selected, we can then download the attribute table and/or geographic boundaries.\nCensusMapper is partly built on anR library for downloading census data called cancensus. If you work with R, it is definitely worth checking out!",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "urban-data-analytics/canadian-census-data/canadian-census-data.html#further-analysis-and-visualization-of-census-data",
    "href": "urban-data-analytics/canadian-census-data/canadian-census-data.html#further-analysis-and-visualization-of-census-data",
    "title": "Overview of Canadian census data",
    "section": "Further analysis and visualization of census data",
    "text": "Further analysis and visualization of census data\nWhile CensusMapper (and other online tools like it) are great for exploring and downloading data, we often want to make more customized maps (e.g.¬†for a report, a paper, a website, etc.) or analyze census data in conjunction with other data sources (e.g.¬†comparing demographic data to the location of libraries, public transit, or grocery stores, etc.).\nTo do so, the general process is to first download census data directly from the Statistics Canada website above or from CensusMapper and then load it into whatever software or library that you are working with.\nCheck out our other notebooks if you want to learn how to use census data to make a choropleth map or proportional symbol maps in QGIS or Python.",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "urban-data-analytics/statistical-foundations/statistical-foundations.html",
    "href": "urban-data-analytics/statistical-foundations/statistical-foundations.html",
    "title": "Statistical foundations",
    "section": "",
    "text": "Aniket Kali\nDownload this notebook and data\nIn our other notebooks, we‚Äôve learned how to process data - but we also need to know how to understand it and analyze it in convincing ways. Statistics is the core of this - it can reveal what is typical or what is an outlier, what relationships exist between different variables, and whether our assumptions about the data are accurate.\nIn order to do this, we are going to use the Python libraries numpy and scipy. Both of these libraries offer greater mathematical precision and access to a wide variety of statistical methods and tests, which we can apply to a dataset loaded in pandas. For the sake of this tutorial, we also use matplotlib to show some intuitive visualizations - but you don‚Äôt need to worry about tinkering with these yet.\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nFor this tutorial, we are going to use election and census data that was adapted for a project published by the School of Cities looking at immigrant vote patterns in the GTA. The data we are using is adapted from the 2025 Ontario election and the 2021 Canadian census. The data frame has the following columns:\ndf = pd.read_csv('./data/ont-ed_stats_2025.csv')\ndf.head()\n\n\n\n\n\n\n\n\nriding_name\nnum_pop_tot\nnum_imm_tot\nnum_imm_new\navg_hou_inc\nnum_vm_tot\ncons_pct\nlib_pct\nndp_pct\noth_pct\n\n\n\n\n0\nBrampton Centre\n102309.0\n48275.0\n14985.0\n103369.0\n68880.0\n51.85\n33.92\n8.77\n5.45\n\n\n1\nScarborough‚ÄîRouge Park\n102256.0\n52637.0\n8294.0\n120745.0\n77090.0\n52.98\n34.75\n10.38\n1.89\n\n\n2\nScarborough North\n94688.0\n60423.0\n11513.0\n103151.0\n87647.0\n52.98\n34.75\n10.38\n1.89\n\n\n3\nAjax\n115392.0\n47716.0\n8721.0\n137570.0\n74759.0\n44.16\n44.96\n7.01\n3.87\n\n\n4\nBeaches‚ÄîEast York\n106811.0\n33588.0\n9031.0\n129975.0\n39034.0\n21.38\n51.17\n22.94\n4.51",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical foundations"
    ]
  },
  {
    "objectID": "urban-data-analytics/statistical-foundations/statistical-foundations.html#descriptive-statistics-the-shape-of-the-data",
    "href": "urban-data-analytics/statistical-foundations/statistical-foundations.html#descriptive-statistics-the-shape-of-the-data",
    "title": "Statistical foundations",
    "section": "Descriptive statistics: the shape of the data",
    "text": "Descriptive statistics: the shape of the data\n\nCentral tendency\nWhat is ‚Äútypical‚Äù in a set of data? This is at the heart of the notion of the central tendency in statistics. There are three common ways to measure this, with the accompanying functions.\n\nMean (np.mean()): the average of all values\nMedian (np.median()): the ‚Äúmiddle‚Äù value if you order them, ie. at the 50th percentile\nMode (stats.mode()): the most common value\n\nLet‚Äôs compute some of these central tendency statistics for the NDP, and compare them to the distribution of vote shares in all ridings.\n\nparty_var = 'ndp_pct'\n\nmean = np.mean(df[party_var])\nmedian = np.median(df[party_var])\nmode = stats.mode(df[party_var])\n\nprint(f\"Mean: {mean:.2f}\")\nprint(f\"Median: {median:.2f}\")\nprint(f\"Mode: {mode.mode} (appears {mode.count} times)\")\n\nMean: 14.11\nMedian: 6.88\nMode: 3.95 (appears 2 times)\n\n\n\nplt.figure(figsize=(8, 5))\nplt.hist(df[party_var], bins=20, color='orange', edgecolor='black')\n\nplt.axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:.2f}%')\nplt.axvline(median, color='blue', linestyle='--', linewidth=2, label=f'Median: {median:.2f}%')\nplt.axvline(mode.mode, color='green', linestyle='--', linewidth=2, label=f'Mode: {mode.mode:.2f}%')\n\nplt.title('Distribution of Vote Percentages by Riding')\nplt.xlabel('Vote Percentage (%)')\nplt.ylabel('Number of Ridings')\nplt.grid(axis='y', alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotice the difference between the mean and the median. While the mean accounts for all of the data directly, it is also sensitive to outliers and can be pulled in one direction or another. This is often called a skew (the histogram above shows a right skew).\nTry setting the variable party_var to different parties to see how the matter of mean vs.¬†median plays out for different distributions.\nIn spreadsheet software like Excel or Google Sheets, you can also calculate the mean, the median, and mode for a given variable using the appropriate functions (=AVERAGE,=MEDIAN, =MODE). You can also create a histogram (a commonly used chart type that shows frequency distributions, as seen above). To do this, you must select the range of the variable you wish to chart, go to ‚ÄòInsert‚Äô, then ‚ÄòStatistical‚Äô from the Charts group, and then select the chart type ‚ÄòHistogram‚Äô. You can then modify the Histogram chart elements, such as the number or size of the bins. Learn more here:\n\nCreate a histogram in Excel, including modifying the bins\nCreate a histogram in Google Sheets\n\n\n\nDispersion\nSimilar to how we ask what is typical in a set of data, it‚Äôs also important to ask how that data varies. This is what we refer to when we talk about the dispersion of a distribution, and there are four common measures (with accompanying functions):\n\nStandard deviation (np.std()): How spread out data points are from the mean, in the same units as the original data.\nVariance (np.var()): The average squared deviation from the mean, representing how wildly individual values differ from the average or overall unevenness.\nRange (np.max() - np.min()): The difference between the maximum and minimum values, showing the total spread of the dataset.\nInterquartile range (np.percentile(..., [25, 75])): The range of the middle 50% of data (Q3‚ÄìQ1), reducing sensitivity to outliers.\n\nLet‚Äôs take a look at how the number of immigrants across different ridings varies, and compare it with the mean and median.\n\ncol_var = 'num_imm_tot'\n\nmean = np.mean(df[col_var])\nmedian = np.median(df[col_var])\nstd_dev = np.std(df[col_var], ddof=1)\nvariance = np.var(df[col_var])\ndata_range = np.max(df[col_var]) - np.min(df[col_var])\nq1, q3 = np.percentile(df[col_var], [25, 75])\niqr = q3 - q1\n\nprint(f\"Mean: {mean:.2f}\")\nprint(f\"Median: {median:.2f}\")\nprint('===')\nprint(f\"Std Dev: {std_dev:.2f}\")\nprint(f\"Variance: {variance:.2f}\")\nprint(f\"Range: {data_range:.2f}\")\nprint(f\"IQR: {iqr:.2f}\")\n\nMean: 52330.33\nMedian: 54255.00\n===\nStd Dev: 13630.44\nVariance: 182348372.93\nRange: 62964.00\nIQR: 18211.50\n\n\n\nplt.figure(figsize=(10, 6))\nax = plt.gca()\n\n# Histogram\nn, bins, patches = plt.hist(df[col_var], bins=20, color='#2ca02c', edgecolor='black', alpha=0.7)\n\n# Central tendency and dispersion\nplt.axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:,.0f}')\nplt.axvline(median, color='blue', linestyle='--', linewidth=2, label=f'Median: {median:,.0f}')\n\nplt.axvspan(mean - std_dev, mean + std_dev, color='red', alpha=0.1, label=f'¬±1 Std Dev: {std_dev:,.0f}')\nplt.axvspan(q1, q3, color='blue', alpha=0.1, label=f'IQR: {iqr:,.0f}')\n\n# Annotations\nplt.title('Distribution of Immigrant Population by Riding', pad=20, fontsize=14)\nplt.xlabel('Number of Immigrants', fontsize=12)\nplt.ylabel('Number of Ridings', fontsize=12)\nplt.grid(axis='y', alpha=0.2)\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTake a look at how different measures of dispersion capture different parts of the data. Once again, set col_var to a different variable or two and examine the different measures of dispersion and how they play out.\nIn spreadsheet software like Excel or Google Sheets, you can measure the dispersion of your data using individual formulas that you apply to the specific columns you wish to analyze. You can calculate specific percentiles or the standard deviation using the =PERCENTILE and =STDEVA functions. As noted in the previous section, a histogram will also allow you to see the overall dispersion of your data.",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical foundations"
    ]
  },
  {
    "objectID": "urban-data-analytics/statistical-foundations/statistical-foundations.html#bivariate-analysis-finding-similarities-in-the-data",
    "href": "urban-data-analytics/statistical-foundations/statistical-foundations.html#bivariate-analysis-finding-similarities-in-the-data",
    "title": "Statistical foundations",
    "section": "Bivariate analysis: finding similarities in the data",
    "text": "Bivariate analysis: finding similarities in the data\n\nCorrelation\nCorrelation measures how closely two variables move together, ranging from -1 (as X increases, Y decreases), to +1 (as X increases, Y increases). A value near 0 indicates no linear association. While it is acceptable to use the word ‚Äòrelationship‚Äô here instead of ‚Äòassociation‚Äô, it‚Äôs worth noting that a strong correlation is not enough to show causation ‚Äì namely, if we had a correlation value of 1, we can‚Äôt definitively say that Y increases because X increases, only that they increase together.\nWe‚Äôll be looking at a very common measure called Pearson correlation. It measures how closely two variables follow a straight-line relationship, ideal for normally distributed data with linear trends (e.g., height vs.¬†weight). While we won‚Äôt look at it today, it‚Äôs worth independently looking into ‚ÄúSpearman Rank Correlation‚Äù as well - which can capture nonlinear trends.\nWe‚Äôll look at the example of vote share for the Progressive Conservatives and the number of immigrants in a riding below. In the project that this data was drawn from, we used correlation to show how immigrant voters in the GTA are shifting conservative over time.\n\ncensus_var = 'num_imm_tot'\nparty_var = 'cons_pct'\n\nr, p_val = stats.pearsonr(df[census_var], df[party_var])\nprint(f\"Pearson corr: {r:.2f} (p-value: {p_val:.3f})\")\n\nPearson corr: 0.40 (p-value: 0.003)\n\n\nHere we can see a correlation between the two variables we have chosen. Try setting census_var and party_var to different column values and see if there is a correlation between those as well, or not!\nIn spreadsheet software like Excel or Google Sheets, you can calculate the Pearson correlation using the =PEARSON function for any two variable arrays. Try replicating the analysis above in Excel or Google Sheets. Learn more here:\n\nPearson correlation in Excel\nPearson correlation in Google Sheets",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical foundations"
    ]
  },
  {
    "objectID": "urban-data-analytics/statistical-foundations/statistical-foundations.html#linear-regression-relationships-between-variables",
    "href": "urban-data-analytics/statistical-foundations/statistical-foundations.html#linear-regression-relationships-between-variables",
    "title": "Statistical foundations",
    "section": "Linear regression: relationships between variables",
    "text": "Linear regression: relationships between variables\n\nSimple linear regression (in Python)\nSimple linear regression identifies the straight-line relationship between two variables, allowing you to predict outcomes (e.g., voting percentages) based on another factor (e.g., immigrant population). It calculates a ‚Äúbest-fit‚Äù line that minimizes the distance between all data points and the line itself, summarized by the equation y = slope  x + intercept*.\n\ncensus_var = 'num_imm_tot'\nparty_var = 'cons_pct'\n\nx = df[census_var].values  # Independent variable\ny = df[party_var].values     # Dependent variable\n\n# Fit regression\nslope, intercept, r_value, p_value, _ = stats.linregress(x, y)\nr_squared = r_value ** 2\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.scatter(x, y, color='blue', alpha=0.5, label='Actual Data')\nplt.plot(x, intercept + slope * x, 'r-', label=f'Regression Line: $R¬≤$={r_squared:.2f}\\nslope={slope:.5f}')\nplt.xlabel('Immigrant Population')\nplt.ylabel('Conservative Vote %')\nplt.title('Predicting Votes from Demographics', pad=15)\nplt.legend()\nplt.grid(alpha=0.2)\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Model Equation: {party_var} = {intercept:.2f} + {slope:.5f} * {census_var}\")\nprint(f\"R¬≤ = {r_squared:.3f} (Explains {r_squared*100:.1f}% of variance)\")\nprint(f\"p-value = {p_value:.4f} {'(Significant)' if p_value &lt; 0.05 else '(Not significant)'}\")\n\nModel Equation: cons_pct = 23.99 + 0.00039 * num_imm_tot\nR¬≤ = 0.161 (Explains 16.1% of variance)\np-value = 0.0026 (Significant)\n\n\nNot all regression lines fit the underlying data in the same way. For this reason, R¬≤ is used to measure how well the regression line fits the data, ranging from 0 (no fit) to 1 (perfect fit). It answers: ‚ÄúWhat percentage of variation in voting patterns can be explained by immigrant population?‚Äù. In the above example, we can say that 16.1% of the differences in conservative votes across ridings are predictable from immigrant numbers‚Äîthe rest is due to other factors.\nNow try the following two exercises to examine outcomes:\n\nChange the x-variable to avg_hou_inc. Does wealth predict conservative votes better than immigrant population?\nAdd ndp_pct as y-variable. Is the relationship positive or negative?\n\n\n\nSimple linear regression (in Excel)\nIt is less common to use spreadsheet software like Excel or Google Sheets for regression analysis. However, there are ways to manually obtain the model equation for a simple linear regression and obtain the associated R¬≤ value.\n\nFirst, create a scatter plot of the two variables you wish to use for your regression. Select your data arrays, go to ‚ÄòInsert‚Äô, then under Chart Type select ‚ÄòScatter Plot‚Äô.\nOnce your scatter plot is displayed, click on the drop-down menu for ‚ÄòAdd Chart Element‚Äô in the top left corner of the Chart Design tab, select ‚ÄòTrendline‚Äô, and then ‚ÄòLinear Trendline‚Äô.\nOnce your trendline is visible on your scatter plot chart, you can then go to Trendline Options and check the following boxes ‚ÄòDisplay equation on chart‚Äô and ‚ÄòDisplay R-squared on chart‚Äô.\nYou should now see the model equation and the R-square value displayed on your scatter plot.\nNote that you can also manually calculate the R-squared value using the =RSQ function.\n\nSee this video for a live example of how to do this Excel. Note that similar steps can be followed in Google Sheets.\n\nYouTube tutorial by Steven Bradburn on adding a trendline, equation, and R2 in Excel.\n\nIf you have the Data Analysis Toolpak add-on, you can also use this to explore your linear regressions in Excel:\n\nData Analysis Toolpak: Linear Regression in Excel\n\nAnd for those using Google Sheets!\n\nLinear regression in Google Sheets tutorial\n\n\n\nOther kinds of regression\nWhile linear regression is ideal for modeling straight-line relationships, real-world data often requires more flexible approaches. Below are key alternatives with use cases relevant to political/demographic data, along with their Python implementations:\n\nMultiple Linear Regression: Modeling how multiple demographic factors jointly influence voting patterns (e.g., predicting Conservative vote share using both immigrant population and average income).\nPolynomial Regression: Modeling curved relationships (e.g., voter turnout vs.¬†age, where middle-aged groups vote more than very young or elderly).\nLogistic Regression: Predicting binary outcomes (e.g., whether a riding will vote Conservative (1) or not (0) based on income thresholds).\nRidge/Lasso Regression: Handling multicollinearity (e.g., when immigrant population and visible minority numbers are strongly correlated).\nMany more!\n\nIn case you want to do these more complex kinds of regression, the typical go to library in Python is scikit-learn.",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical foundations"
    ]
  },
  {
    "objectID": "urban-data-analytics/statistical-foundations/statistical-foundations.html#hypothesis-testing-making-conclusions",
    "href": "urban-data-analytics/statistical-foundations/statistical-foundations.html#hypothesis-testing-making-conclusions",
    "title": "Statistical foundations",
    "section": "Hypothesis testing: making conclusions",
    "text": "Hypothesis testing: making conclusions\nHypothesis testing evaluates whether observed patterns in data are statistically significant or likely due to random chance. A t-test compares the means of two groups (e.g., Conservative vote share in high- vs.¬†low-income ridings) to determine if their difference is meaningful.\nThe t-statistic measures the size of the difference relative to the variability in the data‚Äîthink of it like a ‚Äúsignal-to-noise ratio‚Äù where if the group difference (signal) stands out from natural variation (noise). Larger absolute values (e.g., |t| &gt; 2) suggest stronger evidence against no difference (a strong ‚Äúsignal‚Äù), and the sign indicates direction (e.g., positive = Group A &gt; Group B).\nThe p-value then calculates how likely we‚Äôd see this t-statistic if no true difference existed. If p &lt; 0.05, we reject the null hypothesis.\nThere‚Äôs two different t-tests we‚Äôll illustrate today. - One-sample: Compares data to a fixed number - Two-sample: Compares two datasets to each other.\n\nOne-Sample t-test\nOur dataset only includes ridings within the Greater Toronto Area (GTA) - but more than half of the ridings exist in the rest of Ontario. In the 2025 election, the Conservatives won 43% of the vote - does the average vote share for Conservatives in the GTA vary from Ontario in full?\n\nparty_var = 'cons_pct'\ntotal_party_vote = 43\n\nsample_data = df[party_var]\nt_stat, p_value = stats.ttest_1samp(sample_data, popmean=total_party_vote)\n\nprint(f\"t-statistic: {t_stat:.2f}, p-value: {p_value:.4f}\")\nprint(\"Significantly different!\" if p_value &lt; 0.05 else \"No significant difference.\")\n\nt-statistic: 0.76, p-value: 0.4497\nNo significant difference.\n\n\n\nplt.figure(figsize=(8, 4))\nplt.hist(sample_data, bins=15, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(total_party_vote, color='red', linestyle='--', label=f'Vote share in full election ({total_party_vote}%)')\nplt.xlabel('Vote %')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTwo-Sample t-test\nWe‚Äôve talked a lot about immigrant ridings as a whole, but we can also examine different subsets of them. For example: Do high-immigrant ridings vote differently than low-immigrant ridings?\n\ncensus_var = 'num_imm_tot'\nparty_var = 'cons_pct'\n\n# Split data into two groups (median split)\nmedian_data = df[census_var].median()\nhigh_data = df[df[census_var] &gt; median_data][party_var]\nlow_data = df[df[census_var] &lt;= median_data][party_var]\n\n# Independent t-test\nt_stat, p_value = stats.ttest_ind(high_data, low_data)\n\nprint(f\"t-statistic: {t_stat:.2f}, p-value: {p_value:.4f}\")\nprint(\"Significantly different!\" if p_value &lt; 0.05 else \"No significant difference.\")\n\nt-statistic: 2.52, p-value: 0.0149\nSignificantly different!\n\n\n\nplt.figure(figsize=(8, 4))\nplt.hist(high_data, bins=15, alpha=0.5, label=f'High-{census_var} ridings')\nplt.hist(low_data, bins=15, alpha=0.5, label=f'Low-{census_var} ridings')\nplt.xlabel('Vote %')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFinally, substitute out the census and party variables for different versions and run this analysis. Assess whether there are significant differences in the new hypothesis that you are testing.\nAs previously noted, it is less common to perform statistical analysis using spreadsheet software. However, some options are available. For example:\n\nYou can use the =T.TEST function in Excel and Google Sheets to return the p-value for a two-sample t-test (note that it will only return the p-value, not the t-statistic).\nFor a one-sample t-test, you can calculate the p-value and t-statistic manually. This video tutorial provides an example of how to do this.\nIf you are using the Data Analysis Toolpak add-in in Excel, you can run both one-sample or two-sample t-tests using the add-in features.",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical foundations"
    ]
  },
  {
    "objectID": "urban-data-storytelling/urban-data-storytelling-importance/urban-data-storytelling-importance.html",
    "href": "urban-data-storytelling/urban-data-storytelling-importance/urban-data-storytelling-importance.html",
    "title": "The importance of urban data storytelling",
    "section": "",
    "text": "Karen Chapple, Evelyne St.¬†Louis, Michelle Zhang\nThis page covers 1) an overview of data storytelling, 2) elements of a good data story, and 3) examples of effective data stories.",
    "crumbs": [
      "Urban Data Storytelling",
      "The importance of urban data storytelling"
    ]
  },
  {
    "objectID": "urban-data-storytelling/urban-data-storytelling-importance/urban-data-storytelling-importance.html#what-is-urban-data-storytelling",
    "href": "urban-data-storytelling/urban-data-storytelling-importance/urban-data-storytelling-importance.html#what-is-urban-data-storytelling",
    "title": "The importance of urban data storytelling",
    "section": "What is urban data storytelling",
    "text": "What is urban data storytelling\n\n\n \n\n\nTo tell a data story, start by understanding that cities are not just a collection of places but a web of interactions and interconnected systems. To build a coherent and compelling story, we must make a series of thoughtful decisions: how we observe our cities, what we choose to measure, how we analyze that information, and how we communicate our findings to others. This process lays the foundation for meaningful data storytelling that reflects the complexity of urban life.",
    "crumbs": [
      "Urban Data Storytelling",
      "The importance of urban data storytelling"
    ]
  },
  {
    "objectID": "urban-data-storytelling/urban-data-storytelling-importance/urban-data-storytelling-importance.html#elements-of-a-good-data-story",
    "href": "urban-data-storytelling/urban-data-storytelling-importance/urban-data-storytelling-importance.html#elements-of-a-good-data-story",
    "title": "The importance of urban data storytelling",
    "section": "Elements of a good data story",
    "text": "Elements of a good data story\nStrong stories have a temporal or sequential structure that shows change over time. They explain not just what is, but what happened and what could be. Good stories are generalizable - they reveal something broader through specific examples. They follow conventions, with plots and protagonists, and often include a moral tension that invites resolution. When done well, data storytelling taps into shared values, which helps create buy-in.\nExamples included in the video show how impactful good data storytelling can be:\n\nIn Toronto, 3D visualizations of cell phone data captured shifts in downtown activity before, during, and after the pandemic, offering a clear story of disruption and gradual recovery.\nAfter the wildfire in Paradise, California, a relocation map illustrated how most displaced families stayed close to home, reinforcing a universal need for safety and connection.\nEmotional power can also come from how data is visualized: the firm Periscopic‚Äôs portrayal of gun deaths as ‚Äúyears of life lost‚Äù reframes statistics into a story about deeply human loss.\nInteractive tools like the Urban Institute‚Äôs prenatal care equity model go even further, showing the difference between our status quo and a truly equitable future.",
    "crumbs": [
      "Urban Data Storytelling",
      "The importance of urban data storytelling"
    ]
  },
  {
    "objectID": "urban-data-storytelling/urban-data-storytelling-importance/urban-data-storytelling-importance.html#steps-to-building-your-urban-data-story",
    "href": "urban-data-storytelling/urban-data-storytelling-importance/urban-data-storytelling-importance.html#steps-to-building-your-urban-data-story",
    "title": "The importance of urban data storytelling",
    "section": "Steps to building your urban data story",
    "text": "Steps to building your urban data story\nTo build an effective data story, it is important to always start with identifying your goals and your audience (Step A). From there, it is often an iterative process between crafting your specific narrative (Step B) and analyzing your data (Step C). You may go back-and-forth between these two general steps many times before finally bringing it all together into your final product or deliverable (Step D).\n\nA. Identify your goals\n\nIdentify your project goals. Thinking about your specific project, identify what you are aiming to achieve in the short term, medium term, long term in relation to your specific project.\nMap your stakeholders and their positions. Identify the people or organizations who have an interest in your work, or who you wish had a stake in your work. What are their interests, and how do these interests relate to your project goals? How involved (or uninvolved) are each of these parties? What is your timeline for involving/engaging them?\nPrioritize an audience for your story. From your list of stakeholders, identify which specific audience you want to engage with for this story, and why.\n\n\n\nB. Craft your data story\n\nIdentify the type of story that is right for your goals. Figure out what you need to do to get yourself, your organization, and your stakeholders from where you are now to the goal post that you set for yourself. Do you need to tell a descriptive story, a persuasive story, x story, or y story?\nBuild a story outline that creates shared ground with your audience. Consider how to build a story that creates shared ground with your audience, and that reflects and taps into your audience‚Äôs interests. What findings and metrics would connect to or speak to their values or tap into their interests? How can you look for opportunities or assets, and offer up forward-looking solutions?\nBrainstorm your key metrics. Without being overly concerned just yet with data access and availability, brainstorm the key metrics that would be best suited to tell your story. These could be metrics that validate your success, highlight a need or issue to address, demonstrate the effectiveness of an intervention, etc.\nDraft an actionable takeaway for your audience. Think about the final messaging or call to action you want to leave with your audience. What is your main takeaway and what specific action(s) do you want your audience to take?\n\n\n\nC. Work on your data\n\nSelect and narrow down your data sources. Now that you have drafted a story arc and brainstormed key metrics, it is time to investigate and take stock of the data resources that are available to you now. Explore what datasets and variables are available from publicly available sources, third party proprietary data, or any other internal data you or your stakeholders might have access to. Narrow down a short list of accessible datasets and/or variables which you will use for your analysis and visualization.\nUnderstand ethical issues related to your data. Before proceeding with data processing and analysis, take the time to understand your datasets. How was this data collected, by whom, when, and why? Are there any confidentiality concerns? Are there any biases or limitations you should be aware of with this dataset?\nQuery, process, and analyze your data. Now is the time to dig into your dataset to understand and uncover trends, patterns, and findings for your story. Using spreadsheet software, geographic information system (GIS) software, and/or coding, start cleaning, querying, and processing your data. Explore your data by creating basic summary statistics, pivot tables, maps, etc. Identify the key findings from your data.\nCreate data visualizations. Now that you have analyzed your data and decided what findings you want to highlight, create your data visualizations (charts and/or maps) and apply the principles of effective data visualization (uncluttered visuals, cohesive colors and fonts, etc.)\n\nCreate non-spatial data visualizations\nCreate spatial data visualizations\nApply principles of effective data visualization (aesthetics, graphic design)\n\n\n\n\nD. Bring it all together\n\nWrap up your final deliverable. Discuss and iterate your data insights with your team and partners. Integrate your data visualizations within your narrative to bring together your final story. Finalize your story in the form of the final deliverable of your choosing - this could be a story map, webpage, slide deck, report, etc.\nCommunicate using your final deliverable. Circling back to steps 1-12, implement tips for effective visual deliverables (e.g.¬†reduce visual clutter, minimize words on slides, etc.) and, if applicable, apply tips for effective verbal presentations (e.g.¬†tone, voice, etc.)\n\n\n\n\n\nFramework for data storytelling",
    "crumbs": [
      "Urban Data Storytelling",
      "The importance of urban data storytelling"
    ]
  },
  {
    "objectID": "urban-data-visualization/data-visualization/data-visualization.html",
    "href": "urban-data-visualization/data-visualization/data-visualization.html",
    "title": "Data visualization",
    "section": "",
    "text": "Isabeaux Graham, Jeff Allen\nData visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools help people understand patterns, trends, and outliers in data. At its core, data visualization translates abstract numbers into something visible and intuitive, helping our audiences better understand what the data is telling us.\nWe live in a world increasingly shaped by data ‚Äì from climate change and public health to business performance and social media behavior ‚Äì however, raw numbers alone are difficult to interpret. Visualizations help bridge that gap by making data more accessible, understandable, and actionable. It‚Äôs not just about making things look good, it‚Äôs about making data make sense.\n‚ÄúThe simple graph has brought more information to the data analyst‚Äôs mind than any other device. It specializes in providing indications of unexpected phenomena.‚Äù ‚Äì John Tukey",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/data-visualization/data-visualization.html#why-visualize-data",
    "href": "urban-data-visualization/data-visualization/data-visualization.html#why-visualize-data",
    "title": "Data visualization",
    "section": "Why visualize data?",
    "text": "Why visualize data?\nWe visualize data to help us understand it better. When we turn numbers into pictures like charts or graphs, it becomes easier to see patterns, trends, or problems. Broadly speaking, visualization can serve two major goals:\n\nExploration ‚Äì Using charts to help make sense of the data. It helps you find interesting things or answer questions\nCommunication ‚Äì Using visuals to help other people understand what you found in the data\n\nThese goals are not mutually exclusive. The best data work is iterative. We often begin by visualizing data to explore patterns, test ideas, and uncover insights. Once we have made sense of the data ourselves, we can use visualizations to communicate key findings, tell a compelling story, or make a case for action. Whether the goal is understanding, persuasion, or advocacy, good data visualization helps bridge the gap between raw information and meaningful insight.\nVisualizing data is ultimately about communication and striking the right balance between analysis, design, and narrative. To share data in a clear and meaningful way, it helps to think through this data storytelling framework:\n\n\n\nFramework for data storytelling\n\n\n\nData Analysis: What does the data tell us? What patterns or relationships matter?\nVisualization: How can this data be represented graphically to communicate findings effectively?\nNarrative: What story does the data tell? What‚Äôs your argument or takeaway? Who is your target audience?\n\nThis framework helps situate visualization as part of a broader storytelling or decision-making process. Whether you are creating a data story with multiple visuals or a single chart for a report, it is important to keep all of these things in mind. The purpose behind your visualization determines everything from the chart type you use to the level of detail, annotation, and tone you adopt.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-exploratory-analysis",
    "href": "urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-exploratory-analysis",
    "title": "Data visualization",
    "section": "Data visualization for exploratory analysis",
    "text": "Data visualization for exploratory analysis\nBefore we share our findings with others, we often need to make sense of the data ourselves. This is where exploratory data analysis comes in. This is the process of visually investigating datasets to uncover trends, spot anomalies, test hypotheses, and surface insights that might otherwise be hidden in raw numbers or tables.\nVisualization during this stage can reveal unexpected patterns and outliers, correlations and clusters, distribution shapes, and data quality issues.\nIt also helps uncover relationships that summary statistics alone might obscure. The exploratory process is typically iterative and often messy, with charts not needing to be beautiful, but instead, informative. Quick plots, heatmaps, and scatterplots are all useful tools in this phase, enabling a deeper understanding of the data before any formal analysis or presentation.\nEven datasets with identical statistical properties can convey vastly different stories when visualized. A perfect example of this is Anscombe‚Äôs Quartet, a set of four datasets that have nearly identical means, variances, and correlation coefficients, yet their graphical representations reveal dramatic differences. This illustrates the importance of visualizing data, rather than solely relying on summary statistics, as the true patterns and relationships may be hidden within the numbers themselves.\n\n\n\nAnsecombe‚Äôs Quartet, for datasets with almost identical summary statistics, but widely different patterns when charted",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-communication-and-storytelling",
    "href": "urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-communication-and-storytelling",
    "title": "Data visualization",
    "section": "Data visualization for communication and storytelling",
    "text": "Data visualization for communication and storytelling\nOnce we‚Äôve explored the data and identified key findings, the next step is to share these insights with others. Data visualization becomes a powerful tool for communication, helping us inform, engage, and persuade different audiences. In this context, visualization is not just about showing data, it‚Äôs about shaping understanding and sometimes even inspiring action.\nData stories combine charts and text to walk the audience through a narrative arc. These stories often include:\n\nContext ‚Äì What‚Äôs the issue or question?\nData ‚Äì What patterns or evidence are we seeing?\nInsight ‚Äì Why does it matter?\nAction ‚Äì What should happen next?\n\nGood data visualizations and stories often zoom in and out to show both the big picture and key details while guiding the audience with clear visual hierarchy and annotations.\nIn advocacy contexts, data visualizations are used to support arguments, influence policy, and raise awareness. The visual design here should support the clarity and urgency of the message while still being transparent and truthful.\n\nDesigning for specific audiences\nNot all data visualizations are created for the same audience or communication goals. Some are meant to be read in detail (like charts in a policy report), while others are intended to create an emotional impact at a glance (like a billboard or social media graphic).\nFor example, a line chart depicting global temperatures over time provides nuance and precision, making it an excellent choice for scientists and analysts.\n\n\n\nGlobal Temperatures Over Time (Source: NASA/GISS)\n\n\nAlternatively, warming stripes, a minimalist visualization uses a simple color gradient to represent temperature changes. This approach communicates the same pattern but in a more striking and emotionally impactful way, making it highly effective for public engagement and advocacy.\n\n\n\nWarming strips",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/data-visualization/data-visualization.html#data-visualization-theory",
    "href": "urban-data-visualization/data-visualization/data-visualization.html#data-visualization-theory",
    "title": "Data visualization",
    "section": "Data visualization theory",
    "text": "Data visualization theory\nA well-designed visualization isn‚Äôt just about making data look good ‚Äì it‚Äôs about making it understandable, accurate, and accessible. This often requires a base understanding of how we encode data visually, how the human brain processes that information, and how to make charts function well across different audiences and mediums.\n\nVisual variables\nAt the heart of every chart or graph is visual encoding: the transformation of data into visual elements. Each variable in your dataset must be ‚Äúmapped‚Äù to a visual channel, often called a visual variable. The effectiveness of these visual encodings depends on what type of data you‚Äôre working with.\n\n\n\nBertin‚Äôs Visual Variables via Axis Maps\n\n\n\n\nPerception and cognition\nEffective data visualization leverages our brain‚Äôs innate ability to rapidly process certain visual elements. This phenomenon, known as preattentive processing, allows viewers to quickly and effortlessly identify certain patterns, contrasts, and structures without conscious effort.\nThis allows us to instantly spot outliers, notice trends and clusters, and recognize visual hierarchies.\nKey preattentive features include color, form, orientation, and size. For example, we can quickly distinguish a red circle among blue circles (color) and identify a square amidst a group of circles (form). These features are processed so swiftly and effortlessly that they can influence our perception and understanding of visual information without conscious effort.\n\n\n\nExample of preattentive processing. Source: www.csc2.ncsu.edu/faculty/healey/PP/\n\n\nEven when designing a table for a slide deck or a report, you can use the theory of preattentive processing to pick a visual variable to focus your readers attention on specific data points that are important to your story.\n\n\n\nExample of preattentive processing to highlight rows in a table (Source: CNN)\n\n\nVia preattentive processing, we can guide viewers‚Äô attention efficiently, enhance comprehension, and reduce cognitive load.\n\n\nPerceptual Rankings\nPerceptual Rankings help explain why some encodings are easier to interpret than others. Not all visual variables are equally effective.\nYou may have noticed that in the scatter-plot example in the previous section, that it is generally easier to pick out a red circle among a group of blue circles rather than a blue square among a group of blue circles.\nResearch has established a hierarchy of visual channels based on their accuracy and ease of interpretation:\n\n\n\nPerceptual Rankings (Source: Visualization Analysis and Design by Tamara Munzner)\n\n\nUnderstanding these perceptual principles ensures that visualizations communicate information clearly and intuitively, aligning with how viewers naturally process visual stimuli.\nFor example, based on this research, position on a common scale is easier to interpret than size or area for quantitative data. Comparing volumes is also relatively difficult. This leads to two recommendations‚Ä¶\n\nAvoid pie charts (especially for more 3 categories) since they can be less effective than alternatives (e.g.¬†bar charts)\nAvoid 3D charts in most cases because they tend to over-complicate, be difficult to read, and add extra visual clutter compared to 2D alternatives\n\n\n\nVisual accessibility\nYour data visualizations should be readable by everyone, which means thinking beyond aesthetics and into the realm of inclusive design.\nFor charts and maps, there are two main things to consider\n\nUsing colourblind safe colours. Tools, such as ColorBrewer can be used to pick safe colours. There are also tools for uploading images (e.g.¬†of a chart or a map), and test how it is viewed for different types of colourblindness. Highly recommend using this for any graphics that will appear in publications\nUsing adequate font sizes and making sure there is adequate level of contrast between foreground and background, especially with text. The greater the contrast between the foreground and background, the easier it is to read. For text, keep in mind that font size plays a significant role: smaller fonts require even higher contrast between the text color and background to maintain readability. There are online tools for checking contrast ratios between background and foreground that are useful for design.\n\n\n\n\nExamples of different text on a background at different contrast ratios",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/data-visualization/data-visualization.html#chart-components",
    "href": "urban-data-visualization/data-visualization/data-visualization.html#chart-components",
    "title": "Data visualization",
    "section": "Chart components",
    "text": "Chart components\nStrong visualizations depend not just on the data and encodings, but on thoughtful framing. The supporting elements of a chart provide clarity and context.\nThinking about and directly designing each of the different components of a chart, rather than just using the defaults of a software (e.g.¬†what a chart in Excel or a Python library will give you initially), will go a long way in making effective graphics. The graphic below shows the components commonly used when constructing a chart. Try to think about the design of each of these components in your own charts. If it doesn‚Äôt clarify the data and story, it probably distracts, and it‚Äôs probably best to either remove or de-emphasize it.\n\n\n\nChart components (Source: data.europa.eu/apps/data-visualisation-guide)",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/data-visualization/data-visualization.html#practical-tips-for-effective-data-visualization",
    "href": "urban-data-visualization/data-visualization/data-visualization.html#practical-tips-for-effective-data-visualization",
    "title": "Data visualization",
    "section": "Practical tips for effective data visualization",
    "text": "Practical tips for effective data visualization\nA strong visualization can make complex data accessible; however, without care, the message can get lost. Great visualizations balance clarity, precision, and aesthetics. This section offers practical tips and guiding principles to elevate your data visualizations.\nNote that these are general recommendations and rules of thumb, not rules that you must follow 100% of the time! Data visualization is a combination of technical, design, and artistic skills, and there are often exceptions to the rules :)\n\nHierarchy in graphics and stories\nGuide the viewer‚Äôs eye by creating a hierarchy between background and foreground. Your key message should be the focus and highlighted while everything else (axes, grid-lines, background elements) should support and not compete with it.\n\nBold or highlight key data points.\nUse size, contrast, and colour to signal importance.\nDe-emphasize secondary elements like gridlines, minor tick marks, or axis labels\n\nFor example, the line chart in the image above has a strong visual hierarchy between the key data points it wants to show (e.g.¬†the lines for Sweden, the E.U., and Ireland) relative to the rest of the chart components.\nWhen guiding your readers through a story with a series of visualizations, sometimes it is useful to follow a Data Visualization Sandwich metaphor:\n\nThis is very similar to journalistic styles of writing, ‚Äúdon‚Äôt bury the lead‚Äù and ‚Äúbottom line up-front‚Äù.\nLet‚Äôs look at an example. This is a map of the United States from an article by the Guardian on how the USA is facing above average rises in temperatures. This map acts as the Patty ‚Äì it draws the attention of the reader and introduces the topic of the article.\n\nThe article then takes a deeper look at specific counties that have experienced significant temperature increases via a table:\n\nFinally, this visual, which could be considered a patty or bun, helps illustrate the temperatures across U.S. states. It also includes a ‚Äútopping‚Äù ‚Äì an annotation over California that highlights how the entire population has experienced a temperature increase since 1895.\n\n\n\nDe-clutter and high data-ink ratio\nCoined by Edward Tufte, the data-ink ratio refers to the proportion of visual elements that represent actual data, rather than decoration, relative to all ‚Äòink‚Äô on the chart.\nIn other words, this is about reducing clutter and focusing graphical elements of a chart on the data.\n\n\n\nLeft: Low data-ink ratio / Right: High data-ink ratio\n\n\nHere are a few recommendations for reducing clutter and increasing data-ink ratios:\n\nRemove non-essential ‚Äúchartjunk‚Äù (3D effects, background shading, unnecessary gridlines, borders).\nUse direct labels instead of relying on legends.\nChoose simple chart types unless complexity is truly needed.\nUse subtle formatting of reference information like gridlines to keep the audience focused on the data itself.\nAvoid overuse of colors, labels, and gridlines.\nYou don‚Äôt need to visualize every possible variable or data point, focus on the story you‚Äôre trying to tell.\nGroup or collapse less important data to simplify interpretation.\nWhitespace is your friend\n\nHere‚Äôs an example of a visualization of COVID-19 deaths from The Guardian that has limited visual clutter and strong hierarchy.\n\n\n\nDesign for your output\nEffective visualization isn‚Äôt one-size-fits-all. Always consider where and how your work will be seen. Tailor your design choices to the medium and the audience:\n\nPrint vs digital: Pick fonts, line weights, and colors that are clear at the specific resolution and/or paper size that you are designing for. What works on a low-resolution screen may not translate well to paper.\nPresentations vs.¬†reports: Slides call for bold, minimal visuals with large text and fewer details. Reports allow for more complexity and written explanation.\nSocial media: Prioritize clarity at small sizes. Expect compression and low resolution and keep text minimal and legible.\nAudience expertise: Technical audiences may appreciate complex charts and granular data. Broader or non-expert audiences often benefit from simpler visuals, clear labeling, and guiding annotations.\n\nIf you design a chart to fit within a specific medium (e.g.¬†mobile view on a screen), it‚Äôs recommended you redesign the chart if you want to have it in a different context (e.g.¬†printed in a report). This is to prevent fonts being resized so they are no longer legible, graphics losing resolution and becoming fuzzy, or distorted if they are resized.\n\n\nBrand guidelines\nIf you‚Äôre creating visualizations within an organization or for a specific campaign, it‚Äôs important to follow established brand guidelines.\nThis includes using approved typefaces, colors, and logos, and aligning your visuals with the tone and messaging of the broader content.\nMaintaining visual consistency across all charts, graphics, and reports reinforces brand identity, leads to more cohesive products, and builds recognition with your audience.\n\n\nAnnotations\nGreat charts don‚Äôt always speak for themselves. Use clear titles, subtitles, and axis labels. Add callouts or annotations to highlight patterns or anomalies. You can guide the reader to your takeaway rather than leaving it to interpretation.\n\n\n\nExample of how simple text labels and titles can help tell an effective visual story (Source: Wall Street Journal 2015)\n\n\n\n\nChoosing charts based on your data\nMatch the visualization to the type of data and the message you‚Äôre trying to convey. Here‚Äôs a quick overview of very common charts for different types of data:\n\nChoosing the right chart type is essential to making your data clear, compelling, and truthful. While there‚Äôs no single ‚Äúcorrect‚Äù choice for every scenario, understanding your data and communication goals will help you select a format that highlights the story you want to tell. Whether you‚Äôre comparing categories, showing trends over time, exploring distributions, or revealing relationships, each chart type has strengths and limitations.\nFor a deeper dive into chart selection, visit From Data to Viz, a comprehensive, visual guide that helps you choose the most appropriate chart based on your data structure and communication goals.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html",
    "href": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html",
    "title": "Categorical dot maps",
    "section": "",
    "text": "Jeff Allen\nDot maps are super useful for mapping demographic categories.\nCheck out the map below. Each dot on the map represents 10 households in the City of Toronto, each are colored by their housing tenure - whether they rent or own, and if so, whether they live in subsidized housing or have a mortgage.\nThe benefit of dot maps like this, compared to areal choropleth maps, is that they can show both relative density (i.e., a greater concentration of households in one area compared to another) as well as relative proportions (e.g., more households renting than owning their home in one part of the city compared to another).\nThese maps also display well at multiple scales. We can examine general patterns across the entire city or zoom in on an area to identify local patterns.\nDot maps showing demographic and household data, like the one above, are often based on census data. Census data are usually publicly available pre-aggregated to spatial boundaries. In Canada, for instance, urban maps are often created using neighbourhood Census Tracts or smaller Dissemination Area boundaries.\nThe dots on demographic dot maps, like those shown above, are therefore not perfect representations of where people live; they are estimated. For example, in the map above, if we know that there are 100 households in subsidized housing in a neighbourhood, we generate 10 dots randomly placed within that neighbourhood. If we have available data, this process can be improved via dasymetric mapping, where we only place dots in locations where we think people live. For example, in the above map, we used a land-use layer to place dots only in residential zones rather than parks or industrial zones. Once we have this working for one neighbourhood, it is then repeated for all neighbourhoods and all categories we want to include on our map.\nIn this tutorial, we‚Äôre going to cover how to create dot maps like these using Python, mostly using geoPandas, with some final cartography in QGIS and Inkscape. The example data will be to replicate the map above, but the methods and code can be applied to anywhere with similar data.",
    "crumbs": [
      "Urban Data Visualization",
      "Categorical dot maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html#prerequisites",
    "href": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html#prerequisites",
    "title": "Categorical dot maps",
    "section": "Prerequisites",
    "text": "Prerequisites\nPrior knowledge of pandas, geopandas, QGIS, and Inkscape (or similar graphic design software) would be helpful for the following tutorial. Here are the links to download this notebook and data.\n\nJupyter notebook\nDatasets\n\nIf you are running the notebook and/or script locally (generally recommended), you will need to use the following libraries. You‚Äôll have to install them (e.g.¬†via pip or conda) if you do not have them installed already.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport random\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Urban Data Visualization",
      "Categorical dot maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html#data-loading-and-processing",
    "href": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html#data-loading-and-processing",
    "title": "Categorical dot maps",
    "section": "Data loading and processing",
    "text": "Data loading and processing\nWe‚Äôre going to replicate the map of Toronto shown at the top of this page. I‚Äôve pre-filtered the source datasets for Toronto, and they are included in the download link above. The datasets we‚Äôll be using are: - Census Dissemination Areas (DA) Polygons - Census Data on Housing Tenure - Toronto Land Use Spatial Data\nAlso included are layers solely for cartographic purposes (i.e.¬†as reference layers on the final map) - Toronto Boundary Polygon from OpenStreetMap - Lake Ontario Polygon from OpenStreetMap - Major Trasnit Lines & Stops from Metrolinx\nTo get started, let‚Äôs load the census data:\n\nda = gpd.read_file(\"data/toronto-da-2021.geojson\")\ndft = pd.read_csv(\"data/toronto-tenure-da-2021.csv\")\ndft.fillna(0, inplace=True)\n\nThe GeoDataFrame, called da, represents the spatial boundaries of each Dissemination Area (DA). The dft DataFrame contains a table of the number of households in each DA that either rent or own their home. We can join these two based on their unique identifier, DAUID.\n\ndft[\"DAUID\"] = dft[\"DAUID\"].astype('str')\nda = da.merge(dft, how='left', on='DAUID')\nda.head(5)\n\n\n\n\n\n\n\n\nDAUID\nDGUID\nLANDAREA\nPRUID\ngeometry\nOwner\nRenter\nOwner_with_mortgage\nRenter_in_subsidized_housing\n\n\n\n\n0\n35200002\n2021S051235200002\n0.0504\n35\nMULTIPOLYGON (((-79.20270 43.82367, -79.20228 ...\n95.0\n10.0\n50\n0\n\n\n1\n35200003\n2021S051235200003\n0.0468\n35\nMULTIPOLYGON (((-79.20080 43.81978, -79.20129 ...\n70.0\n15.0\n51\n0\n\n\n2\n35200004\n2021S051235200004\n0.0488\n35\nMULTIPOLYGON (((-79.20522 43.81890, -79.20524 ...\n100.0\n0.0\n57\n0\n\n\n3\n35200005\n2021S051235200005\n0.0443\n35\nMULTIPOLYGON (((-79.20414 43.81748, -79.20447 ...\n90.0\n15.0\n50\n0\n\n\n4\n35200006\n2021S051235200006\n0.0572\n35\nMULTIPOLYGON (((-79.19962 43.81726, -79.20026 ...\n120.0\n15.0\n72\n0\n\n\n\n\n\n\n\nLet‚Äôs calculate a column containing the total number of households in each DA. Then create two additional columns: one for households that own their home without a mortgage, and another for those that rent their home without living in subsidized housing.\n\nda[\"Total\"] = da[\"Owner\"] + da[\"Renter\"]\nda[\"Owner_no_mortgage\"] = da[\"Owner\"] - da[\"Owner_with_mortgage\"]\nda[\"Renter_not_in_subsidized_housing\"] = da[\"Renter\"] - da[\"Renter_in_subsidized_housing\"]\n\nWe now have four categories that add up to the total number of households in a DA - Owners with a mortgage - Owners without a mortgage - Renters in subsidized housing - Renters not in subsidized housing\nLet‚Äôs make a choropleth map of the percent of each using the same classification scheme to explore and compare. We‚Äôre also plotting in yellow DAs that have a Total = 0 (i.e.¬†those that return a null when dividing by 0).\n\nfig, ax = plt.subplots(ncols=2, nrows=2, figsize=(9,6))\n\nvariables = [\n    \"Owner_with_mortgage\", \n    \"Owner_no_mortgage\", \n    \"Renter_not_in_subsidized_housing\", \n    \"Renter_in_subsidized_housing\"\n]\ntitles = [\n    \"% of households that own with a mortgage\", \n    \"% of households that own without a mortgage\", \n    \"% of households that rent not in subsidized housing\", \n    \"% of households that rent in subsidized housing\"\n]\n\nfor i, v in enumerate(variables):\n    # background border and color (to highlight 'no data' values)\n    da.plot(\n        edgecolor = \"#c2c2c2\",\n        color = \"#F1C500\",\n        linewidth = 1,\n        ax = ax[int(i / 2), i % 2]\n    );\n    # DA layer shaded by percent of households for each category\n    da.plot(\n        column = 100 * da[v] / da[\"Total\"], \n        cmap = 'Greys', \n        legend = True,\n        ax=ax[int(i / 2), i % 2],\n        scheme='user_defined',\n        classification_kwds=dict(bins=[20, 40, 60, 80, 100]),\n        legend_kwds = {\n            \"loc\": \"lower right\",\n            \"fontsize\": 6,\n            \"title\": \"Percent\",\n            # \"alignment\": \"left\",\n            \"reverse\": True\n        }\n    ).set_axis_off()\n    # sub-plot titles\n    ax[int(i / 2), i % 2].set_title(\n        titles[i], \n        fontsize=10, \n        loc='left'\n    )\nplt.tight_layout()\n\n\n\n\n\n\n\n\nFrom the choropleth maps, we can already identify some patterns. Renting is more common in downtown areas and some suburban nodes, while subsidized renting is highly concentrated in a few Dissemination Areas. Owning, on the other hand, is more evenly distributed, but those who own their homes without a mortgage tend to live in more central locations.\nThese choropleth maps are a good start, but they have some limitations:\n\nThey only show the percent (i.e.¬†rate) of each tenure type, not the density. Areas of the same value on the maps above may have very different concentrations of dwellings.\nIn order to look at all four variables, we needed to make 4 separate plots. These are useful for overall comparisons, but it‚Äôs difficult to compare specific neighbourhoods.\nThe concentration of dwellings probably varies across each DA polygon. Some of these DAs are quite large, but mostly consist of non-residential land (e.g.¬†parks, industrial, etc.), so shading the entire DA can be a bit misleading in terms of where households are.\nA few of the DAs have 0 population. We coloured these as an extra category to our map (yellow), but this can distract a bit from the overall story.",
    "crumbs": [
      "Urban Data Visualization",
      "Categorical dot maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html#categorical-dot-maps",
    "href": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html#categorical-dot-maps",
    "title": "Categorical dot maps",
    "section": "Categorical dot maps",
    "text": "Categorical dot maps\nOne solution to the above-noted issues are categorical dot maps. These maps can show the relative proportions and densities among one or more categories.\nThe first step in creating a dot map like the above is to decide on a rate of households per dot, and then calculate the number of dots to generate per area based on this rate and the number of households in the area. For example, if there are 200 households in a DA, and the rate is 10 households per dot, then we would generate 20 dots in the DA.\nWe can do this pretty simply as shown below, using the variable d to represent the rate of households per dot.\nNote that it can be difficult to initially judge what this rate, d, should be. This may vary depending on the intended size of your map as well as the spatial distribution of what you‚Äôre trying to map. Deciding on a dot rate often requires some iteration in generating the dots, looking at the output on a map, and then re-generating them if need be. (I initially started with d = 20, but decided to reduce since I thought the map was too sparse).\n\nd = 10\nda[\"Owner_with_mortgage_dots\"] = da[\"Owner_with_mortgage\"] / d\nda[\"Owner_no_mortgage_dots\"] = da[\"Owner_no_mortgage\"] / d\nda[\"Renter_in_subsidized_housing_dots\"] = da[\"Renter_in_subsidized_housing\"] / d\nda[\"Renter_not_in_subsidized_housing_dots\"] = da[\"Renter_not_in_subsidized_housing\"] / d\n\nThe second step is generating dots that we can plot on a map. For each DA, we generate _ random dots, based on the numbers calculated above. Below is a schematic example.\nThe simplest approach is on the left, placing dots randomly throughout the DA polygon.\nOn the right is a dasymetric (dasy = dense, metric = measure) approach, where we use another dataset, such as land-use data, to clip out non-residential areas prior to generating dots. For example, on the right, we‚Äôre only placing dots in the areas that are not retail or parks. This provides better estimation that the dots are located where households actually live.\n\nLet‚Äôs try to dasymetrically generate dots for every DA in Toronto!\nFor Toronto, we fortunately have a dataset on land-use that we can use to classify areas as ‚Äúresidential‚Äù (i.e.¬†candidate areas for placing dots), and ‚Äúnon-residential‚Äù. Let‚Äôs load and quickly plot the data:\n\nlu = gpd.read_file(\"data/toronto-land-use-2022-sp.shp\")\n\nfig, ax = plt.subplots(figsize=(8,7))\n\nlu.plot(\n    column = \"Class_name\", \n    categorical = True,\n    legend = True,\n    edgecolor = None,\n    ax=ax,\n    legend_kwds = {\n        \"loc\": \"lower right\",\n        \"fontsize\": 7,\n        \"title\": \"Land Use\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    }\n).set_axis_off();\n\n\n\n\n\n\n\n\nResidential are classified as [\"MixedUse\", \"Neighbourhoods\", \"ApartmentNeighbourhoods\"]. Let‚Äôs query out these layers and save to a new GeoDataFrame.\n\nres_classes = [\"MixedUse\", \"Neighbourhoods\", \"ApartmentNeighbourhoods\"]\nres = lu[lu['Class_name'].isin(res_classes)]\n\nNow let‚Äôs cut out the non-residential zones from the Dissemination Area polygons. We can think of this operation like a cookie cutter.\nThis is a two-step process in geopandas. First we use the overlay function to select out only the residential areas from the DAs. Then we dissolve the geometries (i.e.¬†a spatial group-by) by the DAUID since the instresection can lead to several different features with the same DAUID. This happens when two non-touching residential zones spatially overlap with a single Dissemination Area.\n\ngdf = gpd.overlay(da, res, how='intersection').dissolve(by = \"DAUID\")\n\nHere‚Äôs the result for a few DAs. The clipped residential area in yellow, overlaid by the original DA geometries.\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\ngdf.plot(\n    color = \"#F1C500\",\n    linewidth = 0,\n    ax = ax\n)\nda.plot(\n    facecolor = 'none',\n    linewidth = 0.6,\n    edgecolor = \"#343434\",\n    ax = ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nNow let‚Äôs generate some dots! Here‚Äôs a function that takes an input geometry and returns a random point inside of it. Specifically, this function generates a random point within the bounding box of the polygon, checks if it‚Äôs contained by the polygon, if so, returns it, but if not, continues to generate points until this condition is met.\n\ndef gen_dot(polygon):\n    while True:\n        pt = Point(random.uniform(polygon.bounds[0], polygon.bounds[2]), random.uniform(polygon.bounds[1], polygon.bounds[3]))\n        if (polygon.contains(pt)==True):\n            points = [pt.x,pt.y]\n            break\n    return points\n\nLet‚Äôs apply this to loop over our clipped DA dataset, generating all of our dots! This might take a few seconds, for me it took less than a minute.\nAt the end of this cell, we‚Äôre also saving the dots as a .geojson file so we can load it later, either in this same notebook or elsewhere (e.g.¬†in QGIS).\n\ngdf\n\n\n\n\n\n\n\n\ngeometry\nDGUID\nLANDAREA\nPRUID\nOwner\nRenter\nOwner_with_mortgage\nRenter_in_subsidized_housing\nTotal\nOwner_no_mortgage\nRenter_not_in_subsidized_housing\nOwner_with_mortgage_dots\nOwner_no_mortgage_dots\nRenter_in_subsidized_housing_dots\nRenter_not_in_subsidized_housing_dots\nClass_name\n\n\nDAUID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n35200002\nMULTIPOLYGON (((-79.20544 43.82228, -79.20546 ...\n2021S051235200002\n0.0504\n35\n95.0\n10.0\n50\n0\n105.0\n45.0\n10.0\n5.0\n4.5\n0.0\n1.0\nNeighbourhoods\n\n\n35200003\nPOLYGON ((-79.20144 43.82048, -79.20145 43.820...\n2021S051235200003\n0.0468\n35\n70.0\n15.0\n51\n0\n85.0\n19.0\n15.0\n5.1\n1.9\n0.0\n1.5\nNeighbourhoods\n\n\n35200004\nMULTIPOLYGON (((-79.20326 43.82005, -79.20325 ...\n2021S051235200004\n0.0488\n35\n100.0\n0.0\n57\n0\n100.0\n43.0\n0.0\n5.7\n4.3\n0.0\n0.0\nNeighbourhoods\n\n\n35200005\nMULTIPOLYGON (((-79.20222 43.81780, -79.20229 ...\n2021S051235200005\n0.0443\n35\n90.0\n15.0\n50\n0\n105.0\n40.0\n15.0\n5.0\n4.0\n0.0\n1.5\nNeighbourhoods\n\n\n35200006\nPOLYGON ((-79.20101 43.81886, -79.20102 43.818...\n2021S051235200006\n0.0572\n35\n120.0\n15.0\n72\n0\n135.0\n48.0\n15.0\n7.2\n4.8\n0.0\n1.5\nNeighbourhoods\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35205065\nMULTIPOLYGON (((-79.42450 43.64006, -79.42486 ...\n2021S051235205065\n0.0441\n35\n375.0\n625.0\n350\n0\n1000.0\n25.0\n625.0\n35.0\n2.5\n0.0\n62.5\nMixedUse\n\n\n35205066\nPOLYGON ((-79.48814 43.61556, -79.48807 43.615...\n2021S051235205066\n0.0871\n35\n15.0\n1030.0\n0\n30\n1045.0\n15.0\n1000.0\n0.0\n1.5\n3.0\n100.0\nApartmentNeighbourhoods\n\n\n35205067\nPOLYGON ((-79.48638 43.61862, -79.48636 43.618...\n2021S051235205067\n0.3676\n35\n970.0\n245.0\n390\n0\n1215.0\n580.0\n245.0\n39.0\n58.0\n0.0\n24.5\nApartmentNeighbourhoods\n\n\n35205068\nMULTIPOLYGON (((-79.49417 43.61705, -79.49421 ...\n2021S051235205068\n0.2857\n35\n345.0\n150.0\n192\n0\n495.0\n153.0\n150.0\n19.2\n15.3\n0.0\n15.0\nMixedUse\n\n\n35205069\nPOLYGON ((-79.48435 43.62147, -79.48448 43.621...\n2021S051235205069\n0.0184\n35\n480.0\n305.0\n380\n0\n785.0\n100.0\n305.0\n38.0\n10.0\n0.0\n30.5\nMixedUse\n\n\n\n\n3736 rows √ó 16 columns\n\n\n\n\n%%time\n\nvariables = [\n    \"Owner_with_mortgage\", \n    \"Owner_no_mortgage\", \n    \"Renter_not_in_subsidized_housing\", \n    \"Renter_in_subsidized_housing\"\n]\n\noutput = []\n\nfor index, row in gdf.iterrows():\n    for var in variables:\n        n = round(row[var + \"_dots\"])\n        i = 0\n        while i &lt; n:\n            dot = gen_dot(row[\"geometry\"])\n            output.append([var,dot[0],dot[1]])\n            i += 1\n\nCPU times: user 39 s, sys: 26.8 ms, total: 39 s\nWall time: 39.3 s\n\n\n\n# converting the output to a geodataframe\ndots = pd.DataFrame(output, columns = [\"type\",\"x\",\"y\"])\n\n\ndots\n\n\n\n\n\n\n\n\ntype\nx\ny\n\n\n\n\n0\nOwner_with_mortgage\n-79.203825\n43.822403\n\n\n1\nOwner_with_mortgage\n-79.202771\n43.823234\n\n\n2\nOwner_with_mortgage\n-79.203576\n43.823427\n\n\n3\nOwner_with_mortgage\n-79.204661\n43.822466\n\n\n4\nOwner_with_mortgage\n-79.204534\n43.821935\n\n\n...\n...\n...\n...\n\n\n115978\nRenter_not_in_subsidized_housing\n-79.482849\n43.621655\n\n\n115979\nRenter_not_in_subsidized_housing\n-79.483320\n43.621486\n\n\n115980\nRenter_not_in_subsidized_housing\n-79.484118\n43.621624\n\n\n115981\nRenter_not_in_subsidized_housing\n-79.483111\n43.620920\n\n\n115982\nRenter_not_in_subsidized_housing\n-79.483444\n43.621202\n\n\n\n\n115983 rows √ó 3 columns\n\n\n\n\ngeometry = [Point(xy) for xy in zip(dots['x'], dots['y'])]\ndots = gpd.GeoDataFrame(dots, geometry=geometry)\ndots = dots[[\"type\",\"geometry\"]].set_crs('epsg:4326')\ndots.to_file(\"data/dots.geojson\", driver=\"GeoJSON\")\n\nGreat! Now let‚Äôs plot these dots on a map to see if it worked properly!\n\nfig, ax = plt.subplots(figsize=(10,7))\n\n# background DA polygon layer\nda.plot(\n    facecolor = 'none',\n    linewidth = 0.6,\n    edgecolor = \"#e9e9e9\",\n    ax = ax\n)\n\n# dot layer coloured by each category\ndots.plot(\n    ax = ax,\n    column='type',\n    categorical=True, \n    legend=True,\n    cmap = 'Accent',\n    markersize = 0.02,\n    legend_kwds = {\n        \"loc\": \"lower right\",\n        \"fontsize\": 7,\n        \"alignment\": \"left\"\n    }\n).set_axis_off()",
    "crumbs": [
      "Urban Data Visualization",
      "Categorical dot maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html#cartographic-details",
    "href": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html#cartographic-details",
    "title": "Categorical dot maps",
    "section": "Cartographic details",
    "text": "Cartographic details\nOkay! So the above worked pretty well, but the map isn‚Äôt great, and could use some additional context and layout items.\nFrom here, let‚Äôs pivot over to trying to make the map prettier and more useful in QGIS and Inkscape.\n(Note that we could probably do most of the following in Python, but my personal preference is to tweak the symbology of layers via a QGIS rather than in code).\nMy workflow for designing ‚Äòstatic‚Äô maps with larger datasets is as follows:\n\nvisualize the data layers in QGIS\nexport it as a high-resolution raster image .png (we could export it as a vector .svg, but would like result in a large file size difficult to load in Inkscape)\nopen this .png in Inkscape, and add additional layout items (e.g.¬†title, legend, north arrow, etc.)\n\nBelow is a screenshot of the dot layer, as well as the other reference layers, loaded into QGIS. The QGIS project file .qgis is in the download zip at toronto-housing-dot-map.qgz. In QGIS, I loaded the layers and then made some tweaks to try to improve the map‚Äôs legibility and aesthetics.\n\nI‚Äôve overlaid major transit routes (TTC) for general reference and orientation.\nI‚Äôve added a boundary polygon for the City of Toronto to better distinguish the study area.\nThe dots are colours are specifically chosen for their contrast.\nThere are several ‚Äòbackground‚Äô layers added to the map (land use, DA boundaries, Lake Ontario). These provide some geographic reference, but the colours are chosen to be more nuanced and not distract from the main layer of interest (the dots).\nI‚Äôve rotated the map ~17 degrees. This creates a better ‚Äúfit‚Äù of the geography onto the rectangular page by omitting empty ‚Äúwhite‚Äù space.\n\n\nIf we like what we have, we can export it and open it in Inkscape to add a few layout items. It‚Äôs important to export the map in exactly the same dimensions that we want it to be in our final layout (i.e.¬†so we aren‚Äôt resizing it in Inkscape which can cause distortion or resolution change).\nFor this map, I exported it at 10 inches wide by 6 inches tall. This would fit nicely on a landscape-oriented letter page with a margin, but also view well online on most screens.\nBelow is a screenshot of the map in Inkscape, with each of the layout items selected. Rotating the map 17 degrees has allowed for more efficient use of space. The legend fits nicely in the bottom right corner and there is little white space elsewhere.\n\nAs part of the legend, I also wanted to include a mini-chart showing the distribution of each of the four tenure-types shown on the map.\nI wanted relatively unique labelling on this chart, showing the percent of rent and own on one side, and the sub-categories on the other. This is a simple chart, but with very specific labelling, so I decided to create this ‚Äúby hand‚Äù, drawing the boxes and labels myself. To get the percentages, I quickly plopped out the block of code below, and then manually drew the rectangles to be in proportion to each number.\n\nfor v in variables:\n    print(v, da[v].sum(), 100 * da[v].sum() / da[\"Total\"].sum())\n\nOwner_with_mortgage 339776 29.282451695192787\nOwner_no_mortgage 262904.0 22.65749694055191\nRenter_not_in_subsidized_housing 484509.0 41.75577847872175\nRenter_in_subsidized_housing 73151 6.304272885533551\n\n\nHere‚Äôs the final map! exported at 300dpi.",
    "crumbs": [
      "Urban Data Visualization",
      "Categorical dot maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html#final-thoughts",
    "href": "urban-data-visualization/categorical-dot-maps/categorical-random-dot-maps.html#final-thoughts",
    "title": "Categorical dot maps",
    "section": "Final thoughts",
    "text": "Final thoughts\nOf course dot maps aren‚Äôt always the best option. They can be a bit difficult to parse out colours and patterns, especially once we add more and more categories. If you want to dig into the patterns of each specifically, i.e.¬†are less concerned about the comparisons, or are only interested in density OR proportions (rather than both at the same time), a series of individual choropleth or single-category dot density maps would be good to pursue as well. It‚Äôs rare to have too many maps :)\n\nWikipedia page on dot maps\nWikipedia page on dasymetric mapping",
    "crumbs": [
      "Urban Data Visualization",
      "Categorical dot maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/choropleth-maps/choropleth-maps.html",
    "href": "urban-data-visualization/choropleth-maps/choropleth-maps.html",
    "title": "Choropleth maps",
    "section": "",
    "text": "Jeff Allen\nChoropleth maps use color to show how a variable changes across geographic areas ‚Äî perfect for spotting patterns, trends, and regional differences at a glance. These are one of the most common types of thematic maps.\nHere are a couple examples.\nThis notebook will cover the base theory and show how you can make maps like these in QGIS",
    "crumbs": [
      "Urban Data Visualization",
      "Choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/choropleth-maps/choropleth-maps.html#colour-options",
    "href": "urban-data-visualization/choropleth-maps/choropleth-maps.html#colour-options",
    "title": "Choropleth maps",
    "section": "Colour options",
    "text": "Colour options\nThere are three main types of colour schemes for choropleth maps; categorical, sequential, and divergent; each are best for different types of data, summarized in the image below.\n\nWith categorical colour schemes, each unique value in a column is given a unique colour and symbol. There should be some thought given to what colours to pick for which categories, and whether to group multiple categories as one colour (e.g.¬†should parks and cemeteries be the same colour green?), but the steps to create a categorical map are straightforward. For example, in QGIS, right-click the layer, go to Properties, then Symbology, and then at the top there will be a dropdown where one of the options is Categorical. The below dropdown is the Value, which is the column in your data that you want to use to define the style of your data.\nWith sequential and divergent colour schemes, there are a couple extra design and classification steps to follow. The first is whether we want to group (or bin) our numeric data or whether we want a continuous colour scheme.\n\nWith ordinal data, having distinct colours is the default choice, but if your data are integers or floats, then you need to choose whether to represent continuously or to ‚Äúbin‚Äù the data. Here are quick benefits and drawbacks of each.\nContinuous choropleth (e.g.¬†smooth gradient):\n\nBest for showing gradual variation across space\nShows fine-grained variation, highlights subtle spatial patterns\nGreat for dense, relatively evenly distributed data\nAvoids arbitrary cutoffs\n\nGrouped choropleth (e.g.¬†classified into ranges)\n\nBest for simplifying or comparing groups\nEasier to interpret and compare regions\nUseful for highlighting thresholds (e.g.¬†income brackets), enables meaningful grouping or policy categories\nCleaner, more readable legends\nDownside are that cutoffs can skew legibility (e.g.¬†if a cutoff is at 20 and 40, a 21 and 39 are the same colour, but 19 and 21 are different)\n\nThe choice depends on your data, research goals, story, and audience.",
    "crumbs": [
      "Urban Data Visualization",
      "Choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/choropleth-maps/choropleth-maps.html#classifying-data",
    "href": "urban-data-visualization/choropleth-maps/choropleth-maps.html#classifying-data",
    "title": "Choropleth maps",
    "section": "Classifying data",
    "text": "Classifying data\nIf we choose to bin or group our numeric data, we have a further set of choices to make: how do we define the breaks between groups? The method used to define these breaks affects how patterns appear on the map. Here are some of the most common ways to define these breaks:\n1. Equal Interval\n\nDivides the data range into equal-sized intervals.\nPros: Simple to understand. Good when data is evenly distributed.\nCons: Can mislead if data is skewed. Many values may fall into a few groups and some might even be empty.\n\n|--   |---- |--- -|-----|-   -|\n0    20    40    60    80   100\n2. Quantiles\n\nEach group contains an equal number of data points.\nPros: Ensures each group is populated. Great for comparing relative rankings.\nCons: Class ranges can be uneven. Can exaggerate small differences.\n\n|--    ----| --- - --|-----   -|\n0         38        70       100\n3. Natural Breaks\n\nUses an algorithm (e.g.¬†Jenks) to find class breaks that best group similar values and maximize differences between classes.\nPros: Reflects natural groupings in data. Often produces intuitive classes.\nCons: Not consistent across datasets. Harder to explain to non-experts.\n\n|--  | ---- --- | - ------- | -|\n0    18        55          90  100\n4. Manual / Custom\n\nUser-defined class breaks based on domain knowledge or design reasons (e.g.¬†to have nice rounded numbers).\nPros: Tailored to specific needs. Can align with thresholds of interest. Can be easier to read.\nCons: Can introduce bias. Less replicable.",
    "crumbs": [
      "Urban Data Visualization",
      "Choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/choropleth-maps/choropleth-maps.html#choropleth-maps-in-qgis",
    "href": "urban-data-visualization/choropleth-maps/choropleth-maps.html#choropleth-maps-in-qgis",
    "title": "Choropleth maps",
    "section": "Choropleth maps in QGIS",
    "text": "Choropleth maps in QGIS\nClick here to download the data for this section\n\nWith vector data\nLet‚Äôs try to make a map in QGIS! We‚Äôll try to make a map of Toronto showing the percent of people who live in low-income households by neighbourhood relative to major transit lines.\nFirst, let‚Äôs look at the data. We have a polygon layer which represents census tracts. These data are created by Statistics Canada to share aggregated data from the Canadian census. They approximately correspond to neighbourhoods.\nWe have a .csv table which contains data linked to the unique identifier, ctuid, of each census tract (CTUID stands for Census Tract Unique ID) . We can use the ctuid to join this tabular data to the spatial boundaries of census tracts. When working with census data or many other data sources, it is quite common that our data do not come prepared as a single data set. We often have to join data from multiple tables or sources like this. (For more details about table joins, check out our data processing notebook).\nTo do the join, we first add the table as a layer into QGIS. Then open up the Properties of the census tract polygon layer, and go to Joins. Add a new join (the + at the bottom), using ctuid as the source and target fields. Once complete, we can open up the attribute table and see these additional columns.\nLastly, we also have a line layer representing major transit lines in Toronto (originally sourced from Metrolinx). We can categorize line data by status, using different colours or line types to display whether the transit route is existing or under construction.\nWe can now visualize the census tract polygons as a choropleth map (maps where polygons are shaded by numeric attribute values). Similar to the previous tutorial, open up the layer properties, go to Symbology, and style based on Graduated symbols.\n(Beware that a numeric column can sometimes get imported as a string in QGIS. To convert the string back to a number to visualize it, click on the Œµ on the top-right, and use the to_real() function to convert to a numeric value)\n\n\n\nOptions for colouring vector data in QGIS for % low-income data in Toronto\n\n\nHere‚Äôs the map with the transit lines and stops layered on top (with the transit lines classified categorically based on their Status).\n\n\n\nMap of Toronto showing % low-income in QGIS\n\n\nTry to update the map to use Natural Breaks or Equal Intervals. Also try out different colours!\nTry to update the map to use Natural Breaks or Equal Intervals (you can change the break types using the ‚ÄòMode‚Äô drop-down menu, or increase/decrease the total number of classes). Also try out different colours! Notice how the output changes when you do so.\nChoropleth maps are great for showing a rate or a density (in terms of people per area) or a statistic such as an average, but they are not always ideal for representing a count or total. For example, if we mapped just the total number of low-income residents by neighbourhood, it could exaggerate counts in larger areas relative to smaller areas.\n\n\nWith raster data\nLet‚Äôs now try to colour a raster layer to make a choropleth map. The example data we have is the same air pollution raster used to make the map of Taiwan at the top of this page.\nStart off by opening up this layer in QGIS. It will likely default to a white-to-black (i.e.¬†grayscale) colour scheme based on the values in each cell of the raster. This is already a choropleth map! But if we want to update the colours and the way the data are being classified, we can right click on the layer, go to Properties, then Symbology, and then pick Singleband pseudocolor. Once you have selected your options, click OK in the bottom-right.\n\n\n\nOptions for colouring raster data in QGIS\n\n\nHere we have lots of options to tinker with, including ‚Ä¶\n\nColor ramp, for selecting the colour gradient\nInterpolation, for whether we want continuous or discrete (i.e.¬†group) colours\nMode, for our classification scheme\n\nTry to update the map to show 5 equal interval groups and a blue-to-red divergent colour ramp.",
    "crumbs": [
      "Urban Data Visualization",
      "Choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/choropleth-maps/choropleth-maps.html#choropleth-maps-in-python",
    "href": "urban-data-visualization/choropleth-maps/choropleth-maps.html#choropleth-maps-in-python",
    "title": "Choropleth maps",
    "section": "Choropleth maps in Python",
    "text": "Choropleth maps in Python\nIf you‚Äôre interested in learning more about creating single-variable choropleth maps in Python, including how to classify data, check out the Geographic Data Science textbook‚Äôs chapter on the topic.\nIf you‚Äôre interested in creating bivariate choropleth maps, maps that show correlations between two variables like income and population density on the same map, check out our bivariate choropleth map tutorial\nYou can also refer to the geopandas.GeoDataFrame.plot documentation for more information on how to quickly visualize spatial data in Python.",
    "crumbs": [
      "Urban Data Visualization",
      "Choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/proportional-symbol-maps/proportional-symbol-maps.html",
    "href": "urban-data-visualization/proportional-symbol-maps/proportional-symbol-maps.html",
    "title": "Proportional symbol maps",
    "section": "",
    "text": "Jeff Allen\nProportional symbol maps are an intuitive way to visualize quantitative data on a map or visualization. Instead of relying on color gradients like choropleth maps, proportional symbol maps use symbols (most often circles) sized in direct proportion to the data they represent.\nThey are most often used for representing totals or counts (e.g.¬†larger symbols, greater totals).\nThey are typically applied to point data (e.g.¬†circles for transit station sized by ridership) or to polygons (e.g.¬†census tracts represented by a circle, that is sized based on its population)\nHere are a few examples\nPros of proportional symbol maps:\nCons of proportional symbol maps:",
    "crumbs": [
      "Urban Data Visualization",
      "Proportional symbol maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/proportional-symbol-maps/proportional-symbol-maps.html#on-how-to-size-symbols",
    "href": "urban-data-visualization/proportional-symbol-maps/proportional-symbol-maps.html#on-how-to-size-symbols",
    "title": "Proportional symbol maps",
    "section": "On how to size symbols",
    "text": "On how to size symbols\nWhen creating a proportional symbol map, one of the key decisions is how to size your symbols so they reflect the data meaningfully. There are two main choices you‚Äôll make:\n\nContinuous versus grouped symbol sizes\nContinuous sizing means each symbol is scaled exactly to match the data value it represents. For example, a location with 800 people might have a slightly smaller circle than one with 900.\nPros:\n\nPrecise, smooth visual differences.\nGood for showing nuanced variation in data.\n\nCons:\n\nSmall differences can be hard to detect.\nCan look cluttered or inconsistent, especially in dense urban areas.\n\n\n\n\nContinuous symbol sizes, size is based on specific point on a sliding scale\n\n\nGrouped sizing means values are binned into categories (e.g., low, medium, high), and each category gets a fixed symbol size. For example, circle sizes representing population could be grouped by 0-500, 500-1000, and 1000 and up, therefore locations with population 800 and 900 would have the same size.\nPros:\n\nEasier to read at a glance.\nCleaner visual presentation.\n\nCons:\n\nLess precise‚Äîsmall but important differences might be lost.\n\n\n\n\nGrouped symbol sizes, based on binning data into ranges\n\n\n\n\nArea versus diameter symbol sizes\nIf we scale our circles as a direct mathematical function of our data, we have a few options.\nScaling by diameter means the width of the symbol (the circle‚Äôs diameter) is proportional to the data value.\nPros:\n\nEasy to calculate.\nEasy to explain.\n\nCons:\n\nCan exaggerate large values (e.g.¬†doubling the diameter actually quadruples the area)\n\nScaling by area means the total surface area of the symbol (usually a circle) is proportional to the data value. This is the recommended approach because we perceive the area of a shape more accurately in relation to data.\nPros:\n\nTrue to the math‚Äîdoubling the value doubles the area.\nMore accurate visual representation of magnitude.\n\nCons:\n\nEven if scaling by area is better than scaling by diameter in most cases, some research has found that people tend to underestimate the size differences between circles. This can be addressed with Flannery scaling, which slightly inflates larger symbols in area-based scaling to compensate for how we see them.",
    "crumbs": [
      "Urban Data Visualization",
      "Proportional symbol maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/proportional-symbol-maps/proportional-symbol-maps.html#proportional-symbol-maps-in-qgis",
    "href": "urban-data-visualization/proportional-symbol-maps/proportional-symbol-maps.html#proportional-symbol-maps-in-qgis",
    "title": "Proportional symbol maps",
    "section": "Proportional symbol maps in QGIS",
    "text": "Proportional symbol maps in QGIS\nGreat! Let‚Äôs take a look at an example of how to make a proportional symbol map in QGIS. We‚Äôll use the same data for Edmonton that is shown at the top of this page to try to make a map showing the distribution of population in the city.\nGood prerequisites for this tutorial would be our introductory GIS and spatial data visualization notebooks.\nClick here to download data for census tracts in Edmonton. The data includes the following columns, sourced from the Canadian census. Check out our notebook on Canadian census data for more details on this data source.\n\nCTUID unique identifier for each census tract (which can be used to join with other data)\narea area in square kilometres\npop_2021 population in 2021\ndwe_2021 number of dwellings in 2021\npop_1996 population in 1996\ndwe_1996 number of dwellings in 1996\n\nSince our data are polygons, but proportional symbol maps are typically based on single points, we‚Äôll have to convert our polygons to points first before we can visualize. The common way to generate representative points of polygons is by computing centroids. We can do this via Vector - Geometry - Centroids. Or by opening the Processing - Toolbox and searching for Centroids. The result should look something like this (likely with different colours).\n(Note that computing centroids is one of many common spatial data or geo-processing functions. Read more in our notebook on spatial data processing)\n\n\n\nCentroids of census tracts in Edmonton\n\n\nOnce we have our points, we can then visualize them based on their size! To do so, open up the Properties of the centroids layer, go to Symbology, and then select Graduated. The default is to style by colour, but if you can change the Method to Size. The Size from options define the size of the smallest and largest circle. These are worth tinkering with since you want all circles to be visible, but you don‚Äôt want too much clutter on the map either (i.e.¬†circles too big that they cover everything).\nIn the example below, I am testing out 3 groups of sizes, based on quantiles of 2021 population.\n\n\n\nProportional symbol styling options in QGIS\n\n\nHere‚Äôs the output based on this initial styling.\n\n\n\nResult of proportional symbol styling options in QGIS, based on 2021 census tract population\n\n\nNote as well that the map is in an area-conserving CRS (a local Mercator projection) and that an OpenStreetMap basemap is in the background to give the map some visual reference (check out our introductory QGIS tutorial on how to do this).\nThis output on its own is great for exploratory analysis, can clearly pick up patterns of high and low population in Edmonton, but if you were going to export this to include it in a publication it would be better to include a title and legend. Check out some of the other example maps on this page about how legends for proportional symbol maps can be designed. This is sometimes done via graphic design software since base legend options in QGIS and similar tools can be limited in terms of custom layout and legend design.\nWhat we did, as well, is relatively simple. It only shows population in one year. However, in the data (the same source as the very first map shown in this notebook) we have population and dwelling counts for 2021 and 1996. From here you could use proportional symbols to show‚Ä¶\n\n1996 population or dwelling counts\nPopulation growth\nPopulation decline\nGrowth or decline on the same map!",
    "crumbs": [
      "Urban Data Visualization",
      "Proportional symbol maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/proportional-symbol-maps/proportional-symbol-maps.html#more-symbol-options",
    "href": "urban-data-visualization/proportional-symbol-maps/proportional-symbol-maps.html#more-symbol-options",
    "title": "Proportional symbol maps",
    "section": "More symbol options! :)",
    "text": "More symbol options! :)\nCircles are the most common shape for proportional symbol maps - but there are many other options available! Check out the following map, which uses triangles (or spikes) to show where Christmas trees grow in the USA. To make maps similar to this with your own data in QGIS or other software, the process would be to select a different symbol, or even load in your own custom symbol (e.g.¬†as an .svg).\n\n\n\nMap of the USA where the proportional heights of triangles represent how many Christmas trees are grown and sold (Source)\n\n\nProportional symbol maps can also be combined with charts to show size alongside other data. It is possible to create chart-based symbols like this in QGIS by going to a layers Properties and then Diagrams. There are options for pie charts, bar charts, and histograms. Just be careful with the size and number of variables in each chart, as if there are many on a map, it can become difficult to read and parse out trends.\n\n\n\n1858 map by (Charles Joseph Minard)[https://en.wikipedia.org/wiki/Charles_Joseph_Minard] where each region in France has a circle sized according to the amount of meat shipped to Paris for consumption, including pie charts distinguishing the types of meat. (Source)\n\n\nEverything that we‚Äôve shown above pertains to single points (either a point dataset or polygons represented by their centroids). However, we can also do similar proportional symbols for line-data, for example where the width or thickness of a line is based on a data value. For example, this map visualizes ridership by transit route in Toronto via line-width.\nIn QGIS, you can create something similar by classifying your line data via Graduated (in Symbology) and then either A) manually changing the symbol styling to different line-widths or B) styling via a function using the expression builder\n\n\n\nMap of bus and streetcar ridership by route in Toronto. (Source)",
    "crumbs": [
      "Urban Data Visualization",
      "Proportional symbol maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html",
    "href": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html",
    "title": "Bivariate choropleth maps",
    "section": "",
    "text": "Jeff Allen\nBivariate choropleth maps are pretty two-bular. They use colour to represent the values of two different data variables on the same map. Check out this map of neighbourhood material deprivation (a combined metric of lower income, education, employment rates, etc.) and quality of cycling infrastructure in Winnipeg.\nOf course, we could just map these two variables individually. However, overlaying them onto a single map can be incredibly useful for highlighting areas of correlation (or lack thereof) at different scales.\nFor example, in the bivariate map above, we can quickly identify areas with greater material deprivation but a lack of cycling infrastructure (in pink). These neighborhoods arguably should be at the top of the list for new investments in cycling infrastructure. The dark purple areas are those with high material deprivation but good cycling infrastructure, while the green areas are wealthier areas with good cycling infrastructure.\nThese types of maps can aid effective spatial storytelling and communication of findings, highlighting specific local areas of need, and be a useful exploratory analysis step before more sophisticated spatial modeling.\nIn this tutorial, we will to cover how to create bivariate choropleth maps using Python, mostly using geoPandas, with some final touch-ups and legend design in Inkscape. However, the same processes can be done in other software (e.g.¬†check out these tutorials for making bivariate maps in R and bivariate maps in QGIS).\nThe example data will be on urban health-related data in Canada (replicating the map above), but the methods and code can be applied to any location with two quantitative variables linked to the same set of geometries.\nAs a short side note on history, it is surprising that despite centuries of thematic maps showing multivariate data, bivariate choropleths are a recent creation. They gained popularity in the 1970s via maps created by the U.S. Bureau of the Census (DOI). Here‚Äôs one of their maps. I believe these were the first published bivariate maps, but if you are reading this and know of earlier bivariate maps, please let me know. I‚Äôd be super interested to see them.",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#prerequisites",
    "href": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#prerequisites",
    "title": "Bivariate choropleth maps",
    "section": "Prerequisites",
    "text": "Prerequisites\nPrior knowledge of Python, including pandas and geopandas, as well as Inkscape or similar graphic design software, will be helpful for the following tutorial.\nHere are the links to download the notebook and data for this tutorial:\n\nJupyter Notebook\nDatasets\n\nIn the data download, there is also a standalone Python script, if you want to run the steps all-at-once or integrate with anything else you have cooking.\nIf you are running the notebook and/or script locally (generally recommended), you will need to use the following libraries. You‚Äôll have to install them via pip or conda if you do not have them installed already.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\nimport mapclassify\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#loading-data",
    "href": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#loading-data",
    "title": "Bivariate choropleth maps",
    "section": "Loading data",
    "text": "Loading data\nWe‚Äôre going to replicate the map of Winnipeg shown at the top of this page. I‚Äôve pre-filtered the source datasets for Winnipeg, and they are included in the download link above. The datasets we‚Äôll be using are:\n\nCensus Dissemination Areas (DA) Polygons\nCanadian Marginalization Index (CAN-Marg)\nCanadian Bikeway Comfort and Safety Classification system (Can-BICS)\n\nWe‚Äôre also going to add three additional layers solely for cartographic purposes (i.e.¬†as reference layers on the final map)\n\nCensus Subdivision (CSD) Polygon - i.e.¬†a boundary polygon for Winnipeg\nMajor Streets from OpenStreetMap\nRivers from OpenStreetMap\n\nTo get started, let‚Äôs read the first three data layers and merge them into a single geoDataFrame, joining by the Dissemination Area unique id, dauid. We‚Äôll also load the three cartographic reference layers.\n\ngdf = gpd.read_file(\"data/dissemination-area-winnipeg-2016.geojson\")\ndfm = pd.read_csv(\"data/can-marg-manitoba-2016.csv\")\ndfb = pd.read_csv(\"data/can-bics-winnipeg.csv\")\n\ngdf = gdf.merge(dfb, how='left', on='dauid').merge(dfm, how='left', on='dauid')\n\ncsd = gpd.read_file(\"data/csd-winnipeg-2016.geojson\")\nosm_streets = gpd.read_file(\"data/streets-osm-winnipeg.geojson\")\nosm_rivers = gpd.read_file(\"data/river-osm-winnipeg.geojson\")\n\nLet‚Äôs pick two variables to map, one from each of the tabular datasets:\n\nMaterial resources from CAN-Marg - an indicator of individual and community access to and attainment of basic material needs (including housing, income, education, employment). The higher the value, the fewer the resources (i.e.¬†greater deprivation).\nCan-BICS continuous metric - a weighted sum of the quality of bike infrastructure within a buffer of each DA. The higher the value, the better the infrastructure.\n\nHere‚Äôs a subset of what we want to map, the two variables noted above, plus the geometry data required to plot the data.\n\ngdf[[\"dauid\",\"material_resources_DA16\",\"CBICS_cont\",\"geometry\"]].head(8)\n\n\n\n\n\n\n\n\ndauid\nmaterial_resources_DA16\nCBICS_cont\ngeometry\n\n\n\n\n0\n46110001\n-0.349423\n0.000000\nMULTIPOLYGON (((-97.14934 49.99388, -97.14105 ...\n\n\n1\n46110002\n-0.747839\n0.000000\nMULTIPOLYGON (((-97.09195 49.96864, -97.09705 ...\n\n\n2\n46110003\n-0.321103\n0.000000\nMULTIPOLYGON (((-97.13213 49.95467, -97.13289 ...\n\n\n3\n46110004\n-0.958393\n0.000000\nMULTIPOLYGON (((-97.13575 49.95428, -97.13695 ...\n\n\n4\n46110005\n0.420112\n0.026394\nMULTIPOLYGON (((-97.13289 49.95334, -97.13418 ...\n\n\n5\n46110006\n-0.065753\n0.026394\nMULTIPOLYGON (((-97.12985 49.94970, -97.13065 ...\n\n\n6\n46110007\n-0.277738\n0.000000\nMULTIPOLYGON (((-97.12224 49.95150, -97.12321 ...\n\n\n7\n46110008\n-0.329884\n0.020913\nMULTIPOLYGON (((-97.12579 49.94767, -97.12714 ...",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#choropleth-maps",
    "href": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#choropleth-maps",
    "title": "Bivariate choropleth maps",
    "section": "Choropleth maps",
    "text": "Choropleth maps\nFor a background on single-variable choropleth maps, and how to make them in QGIS, check out our notebook on the subject\nIn Python, geopandas has a built-in plot function that makes it very easy to create choropleth maps for any quantitative variable. In the code below, we use quantiles (equal number of DAs in each coloured group) to create choropleths using geopandas and matplotlib.\nIf you‚Äôre interested in learning more about creating univariate choropleth maps in Python, including how to classify data, check out the Geographic Data Science textbook‚Äôs chapter on the topic. You can also refer to the geopandas.GeoDataFrame.plot documentation for more information.\n\nfig, ax = plt.subplots(ncols=2, figsize=(7,6))\n\ngdf.plot(\n    column = \"material_resources_DA16\", \n    cmap = 'YlOrRd', \n    k = 5,\n    scheme = \"Quantiles\", \n    legend = True,\n    ax=ax[0],\n    legend_kwds = {\n        \"loc\": \"lower left\",\n        \"fontsize\": 7,\n        \"title\": \"Lack of Material\\nResources\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    }\n).set_axis_off();\n\ngdf.plot(\n    column = \"CBICS_cont\", \n    cmap = 'YlGnBu', \n    scheme = \"Quantiles\", \n    legend = True,\n    ax=ax[1],\n    legend_kwds = {\n        \"loc\": \"lower left\",\n        \"fontsize\": 7,\n        \"title\": \"Can-BICS\\nContinuous\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    }\n).set_axis_off();\n\nplt.tight_layout()",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#bivariate-classifications",
    "href": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#bivariate-classifications",
    "title": "Bivariate choropleth maps",
    "section": "Bivariate classifications",
    "text": "Bivariate classifications\nThe first step towards creating a bivariate map is to classify our data.\nI‚Äôll be showing an example for 3x3 classifications using quantiles, but the same principles can be applied to any classification type.\nA bivariate classification has two dimensions, let‚Äôs call those X and Y. The legend can be visualized as a 2-dimensional chart. For this example, I‚Äôm classifying each dimension into three categories, terming them 0 (low), 1, and 2 (high). We then concatenate these two categories together to create a joint classification. For example, a high value in the X dimension and a low value in the Y dimension would be classified as 2-0.\nWe can then colour based on this joint classification.\n\nBut how do we do this in Python and geopandas? Behind the scenes of the univariate choropleth maps in the previous section is a Python library called mapclassify.\nHere‚Äôs an example classifying the Can-BICS variable into k = 3 quantiles (changing this to k = 5 gives us the same classification in the map above)\n\nmapclassify.Quantiles(gdf[\"CBICS_cont\"], k = 3)\n\nQuantiles\n\n   Interval      Count\n----------------------\n[ 0.00,  1.15] |   373\n( 1.15,  2.25] |   372\n( 2.25, 10.79] |   373\n\n\nWe can use the same function to generate bivariate classifications. Below we create 3x3 square classification using quantiles. A k = 3 for each variable X (Can-BICS cycling infrastructure quality) and Y (Can-MARG lack of material resources), and then combine the two together.\n\ngdf['x_group'] = gdf[['CBICS_cont']].apply(mapclassify.Quantiles.make(rolling=True, k = 3))\ngdf['y_group'] = gdf[['material_resources_DA16']].apply(mapclassify.Quantiles.make(rolling=True, k = 3))\ngdf['xy_group'] = gdf['x_group'].astype(str) + \"-\" + gdf['y_group'].astype(str)\n\nHere‚Äôs a random sample of the result that we can use to spot check a few results\n\ngdf[[\"dauid\",\"material_resources_DA16\",\"CBICS_cont\",'x_group',\"y_group\",\"xy_group\"]].sample(5)\n\n\n\n\n\n\n\n\ndauid\nmaterial_resources_DA16\nCBICS_cont\nx_group\ny_group\nxy_group\n\n\n\n\n509\n46110546\n-0.570719\n3.677031\n2\n0\n2-0\n\n\n107\n46110108\n-0.258522\n0.226581\n0\n1\n0-1\n\n\n684\n46110741\n-0.149318\n2.971448\n2\n1\n2-1\n\n\n978\n46111074\n-0.949989\n1.141521\n0\n0\n0-0\n\n\n785\n46110855\n-0.488178\n1.340795\n1\n1\n1-1\n\n\n\n\n\n\n\nNow we are ready to make a bivariate map!\nAt this stage, we can also save the classified data to then view it in QGIS or use for web-mapping: gdf.to_file(\"data/winnipeg-bivariate.geojson\", driver='GeoJSON')\nNote that the steps above could also be replicated in QGIS by creating and calculating new columns in the Attribute table.",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#bivariate-colours",
    "href": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#bivariate-colours",
    "title": "Bivariate choropleth maps",
    "section": "Bivariate colours",
    "text": "Bivariate colours\nNow that we‚Äôve classified our data, we can assign colours and make a map!\nHere are a few examples for colouring that we can choose from:\n\nLet‚Äôs try to make a simple map using gpd.plot() based on our classified data and the first of these colour schemes.\nThe data we have are categorical. To plot categorical data with custom colours, we first have to provide a dictionary to map each category to each colour.\n\ncolor_mapping = {\n    \"0-2\": \"#f73593\", \n    \"0-1\": \"#f78fb6\",\n    \"0-0\": \"#f7fcf5\", \n    \"1-2\": \"#a53593\",\n    \"1-1\": \"#a58fb6\", \n    \"1-0\": \"#a5e8cd\", \n    \"2-2\": \"#403593\",\n    \"2-1\": \"#408fa7\",\n    \"2-0\": \"#40dba7\" \n}\n\nWe can then feed this into our plot. We‚Äôre also going to add the reference layers to make the map a bit more intuitive; the Winnipeg boundary, streets, and rivers.\n\nfig, ax = plt.subplots(figsize=(7,7))\n\n# Winnipeg border\ncsd.plot(\n    edgecolor = \"#c2c2c2\",\n    linewidth = 4.2,\n    ax = ax\n);\n\n# bivariate data\ngdf.plot(\n    column = \"xy_group\",\n    categorical = True,\n    edgecolor = \"white\",\n    linewidth = 0.2,\n    ax = ax,\n    color=gdf[\"xy_group\"].map(color_mapping),\n).set_axis_off();\n\n# Winnipeg rivers\nosm_rivers.plot(\n    color = \"#c2c2c2\",\n    linewidth = 0,\n    ax = ax\n);\n\n# Winnipeg streets\nosm_streets.plot(\n    color = \"#5e5e5e\",\n    linewidth = 0.25,\n    ax = ax\n);\n\n# custom legend\nlegend_elements = []\nfor key, value in color_mapping.items():\n    legend_elements.append(Patch(facecolor=value, edgecolor='gray', label=\"\"))\nax.legend(\n    handles=legend_elements, \n    loc='lower right', \n    fontsize= 12, \n    ncol=3, \n    handletextpad=0.1, \n    labelspacing = 0.1, \n    columnspacing = 0.1\n)\nax.text(0.55, 0.1, 'Material\\nDeprivation', transform=ax.transAxes, fontsize=10, verticalalignment='top')\nax.text(0.75, 0.01, 'Quality of Cycling\\nInfrastructure', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n\n\n\n\n\n\n\n\nLooks great! One wrinkle, however, is that we had to create a custom legend because simply settinglegend = True didn‚Äôt work initially. It appears that there is a known issue with plotting categorical data with custom colours. We can try to continue to tinker with this map using matplotlib, but personally, I prefer to design more nuanced layout items, such as legends, using graphic design software like Inkscape, particularly if the goal is to create visually appealing ‚Äústatic‚Äù maps for an article or report.\n\nfig.savefig('images/winnipeg-bivariate-map-python-export.png')",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#layout-design-in-inkscape",
    "href": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#layout-design-in-inkscape",
    "title": "Bivariate choropleth maps",
    "section": "Layout design in Inkscape",
    "text": "Layout design in Inkscape\nThe line of code above exports the figure as an .svg (Scalable Vector Graphic). We can open it up in Inkscape (or similar graphic design software) and add layout items such as a title, legend, scale bar, north arrow, and other information.\nThe page size is set to 7x7 inches based on the plt.subplots(figsize=(7,7)) included at the top of the code used to render the map.\nGenerally, my workflow is to create at least three layers in the following order: - layout (title, legend, scale bar, north arrow, and other info) - map (in this case, the original exported .svg) - background (often just a single rectangle with a solid fill colour)\nThese can be expanded into more layers (or sub-layers) if desired.\nFrom a cartographic design side, it‚Äôs important to aim for balance. This is a bit of a subjective notion pertaining to the visual weight of the elements of an image. For example, in the map below, I tried to balance the layout by putting one item in each corner. I purposely used the bottom-left corner for the legend since it takes up the most space and fits nicely within the square-ish cutout of Winnipeg‚Äôs border.\nI also used a grid to align some of the items such as the map frame, title, north arrow, and scale bar. Overall, creating these layout items didn‚Äôt take too long, around 20 minutes or so.\nHere‚Äôs a screenshot of Inkscape showing all the items in the layout layer that I created.\n\nAnd here‚Äôs the final map! We can export this to any resolution that we want. This is 300dpi for 7‚Äùx7‚Äù (2100x2100 px), a nice size for fitting into a report.",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#final-thoughts-and-additional-resources",
    "href": "urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#final-thoughts-and-additional-resources",
    "title": "Bivariate choropleth maps",
    "section": "Final thoughts and additional resources",
    "text": "Final thoughts and additional resources\nThe classification method (e.g., quantiles, equal breaks, etc.), the number of classes, and of course, the colours can easily be tinkered with.\nA noted drawback with bivariate maps is that they are often not initially intuitive and sometimes require a good amount of looking back-and-forth with the legend, especially compared to univariate choropleth maps. Even with a 3x3 legend like the map above, it can take a bit of time to read the map and understand and parse out specific values (especially for the middle values). If you‚Äôre more interested in understanding each variable on its own, rather than a direct comparison, it‚Äôs probably best to stick with two side-by-side univariate choropleth maps. If you want to allow for comparison but keep things easier to read, a 2x2 legend might be better, but remember that different aggregation can lead to varying results!\nLastly, here are a few other resources and tutorials on bivariate maps:\n\nWikipedia page on multivariate maps\nColor Statistical Mapping by the U.S. Bureau of the Census | (what I think might be the first published bivariate maps)\nBivariate maps in QGIS | by Joshua Stevens\nBivariate maps in R | by Timo Grossenbacher",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate choropleth maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html",
    "href": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Jeff Allen",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#overview",
    "href": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#overview",
    "title": "Maps and visualizing spatial data",
    "section": "Overview",
    "text": "Overview\nMaps and spatial data visualizations can either be created at the research stage when we are trying to uncover patterns in the data, or as polished communication visuals, when we want to convey patterns in the data to our intended audience.\nCartography is often defined as the science and art of creating maps. It often involves decisions about ‚Ä¶\n\nselection of data to include on the map\ngeneralization of data to reduce complexity and clutter\nhow we want to style and symbolize our data\nhow to order and layer data\nwhat other map layout elements to include (e.g.¬†title, legend, north arrow), as well as how to design these to make a map easy to understand\n\nThis page will cover the basics of what goes into making maps and spatial data visualizations, following each of the steps above.\nThe last part of this notebook shows an example in QGIS of how to create maps with data-defined styling, for both categorical and numeric data.\nIf you are interested in creating specific types of maps, check out the following notebooks:\n\nChoropleth maps\nBivariate choropleth maps\nProportional symbol maps\nCategorical random dot maps\nFlow maps",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#types-of-maps",
    "href": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#types-of-maps",
    "title": "Maps and visualizing spatial data",
    "section": "Types of maps",
    "text": "Types of maps\n\nReference & thematic\nAlmost all maps can be thought of as either:\n\nReference maps, which provide base geographic information (e.g.¬†streets, land forms, water, etc.) often for locating places or for navigation. Google Maps is probably the most well-used example.\nThematic maps, also called geographic or spatial data visualizations, emphasize patterns or trends tied to a specific theme (e.g., population density, election results, air pollution, etc.).\n\nReference maps are often used as base layers for thematic maps.\nFor example, if we‚Äôve collected a dataset about the location of public washrooms in a city, the first thing we might want to do is simply see where they are located by overlaying them onto a base map in GIS. This would be an example of a simple thematic map.\nHere are a few examples of thematic maps that we‚Äôve created, each with different urban data, objectives, and audience in mind.\n\n\n\nHeatmap of Bike Share trips in Toronto in 06/2024\n\n\n\n\n\nIsochrone map of access to outdoor skating rinks in Toronto\n\n\n\n\n\nBi-variate map of public transit accessibility and recent immigrant settlement patterns\n\n\n\n\nStatic & interactive\nStatic maps are fixed, pre-generated images that display geographic information in a single, unchangeable format. They are ideal for simple visualization, printing, or when interactivity isn‚Äôt needed‚Äîsuch as in posters, reports, or static web images.\nStatic maps can vary in size from just a few dozen pixels on a screen or a small figure in a report, to graphics in a slide deck, to large maps printed as posters or dozens of maps as part of a series, like in an atlas. The three maps shown above are examples of static maps.\nInteractive maps are digital and allow users to engage with the content by zooming, panning, clicking for details, or filtering data. They are best for exploration, real-time applications, user-driven analysis, or providing increased engagement via animation in a data-story. Here are a few examples that we‚Äôve developed at the School of Cities:\n\nUrban Activity Atlas\nHeat Vulnerability in Toronto\nKnowledge of Languages in the Greater Toronto & Hamilton Area\n\nInteractive maps can provide flexibility and deeper engagement with spatial data. However, interactive maps typically are much more work to create than a static map as they include all the visual design thinking in a static map, plus additional thinking about how users interact with them and how the maps respond to user inputs, as well as increased technical development time.\nOur general recommendation is to always start with creating static maps, and then move on to creating an interactive version only if a static version is insufficient at conveying the story in your data that you want to convey.\nIf designed well, static maps can be super effective at highlighting key trends and stories in spatial data in a quick and easy to read way.",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#whats-on-the-map",
    "href": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#whats-on-the-map",
    "title": "Maps and visualizing spatial data",
    "section": "What‚Äôs on the map?",
    "text": "What‚Äôs on the map?\nHow do we decide what we are going to include on our map? Sometimes this may be obvious in terms of the key datasets that we want to visualizes, but particularly with any reference layers, this can often require a lot of minor subtle choices.\nThis is because maps are always abstractions of reality. It is impossible to show everything in the world on a piece of paper or a screen - nor would we want to, as we usually want to focus our reader‚Äôs attention on specific data point(s), trend(s), or story.\nCreating maps and spatial data visualizations therefore often involves making decisions about how to select and generalize spatial data to reduce visual clutter and focus on specific data points.\n\nCartographic selection\nCartographic selection is about picking the most important things to draw on a map so it‚Äôs not too crowded. Imagine drawing a treasure map ‚Äî you‚Äôd likely want to show big landmarks like mountains or forests, but leave out tiny rocks or every individual tree.\nIn practice, this can include deciding whether or not to include a dataset at all when creating a map.If we do decide to include the dataset, we may want to filter it to only show certain features.\nFor example, you may want to include a dataset of public transit lines as a reference layer for a map of your city. However, you may not want to include every single transit route. An example of cartographic selection would be to pre-filter the data by mode so we only show major transit lines (e.g.¬†only show metro/subway) or frequency (e.g.¬†only show routes where a bus or train comes every 10 minutes or less). Now of course if the goal of your map and research is about public transit at large, you may want to include all the routes.\nThe process of selecting some, but not all of the data that you have available, reduces clutter on your maps and can make them easier to read.\nWhat data to include depends on your objectives, audience, and story.\n\n\nCartographic generalization\nSimilar to selection is cartographic generalization, which is when mapmakers simplify real-world details to make maps clearer and less cluttered, especially at smaller scales.\nFor example, a coastline with tons of tiny twists and inlets might get smoothed out to keep the overall shape but remove unnecessary complexity. This helps the map stay readable without losing its key features. It‚Äôs like sketching a quick but accurate version of a photo instead of drawing every single pixel.\n\n\n\nShoreline of eastern Canada at two different levels of simplification\n\n\nAnother example would be a dataset that lists sports and recreation facilities in a city by primary use (i.e.¬†baseball, football, tennis, etc). We could choose to have a different colour or symbol for each of these use types (i.e.¬†green for tennis and blue for baseball), or generalize so they all look the same (e.g.¬†the same shade of green).",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbols-and-layer-styling",
    "href": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbols-and-layer-styling",
    "title": "Maps and visualizing spatial data",
    "section": "Symbols and layer styling",
    "text": "Symbols and layer styling\nOnce we‚Äôve decided what we want to include on our map, we have to decide how we want to style each layer. In some mapping tools, like QGIS, style options are often called Symbology.\nBelow are the most commonly used graphic styling options available for vector data layers. If you‚Äôve worked with other design software, especially vector graphic software, many of these will be familiar.\nPoints:\n\nSymbol type (e.g.¬†circle, square, etc.)\nSize\nFill colour\nStroke colour\nStroke width\nOpacity\n\nLines:\n\nWidth\nColour\nStroke style (e.g.¬†solid, dashed, etc.)\nOpacity\n\nPolygons:\n\nFill colour\nFill pattern (solid, hatching, etc.)\nStroke colour\nStroke width\nStroke style (e.g.¬†solid, dashed, etc.)\nOpacity\n\nNote that there are often additional styling options beyond those listed above.\n\nSingle-rule styling\nSingle-rule styling is when we want all features in a dataset to look the same, regardless of how they may different in terms of their attributes\nIn the example in our notebook on introducing spatial data and GIS, we created a simple map of Toronto where we had two vector datasets 1) ward administrative boundaries (polygons) and 2) public libraries (points).\nEach of these layers is styled via a single rule. For example, all public libraries are shown as blue squares, even though some libraries may be larger than others or have longer opening hours.\n\n\n\nScreenshot of QGIS with single-rule based styling for two layers\n\n\n\n\nData-defined styling\nA powerful feature of tools such as QGIS or Python to make maps is that we can visualize spatial data based on a set of rules relating to their associated attribute data. For example, we could:\n\nColour census tract data based on median income\nSize business point data based on their number of employees\nShade different land-cover (wetlands, forest, farmland) with different textures for each category\nStyle street network data based on quality and safety of cycling infrastructure\nMany more!\n\nMany of these options use visual variables like size, hue, saturation, orientation, etc. as shown in our data visualization notebook.\nLet‚Äôs look at a couple examples in QGIS! (If you haven‚Äôt worked with QGIS before, check out our intro to spatial data and GIS notebook)\n\n\nStyling categorical data\nFor the first example, let‚Äôs try to create a map of public washrooms in Toronto in QGIS, using some categorical based styling - where different categories or groups are styled with different colours or symbols.\n\nWashroom facilities (a point dataset of public washroom facilities in Toronto)\nCentrelines (a line dataset representing a mix of features like streets, railways, power lines, rivers, etc. in Toronto)\nGreen space (a polygon dataset of green spaces and public parks in Toronto)\nLake Ontario (a polygon of Lake Ontario)\n\n(All of this data is from the City of Toronto, except for the Lake Ontario polygon, which is from OpenStreetMap)\nLoad the data into QGIS and re-order the layers such that the washroom locations are at the top. For our map, the washrooms are the key data we want to show (i.e.¬†the foreground), while the others are visual reference (i.e.¬†the background).\nAfter loading in the data, right click on each of the layers and then click Open Attribute Table to view the what data is linked to each feature.\nWhile most columns of a spatial dataset could be styled categorically, many wouldn‚Äôt be interesting to represent. Alternatively if there are many different categories, showing too many could lead to excessive visual clutter.\nSo let‚Äôs pick something simple to start with. We‚Äôll colour the washrooms by their type, which only has two options \"Washroom Building\" or \"Portable Toilet\"\nTo style a layer based on a category, right-click the layer, go to Properties, then Symbology, and then at the top there will be a dropdown where one of the options is Categorical. The below dropdown is the Value, which is the column in your data that you want to use to define the style of your data. For this example, lets pick type. Then click Classify. You should see each possible category display with a default symbol next to it.\n\n\n\nScreenshot of categorical styling in QGIS\n\n\nYou can change the default colours and symbols of each category by double clicking on it.\nOnce you hit OK at the bottom-right, your map should update!\n\n\n\nMap of public washrooms in Toronto coloured by their type\n\n\nThis was just the beginning! You can play with categorical styling for other layers as well. Here‚Äôs a couple ideas to improve the the reference layers\nStyle to colour green spaces based on their AREA_CLASS, to give different green spaces types different shades of green\nStyle the centreline data by FEATURE_CODE_DESC, to show major roads as thicker lines\nTip! with categorical styling you can also very quickly filter a dataset. Notice that once a layer is categorized, there are check marks next to each symbol, which allows you to easily hide or show different categories.\nFor example, if you only wanted to show parks and cemeteries in the green space layer, you can simply uncheck all the other layers.\n\n\n\nScreenshot of selectable categorical styling in QGIS\n\n\n(Also note how the map has been rotated ~17-18 degrees, to horizontally align some of the features and reduce empty white space).\n\n\nStyling numeric data\nWe often have ordinal or numeric data that we want to use to define our styling. For example, showing larger numbers in a column with a darker colour and smaller numbers with a lighter colour.\nLet‚Äôs do a quick example, with data indicating camera locations for automated enforcement of speeding in Toronto. This dataset includes an integer column which denotes the number of speeding tickets for each automated camera in February 2025.\nAutomated enforcement camera locations dataset\nWe can colour these using the Graduated option in QGIS. This option allows us to pick a numeric column, and then classify or group the data into different ‚Äúbins‚Äù or ‚Äúbreaks‚Äù, each with a colour, in this case ordered sequentially (more tickets, a darker red).\n\n\n\nScreenshot of graduated styling in QGIS\n\n\nThere are lots of options here for different sets of colours and ways to classify your data. The screenshots show what I did. Try playing with some of these different options\n\n\n\nMap automated speeding tickets by location in Toronto in February 2025\n\n\nFor more ideas and ways on how to classify and map numeric data via data-driven styles, check out our notebook on choropleth maps (maps where areas are shaded based on data values).",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#layers-and-hierarchy",
    "href": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#layers-and-hierarchy",
    "title": "Maps and visualizing spatial data",
    "section": "Layers and hierarchy",
    "text": "Layers and hierarchy\nMaps are often the product of multiple layers, each with their own defined styling. Thinking about the ordering of the layers and their styling relative to each other goes a long way in making an effective map.\nWell designed maps, especially thematic maps, often have a visual hierarchy of background and foreground - where the background provides geographic reference (e.g.¬†streets, bodies of water), while the foreground are the key data that we want to show.\nThese are two examples of maps relating to the 2023 wildfires in Yellowknife that include a strong visual hierarchy of the key data (e.g.¬†fire area, evacuation routes) relative to their geographic reference backgrounds. Both maps are quite simple from a data analytics standpoint, but were part of a story highlighting the scale and impact of the fires.\n\n\n\nMap of the 2023 Yellowknife wildfires\n\n\n\n\n\nScale and location of evacuation resulting from the 2023 Yellowknife wildfires",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#map-layouts",
    "href": "urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#map-layouts",
    "title": "Maps and visualizing spatial data",
    "section": "Map layouts",
    "text": "Map layouts\nYou‚Äôll notice that many of the example maps above include graphic elements beyond specific data layers. Things like ‚Ä¶\n\nTitles\nExplanatory text and annotations\nPlace labels\nBorders\nScale bar\nNorth arrows\nLegends\n\nNot all maps and spatial data visualizations require all of the above, but they can often help communicate what is being shown on the map to your readers and audience.\nThe space that you would combine these items on top of map data layers are sometimes called map layouts, with each item sometimes called map elements or layout elements\nMost mapping and data analysis software and related libraries (e.g.¬†in QGIS, Python, R, etc.), provide tools for adding elements and creating map layouts. In QGIS you can go to Project then New Print Layout. This gives you a blank canvas for you to add your map data and layout elements. Here is a good step-by-step tutorial for layouts and exporting in QGIS.\nIn the layout view in QGIS, you can then export and save your map as .pdf, .png, or .svg.\nWhile the options in QGIS and similar tools are fairly good, they are limited in terms of customization. If we are making maps for publication and communication material (i.e.¬†for not just our internal research), we typically export our map data layers at a high resolution, and then design the layouts and related elements in a graphic design software (e.g.¬†Inkscape, Illustrator, Canva, etc.). This allows for more customization in the design of layout elements, to help make a more professional final product.",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html",
    "href": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html",
    "title": "Exploratory data visualization",
    "section": "",
    "text": "Julia Greenberg, Evelyne St.¬†Louis\nDownload this notebook and data\nData exploration is an important step in the data analysis process. It can highlight anomalies or interesting trends, and reveal how the data are distributed.\nThis notebook explains how to visually explore data by creating plots using the popular seaborn and matplotlib libraries.\nIt will include the following plots:",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#install-and-import-libraries",
    "href": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#install-and-import-libraries",
    "title": "Exploratory data visualization",
    "section": "Install and import libraries",
    "text": "Install and import libraries\nIf you haven‚Äôt already done so, install the following libraries using pip:\n\n!pip install pandas\n!pip install seaborn\n!pip install matplotlib\n\nNow import them using their common aliases. Note that pyplot is a module of the matplotlib library:\n\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#load-bike-share-data",
    "href": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#load-bike-share-data",
    "title": "Exploratory data visualization",
    "section": "Load bike share data",
    "text": "Load bike share data\nThe dataset we‚Äôll be working with is Bike Share Toronto ridership data from January 2022. It‚Äôs already been downloaded for you, so you can just load it below using the read_csv function from the pandas library.\nHowever, if you want to download the data yourself‚Ä¶\n\nGo to this page from the City of Toronto‚Äôs Open Data portal\nClick on the blue ‚ÄúDownload‚Äù bar towards the bottom\nDownload the bikeshare-ridership-2022 zip file\nUnzip the file and open the folder\nSave the Bike share ridership 2022-01.csv file\nMake sure you read the file using the correct filepath\n\n\ndf = pd.read_csv(\"data/Bike share ridership 2022-02.csv\")\n\nLet‚Äôs take a look at the first 5 rows of the dataframe:\n\ndf.head()\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n\n\n2\n14805112\n942\n7399\n01/01/2022 00:04\nLower Jarvis / Queens Quay E\n7686.0\n01/01/2022 00:19\nNaN\n361\nAnnual Member\n\n\n3\n14805113\n4256\n7334\n01/01/2022 00:04\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n4350\nCasual Member\n\n\n4\n14805114\n4353\n7334\n01/01/2022 00:05\nSimcoe St / Wellington St North\n7038.0\n01/01/2022 01:17\nDundas St W / Yonge St\n5074\nCasual Member\n\n\n\n\n\n\n\nWhat do you notice about the data? Which variables are you interested in exploring?",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#histogram",
    "href": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#histogram",
    "title": "Exploratory data visualization",
    "section": "Histogram",
    "text": "Histogram\nHow long do bike share trips last?\nLet‚Äôs start by looking at the trip duration column to see how long people are travelling when using the bike share.\nThe ‚ÄúTrip Duration‚Äù column is in seconds, which is hard to interpret ‚Äì to make it easier, let‚Äôs create a column for minutes by dividing by 60. We can then compute some simple summary statistics on the column using the describe method from pandas.\n\ndf[\"Trip Duration Minutes\"] = df[\"Trip  Duration\"] / 60\ndf[\"Trip Duration Minutes\"].describe()\n\ncount    56765.000000\nmean        14.930107\nstd        206.125166\nmin          0.000000\n25%          6.183333\n50%         10.016667\n75%         16.050000\nmax      38095.650000\nName: Trip Duration Minutes, dtype: float64\n\n\nNow we have the mean, standard deviation, and quantiles. The average trip length is about 15 minutes, which seems reasonable. However, the maximum trip length is about 635 hours! That doesn‚Äôt seem right ‚Äì when analyzing this data, we would need to make a decision about how to handle this value. For example, we might filter it out if we‚Äôre confident it‚Äôs an error. Or maybe it‚Äôs part of a larger pattern of people who forget to dock their bikes.\nThe fact that the median (the ‚Äú50%‚Äù statistic from above) is lower than the mean shows that the data are right-skewed ‚Äì most values are clustered at the lower end of the range ‚Äì and there are some large outliers.\nLet‚Äôs plot a histogram to show the distribution of shorter trips (those less than 2 hours long) using the displot function from the seaborn package. Refer to this documentation to understand each of the parameters/arguments used to create the plots below.\nWe‚Äôll start by filtering the data to only rows where the ‚ÄúTrip Duration Minutes‚Äù column value is less than or equal to 120, using loc. Note that we‚Äôre only selecting the column itself because that‚Äôs the only data we need for this plot.\n\nsns.set_style(\"whitegrid\") # set style to make plots look nicer\n\n# Filter the data\ntrips_120_duration = df.loc[df[\"Trip Duration Minutes\"] &lt;= 120, \"Trip Duration Minutes\"]\n\n# Create the plot, with 15 bins, and make it green\nsns.displot(trips_120_duration,\n            bins = 15,\n            color = \"green\"\n            ).set(title = \"Number of Trips by Trip Duration\")\n\nsns.despine() # get rid of plot borders for cleaner look\n\n\n\n\n\n\n\n\nAccording to this plot, trips are typically fairly short, with the majority of trip durations well below 20 minutes. There are a small number of trips that are longer than 40 minutes but these are relatively rare.\nIf you are using spreadsheet software such as Excel and Google Sheets, try to run the same steps as noted above, using the skills previously covered in this textbook. Here are a few tips and reminders:\n\nCreate a new column to calculate trip length in minutes instead of seconds. For example, if trip length in seconds is in an existing column, column C, and you add a new column D, you would write this function for column D, in your D2 cell: D2=C2/60, and then drag it all the way down all your rows.\nYou can then calculate all your key summary statistics manually by using the appropriate Excel or Google Sheets formulas as noted in previous sections of this textbook (e.g.¬†=AVERAGE, =MAX, etc.)\nYou can also create a histogram of the trip length in minutes column by selecting the appropriate data array, clicking ‚ÄòInsert‚Äô, then selecting ‚ÄòHistogram‚Äô.\n\nAs noted above, there are outliers in this dataset (e.g.¬†trip lengths of 300 or even 635 hours!) that you may want to delete or exclude from your analysis.\n\nIf you want to reproduce the histogram shown above (i.e., only trips that are under 120 minutes), you will need to run additional manual steps to exclude trips that are longer than 120 minutes. There are different ways to do this. For example, you could sort the data from smallest to largest and manually select the appropriate array or copy that subset of the dataset you want to manipulate into a new tab. Alternatively, you could create a new column to categorize each trip as either under or over 120 minutes (using the =IF function), and then use this new column to easily filter your data using the Sort & Filter function, and then insert your Histogram after the data is filtered.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#stacked-histogram",
    "href": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#stacked-histogram",
    "title": "Exploratory data visualization",
    "section": "Stacked histogram",
    "text": "Stacked histogram\nHow does trip duration vary by user type?\nNext, let‚Äôs create a stacked histogram that shows trip duration for casual versus annual members. Unlike the previous plot, we‚Äôll need more than just the ‚ÄúTrip Duration Minutes‚Äù column because we‚Äôre also plotting by ‚ÄúUser Type‚Äù.\n\ntrips_120 = df.loc[df[\"Trip Duration Minutes\"] &lt;= 120]\n\nsns.displot(data=trips_120,\n            x=\"Trip Duration Minutes\",\n            multiple=\"stack\",\n            bins=15,\n            hue=\"User Type\",\n            palette=[\"pink\", \"orange\"]\n            ).set(title = \"Bike Share Trip Duration by User Type\")\n\nsns.despine()\n\n\n\n\n\n\n\n\nThis stacked histogram shows that shorter trips (&lt;20 minutes) are most common for both types of members. There are far fewer trips taken by casual members compared to annual members.\nNote that there is no stacked histogram chart option in Excel or Google Sheets. However, you can get around this by using the Stacked Column Chart type instead and essentially treating it as a histogram. This requires you to manually ‚Äòbin‚Äô your data into bins prior to the analysis. If you wish to change your bin size, you will need to start over! It‚Äôs clunky, but it works. Here is one way to do this:\n\nCreate a new column that categorizes your trip length into predefined bins. These will be the bins that will display on your stacked histogram. For example, using an =IF statement, you could create a new column that categorizes trip length into the following 5 categories: Less than 10 min, 10 to 30 min, 30 to 60 min, 60 to 90 minutes, or 90 to 120 minutes.\nOnce your new variable is created, go ahead and create a pivot table. In your pivot table, drag your new variable (categories of trip length) into the ‚ÄòRows‚Äô box. Drag the ‚ÄòUser Type‚Äô (i.e.¬†Casual or Annual members) into the ‚ÄòColumn‚Äô box. Finally, drag the Trip Id into the ‚ÄòValues‚Äô box, and make sure to select ‚ÄòCount‚Äô (not sum, or average).\nNow, select the appropriate data array, go to Chart Type, ‚ÄòInsert‚Äô, ‚ÄòStacked Column Chart‚Äô. You should now see your 5 bins along the x-axis, and stacked bar charts for each bin showing the total number of trips on the y-axis, broken down between Annual and Casual members.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#kernel-density-plot",
    "href": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#kernel-density-plot",
    "title": "Exploratory data visualization",
    "section": "Kernel density plot",
    "text": "Kernel density plot\nHow does the number of trips vary by day, for each user type?\nWhat if we want to plot the number of trips by day of the month, colored by user type? Let‚Äôs start with a kernel density plot.\nFirst we need to convert the ‚ÄúStart Time‚Äù column to a datetime object using pandas.to_datetime so that we can sort by date.\n\ndf['Start Date'] = pd.to_datetime(df['Start Time'], format='%m/%d/%Y %H:%M')\ndf_sorted_date = df.sort_values('Start Date')\ndf_sorted_120 = df_sorted_date.loc[df_sorted_date[\"Trip Duration Minutes\"] &lt;= 120]\n\nNow let‚Äôs make the plot:\n\nsns.displot(df_sorted_120, \n            x=\"Start Date\", \n            hue=\"User Type\", # try commenting this out and see what happens\n            kind=\"kde\", \n            fill=True,\n            height=6, \n            aspect=11/8.5\n            ).set(title = \"Trip Density by Date and Member Type\")\n\nsns.despine()\n\n\n\n\n\n\n\n\nIt appears that trips peak in the first half of the month, with more trips taken by annual members than casual members, and are less frequent in the later half.\nNote that there is no kernel density plot chart option in Excel or Google Sheets.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#bar-chart",
    "href": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#bar-chart",
    "title": "Exploratory data visualization",
    "section": "Bar chart",
    "text": "Bar chart\nWhich stations have the largest number of trips that both start and end at that station?\nNow let‚Äôs create a bar chart that answers the question: which stations have the largest number of trips that both start and end at that station?\n\n# Filter the data to trips where start station ID = end station ID\ndfs = df.loc[df[\"Start Station Id\"] == df[\"End Station Id\"]]\n\n# Group by station and count the number of trips\ndfs = dfs.groupby(\"Start Station Name\").size().reset_index(name = \"count\")\n\n# Create a bar plot of the top 20 stations, shown in descending order of trips\nsns.catplot(data= dfs.sort_values(\"count\", ascending = False).head(20), \n            x='count', \n            y='Start Station Name',\n            kind=\"bar\"\n            ).set(title = \"Number of Return Trips to the Same Station\")\n\nsns.despine()\n\n\n\n\n\n\n\n\nTommy Thompson Park is the station with the most round trip bike rides, followed by Nassau St / Bellevue Ave and Spadina Ave / Adelaide St W.\nWhy do you think these stations have the most ‚Äúround trip‚Äù rides? What additional analysis could you do to explore the potential reasons why?\nThere are different ways to replicate this analysis In Excel or Google Sheets, though in most cases it will require additional steps. Here is one way to replicate the analysis:\n\nFirst, create a new column to categorize whether a trip started and ended at the same station. So for example, if your ‚ÄòStart Station‚Äô variable is Column E, your ‚ÄòEnd Station‚Äô variable is Column F, and your new categorization column is Column G, your IF statement should be G2=IF(E2=F2, ‚ÄúSame start and end stations‚Äù, ‚ÄúDifferent stations‚Äù). Drag your formula down to all your rows.\nSecond, create a pivot table. Drag your newly created variable into the ‚ÄòFilter‚Äô box, and uncheck ‚ÄúDifferent stations‚Äù. Then drag the ‚ÄòStart station‚Äô variable into your ‚ÄòRows‚Äô box, and your trip ID into your ‚ÄòValues‚Äô box and make sure you select ‚ÄòCount‚Äô (as opposed to Sum or Average, for example). This will give you a count of the number of trips that started and ended at the same station, by station.\nYou can then sort your pivot table in descending order based on this count (i.e.¬†largest to smallest number of trips per station).\nYou can then copy-paste this table into a new tab, and use this to create a new bar chart. Select the appropriate data array, click ‚ÄòInsert‚Äô, then ‚ÄòBar Chart‚Äô under Chart Type.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#line-chart",
    "href": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#line-chart",
    "title": "Exploratory data visualization",
    "section": "Line chart",
    "text": "Line chart\nHow does daily average temperature change over time?\nWhat if we are curious about the relationship between bike share trips and temperature? Maybe we have a hypothesis that more people tend to use bike share when it‚Äôs warmer. Let‚Äôs test this hypothesis by first loading and examining another dataset, which includes the daily temperature in January 2022, and joining it with our bike share data.\nLet‚Äôs load in our weather data, which was already downloaded for you from the federal government‚Äôs historical climate data website.\n\ndf_weather = pd.read_csv(\"data/toronto-historical-weather-2022.csv\")\n\n# don't hide any of the column names, even though there are a lot\npd.set_option('display.max_columns', None)\n\ndf_weather.head() # show the first 5 rows\n\n\n\n\n\n\n\n\nLongitude (x)\nLatitude (y)\nStation Name\nClimate ID\nDate/Time\nYear\nMonth\nDay\nData Quality\nMax Temp (¬∞C)\nMax Temp Flag\nMin Temp (¬∞C)\nMin Temp Flag\nMean Temp (¬∞C)\nMean Temp Flag\nHeat Deg Days (¬∞C)\nHeat Deg Days Flag\nCool Deg Days (¬∞C)\nCool Deg Days Flag\nTotal Rain (mm)\nTotal Rain Flag\nTotal Snow (cm)\nTotal Snow Flag\nTotal Precip (mm)\nTotal Precip Flag\nSnow on Grnd (cm)\nSnow on Grnd Flag\nDir of Max Gust (10s deg)\nDir of Max Gust Flag\nSpd of Max Gust (km/h)\nSpd of Max Gust Flag\n\n\n\n\n0\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-01\n2022\n1\n1\nNaN\n5.1\nNaN\n-2.1\nNaN\n1.5\nNaN\n16.5\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2.4\nNaN\nNaN\nNaN\nNaN\nM\nNaN\nM\n\n\n1\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-02\n2022\n1\n2\nNaN\n-2.1\nNaN\n-10.5\nNaN\n-6.3\nNaN\n24.3\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n2\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-03\n2022\n1\n3\nNaN\n-4.0\nNaN\n-12.9\nNaN\n-8.4\nNaN\n26.4\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n3\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-04\n2022\n1\n4\nNaN\n3.3\nNaN\n-5.7\nNaN\n-1.2\nNaN\n19.2\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n4\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-05\n2022\n1\n5\nNaN\n4.9\nNaN\n-4.5\nNaN\n0.2\nNaN\n17.8\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.3\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n\n\n\n\n\nGiven that there are so many column names in the dataframe, it might also be easier to look only at the column names, instead of the first 5 rows of data:\n\ndf_weather.columns\n\nIndex(['Longitude (x)', 'Latitude (y)', 'Station Name', 'Climate ID',\n       'Date/Time', 'Year', 'Month', 'Day', 'Data Quality', 'Max Temp (¬∞C)',\n       'Max Temp Flag', 'Min Temp (¬∞C)', 'Min Temp Flag', 'Mean Temp (¬∞C)',\n       'Mean Temp Flag', 'Heat Deg Days (¬∞C)', 'Heat Deg Days Flag',\n       'Cool Deg Days (¬∞C)', 'Cool Deg Days Flag', 'Total Rain (mm)',\n       'Total Rain Flag', 'Total Snow (cm)', 'Total Snow Flag',\n       'Total Precip (mm)', 'Total Precip Flag', 'Snow on Grnd (cm)',\n       'Snow on Grnd Flag', 'Dir of Max Gust (10s deg)',\n       'Dir of Max Gust Flag', 'Spd of Max Gust (km/h)',\n       'Spd of Max Gust Flag'],\n      dtype='object')\n\n\nWe only need to keep the Date/Time and Mean Temp (¬∞C) variables, so let‚Äôs subset the dataframe to only those columns:\n\ndf_weather_simp = df_weather[[\"Date/Time\", \"Mean Temp (¬∞C)\"]]\ndf_weather_simp.head()\n\n\n\n\n\n\n\n\nDate/Time\nMean Temp (¬∞C)\n\n\n\n\n0\n2022-01-01\n1.5\n\n\n1\n2022-01-02\n-6.3\n\n\n2\n2022-01-03\n-8.4\n\n\n3\n2022-01-04\n-1.2\n\n\n4\n2022-01-05\n0.2\n\n\n\n\n\n\n\nNow let‚Äôs join the weather dataframe with our original bike share dataframe on their respective date/time columns. First, let‚Äôs remind ourselves which column in the df bike share dataframe has the date of the trip.\n\ndf.head(2) # only show the first 2 rows\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\nTrip Duration Minutes\nStart Date\nDate Only\ndate\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n72.25\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n2.10\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n\n\n\n\n\n\n\nThe Start Date column is the one we want to use ‚Äì Start Time also contains the date but we don‚Äôt need to know what time the trip started, just the date. We have to make sure that this variable has the same data type in each of the dataframes ‚Äì if not, the join (merge) won‚Äôt work.\n\ndf['Start Date'].dtype\n\ndtype('&lt;M8[ns]')\n\n\n\ndf_weather_simp['Date/Time'].dtype\n\ndtype('O')\n\n\nThe variables have different types, so we need to convert them so they‚Äôre the same type:\n\n# Create new 'Date Only' column by extracting only the date from the datetime variable in the bike share data\ndf['date'] = df['Start Date'].dt.date\n\n# Convert the 'Date/Time' variable from string to date \ndf_weather_simp.loc[:, 'Date/Time'] = pd.to_datetime(df_weather_simp['Date/Time']).dt.date\n\n\ndf['date'].dtype\n\ndtype('O')\n\n\n\ndf_weather_simp['Date/Time'].dtype\n\ndtype('O')\n\n\nNow we can merge the dataframes, using the date column in the bike share dataset and the Date/Time column in the weather dataset.\n\ndf_ridership_weather = df.merge(df_weather_simp, \n                                left_on=\"date\", \n                                right_on=\"Date/Time\")\n\ndf_ridership_weather.head(2)\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\nTrip Duration Minutes\nStart Date\nDate Only\ndate\nDate/Time\nMean Temp (¬∞C)\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n72.25\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n2022-01-01\n1.5\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n2.10\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n2022-01-01\n1.5\n\n\n\n\n\n\n\nLet‚Äôs first explore the average temperature for each day in January 2022 by creating a line chart.\n\nfig, ax = plt.subplots(figsize=(11, 5))\n\nsns.lineplot(data=df_ridership_weather, \n             x='date', \n             y='Mean Temp (¬∞C)',\n             marker = \"o\"\n             ).set(title = \"Daily Average Temperature in January 2022\")",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#scatter-plot-with-trend-line",
    "href": "urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html#scatter-plot-with-trend-line",
    "title": "Exploratory data visualization",
    "section": "Scatter plot with trend line",
    "text": "Scatter plot with trend line\nAre bike share trips more common when the weather is warmer?\nNext, we‚Äôll calculate the number of trips (i.e., rows) for each date, and then create a scatter plot of the number of trips versus the average temperature for every day in January 2022. We‚Äôll include a trend (or ‚Äúregression‚Äù) line to illustrate the relationship between the two variables.\n\n# Count the number of rows (trips) by date\ndf_ridership_weather['Count'] = df_ridership_weather.groupby('date')['date'].transform('count')\n\nsns.regplot(data=df_ridership_weather, \n            x=\"Mean Temp (¬∞C)\", \n            y=\"Count\", \n            scatter_kws={\"s\": 8}\n            ).set(title = \"Number of Bike Share Trips vs Daily Temperature in January 2022\")\n\n\n\n\n\n\n\n\nThe trend line has a positive slope, indicating there is a positive relationship between average daily temperature and number of trips.\nWe can also determine how highly correlated the two variables are by calculating the Pearson correlation coefficient using the pearsonr function from the stats module of the scipy library.\n\nfrom scipy.stats import pearsonr\n\npearsonr(df_ridership_weather[\"Mean Temp (¬∞C)\"], df_ridership_weather[\"Count\"])\n\nPearsonRResult(statistic=0.6499094259264812, pvalue=0.0)\n\n\nThe Pearson correlation coefficient (0.65) is positive and relatively close to 1, which means there is a moderately strong positive linear relationship between the number of daily trips and the temperature. A value of 0 would indicate no correlation, while a value close to -1 would indicate a strong negative relationship.\nThe p-value is very small (&lt;0.05), meaning the correlation is statistically significant (i.e., there is strong evidence that the correlation is real).\nWe can conclude that our hypothesis was correct: more people tend to use bike share when it‚Äôs warmer. However, given that we only looked at the correlation between these two variables, we are not able to establish causation. In other words, we can say that there is a relationship, but we cannot say why. We are just in the exploratory phase of the data analysis process, and to say anything more about the reason for the relationship we would need to do more in-depth modeling.\nThere are different ways to replicate this analysis In Excel or Google Sheets, though it will require additional steps. Here is one way to replicate the analysis:\n\nFirst, as noted above, open the weather dataset and clean it so as to only keep the most relevant columns (date and temperature).\nThen, as noted above, make sure that the variable you will use to join the datasets - the date - is formatted in the same way in both datasets. This means you will need to adjust the formatting in the bike share dataset to match the way the date variable is formatted in the weather dataset.\nThen, join the two datasets using the VLOOKUP function. You can review the previously linked VLOOKUP tutorial here.\nOnce your two datasets are joined, create a new pivot table to summarize the number of trips per day. Drag the ‚Äòdate‚Äô field into your ‚ÄòRows‚Äô box, and the ‚Äòtrip ID‚Äô into your ‚ÄòValues‚Äô box, and make sure to select ‚ÄòCount‚Äô.\nCopy-paste your pivot table into a new tab. Now, join your temperature variable back into this new table/tab. In other words, you are trying to create a table where each row is a unique day, and the columns are the number of trips, and average temperature.\nFrom here, select the appropriate data array to create a scatter plot that charts the number of trips against average temperature. As explained previously, you can do this by clicking on ‚ÄòInsert‚Äô under Chart Type, then selecting ‚ÄòScatter Plot‚Äô.\nOnce your scatter plot is displayed, you can add a linear trendline, model equation, and R-square value as described previously in this textbook.\nFinally, you can manually calculate the Pearson correlation coefficient using the =PEARSON function.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization"
    ]
  },
  {
    "objectID": "urban-data-visualization/flow-maps/flow-maps.html",
    "href": "urban-data-visualization/flow-maps/flow-maps.html",
    "title": "Flow maps",
    "section": "",
    "text": "Jeff Allen\nFlow maps are one such way that we can visualize connections and travel between different sets of locations.\nIn the example below, we are mapping travel to hospitals in Calgary. Each line represents a single trip from someone‚Äôs home neighbourhood to a hospital, visualized with a very faint transparency, but when overlaid on top of each other, they highlight overall patterns of visits to these five hospitals.\nCharts like these are sometimes called hub-and-spoke maps or spider diagrams. In the tutorial that follows, we will learn about the data structures behind these maps and how to visualize them, specifically replicating this example.",
    "crumbs": [
      "Urban Data Visualization",
      "Flow maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/flow-maps/flow-maps.html#prerequisites",
    "href": "urban-data-visualization/flow-maps/flow-maps.html#prerequisites",
    "title": "Flow maps",
    "section": "Prerequisites",
    "text": "Prerequisites\nPrior knowledge of pandas, geopandas, QGIS, and Inkscape (or similar graphic design software) would be helpful for the following tutorial. Here are the links to download this notebook and data.\n\nJupyter notebook\nDatasets\n\nIf you are running the notebook and/or script locally (generally recommended), you will need to use the following libraries. You‚Äôll have to install them (e.g.¬†via pip or conda) if you do not have them installed already.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import LineString\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Urban Data Visualization",
      "Flow maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/flow-maps/flow-maps.html#origin-destination-matrices",
    "href": "urban-data-visualization/flow-maps/flow-maps.html#origin-destination-matrices",
    "title": "Flow maps",
    "section": "Origin-destination matrices",
    "text": "Origin-destination matrices\nOrigin-Destination matrices encode many people travel from one set of locations (e.g.¬†neighbourhoods) to another set of locations (e.g.¬†hospitals). These are sometimes called flow or trip tables.\nIn urban geography applications, this data often represents people travelling in the city, to or from specific types of places. But it can also be used to represent the transport or travel of other phenomena e.g.¬†products, money, wildlife, etc.\nThis type of data can be based on real observations e.g.¬†from administrative data that includes visiting patients‚Äô addresses or a survey that asks about daily travel. The data can also be simulated, i.e.¬†estimated, based on a model that predicts trips between locations.\nThe example data that we are going to look at below is the latter, ersatz data of trips to hospitals in Calgary. Let‚Äôs load it and take a look. The data have 3 columns; the first is the unique ID of a Census Dissemination Area (DA), the second is the ID of a hospital, and the third column is the number of visits from each DA to each hospital.\n\nod = pd.read_csv(\"data/od-flows.csv\")\nod.tail(5)\n\n\n\n\n\n\n\n\ndauid\nhospital_id\ntrips\n\n\n\n\n8370\n48062794\n0\n19.0\n\n\n8371\n48062794\n1\n41.0\n\n\n8372\n48062794\n2\n4.0\n\n\n8373\n48062794\n3\n0.0\n\n\n8374\n48062794\n4\n16.0\n\n\n\n\n\n\n\nWhen working with data like this, it is sometimes the case that the data are available in a wide format, where the rows pertain to the origin (e.g.¬†Dissemination Area) and the columns pertain to the destination (e.g.¬†hospitals), instead of the long format shown above.\nIn pandas, it‚Äôs super easy to pivot from long to wide.\n\nod_wide = od.pivot_table(index='dauid', columns='hospital_id', values='trips')\nod_wide.tail(5)\n\n\n\n\n\n\n\nhospital_id\n0\n1\n2\n3\n4\n\n\ndauid\n\n\n\n\n\n\n\n\n\n48062790\n39.0\n49.0\n7.0\n0.0\n31.0\n\n\n48062791\n37.0\n3.0\n11.0\n0.0\n15.0\n\n\n48062792\n25.0\n2.0\n9.0\n0.0\n11.0\n\n\n48062793\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n48062794\n19.0\n41.0\n4.0\n0.0\n16.0\n\n\n\n\n\n\n\nAnd here‚Äôs how to melt from wide to long if you need to.\n\nod_wide.reset_index().melt(\n    id_vars='dauid', \n    value_vars=[0,1,2,3,4], \n    var_name=\"hospital_id\", \n    value_name=\"population\"\n).head(5)\n\n\n\n\n\n\n\n\ndauid\nhospital_id\npopulation\n\n\n\n\n0\n48060056\n0\n20.0\n\n\n1\n48060057\n0\n16.0\n\n\n2\n48060058\n0\n20.0\n\n\n3\n48060059\n0\n14.0\n\n\n4\n48060060\n0\n24.0\n\n\n\n\n\n\n\nWe also have location data for the centroids of Dissemination Areas (DAs) and the hospitals. Let‚Äôs load these and quickly plot them to take a look:\n\nda = gpd.read_file(\"data/da-2021-centroids.geojson\")\nda_poly = gpd.read_file(\"data/da-2021-polygons.geojson\")\nhp = gpd.read_file(\"data/hospital-locations.geojson\")\n\nfig, ax = plt.subplots(figsize=(6,6))\nda_poly.plot(\n    ax = ax,\n    edgecolor = 'LightGray',\n    linewidth = 0.5,\n    color = \"White\"\n)\nda.plot(\n    ax = ax,\n    color = 'LightBlue',\n    markersize = 4\n)\nhp.plot(\n    ax = ax,\n    color = 'Black',\n    markersize = 22\n).set_axis_off()",
    "crumbs": [
      "Urban Data Visualization",
      "Flow maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/flow-maps/flow-maps.html#generating-flow-lines",
    "href": "urban-data-visualization/flow-maps/flow-maps.html#generating-flow-lines",
    "title": "Flow maps",
    "section": "Generating flow lines",
    "text": "Generating flow lines\nFor a flow map, we want to connect the two sets of points, and then style the lines based on the weight connecting them, in this case, the number of trips between them.\nFor this, we are essentially creating a straight line geometry for every row in od. We can do this by first joining in the coordinates to the od flow matrix\n\nda['dauid'] = da['name'].astype('int64')\nodm = od.merge(\n    da, \n    how='left', \n    on = \"dauid\"\n).merge(\n    hp, \n    how='left', \n    left_on='hospital_id',\n    right_on='id'\n)\n\nThen we use some shapely magic, specifically the LineString function to combine two point geometries into a line geometry. The lambda function (a small single expression function) applies this to each row in the GeoDataFrame. (Check out this tutorial for using Lambda functions in pandas)\n\nodm = gpd.GeoDataFrame(\n    {\n        'dauid': odm['dauid'],\n        'hospital_id': odm['hospital_id'],\n        'trips': odm['trips'],\n        'geometry': odm.apply(\n            lambda x: LineString([x['geometry_x'], x['geometry_y']]), axis=1\n        )\n    }\n).set_crs(epsg=\"4326\")\n\n# saving to file\nodm.to_file(\"data/od-flow-lines.geojson\", driver=\"GeoJSON\")\n\nLet‚Äôs plot the result!\n\nfig, ax = plt.subplots(figsize=(6,6))\nodm.plot(\n    ax = ax,\n    linewidth = 0.25,\n    color = 'Black'\n).set_axis_off()",
    "crumbs": [
      "Urban Data Visualization",
      "Flow maps"
    ]
  },
  {
    "objectID": "urban-data-visualization/flow-maps/flow-maps.html#visualizing-flow-lines",
    "href": "urban-data-visualization/flow-maps/flow-maps.html#visualizing-flow-lines",
    "title": "Flow maps",
    "section": "Visualizing flow lines",
    "text": "Visualizing flow lines\nThat‚Äôs a bit of a mess, but it looks like the lines are all there. There are a few simple ways to distinguish lines with more trips, and dim those with fewer. The first is to have the line width be a function of the number of trips\n\nfig, ax = plt.subplots(figsize=(6,6))\nodm.plot(\n    ax = ax,\n    linewidth = odm[\"trips\"] / 1000,\n    color = 'Black'\n).set_axis_off()\n\n\n\n\n\n\n\n\nWe can also plot based on the function of the opacity of the line (i.e.¬†less transparency if the connection has more trips). In the plot function, alpha controls the opacity, and ranges from 0 to 1. To normalize the trips column, we divide by its max value.\n\nfig, ax = plt.subplots(figsize=(6,6))\nodm.plot(\n    ax = ax,\n    linewidth = 1,\n    alpha = odm['trips'] / odm['trips'].max(),\n    color = 'Black'\n).set_axis_off()\n\n\n\n\n\n\n\n\nLet‚Äôs try to better distinguish flows to each hospital, i.e.¬†to better show what the catchment area is of visitors to each. We can do this with the help of a little colour! Below we combine categorical colouring with dynamic opacity styling used above.\n\n# specifying the colour and opacity of each line\n\ncolour_mapping = {\n    0: \"#DC4633\",  \n    1: \"#8DBF2E\",\n    2: \"#F1C500\",\n    3: \"#00A189\",\n    4: \"#6FC7EA\",\n}\nodm[\"colour\"] = odm[\"hospital_id\"].map(colour_mapping)\n\nodm[\"opacity\"] = odm[\"trips\"] / 250\nodm['opacity'] = odm['opacity'].clip(upper=1)\n                   \n# plotting them all on a single plot!\n    \nfig, ax = plt.subplots(figsize=(7,7), facecolor='black')\n\nda_poly.plot(\n    ax = ax,\n    edgecolor = '#484848',\n    linewidth = 2,\n    color = \"Black\"\n)\nda_poly.plot(\n    ax = ax,\n    edgecolor = '#2B2B2B',\n    linewidth = 0.5,\n    color = \"Black\"\n)\n\nfor hospital in [0,1,2,3,4]:\n    odp = odm[odm[\"hospital_id\"] == hospital]\n    odp.plot(\n        ax=ax, \n        linewidth=1, \n        alpha=odp[\"opacity\"], \n        color=odp['colour'], \n        zorder = 1\n    )\n    \nfor hospital in [0,1,2,3,4]:\n    odp = odm[odm[\"hospital_id\"] == hospital]\n    odp.plot(\n        ax=ax, \n        linewidth=2, \n        alpha=odp[\"opacity\"] / 3, \n        color=odp['colour'], \n        zorder = 2\n    )\n    \nhp.plot(\n    ax = ax,\n    color = 'White',\n    markersize = 40,\n    zorder = 3\n)\n\nax.set_title(\n    'Travel to Hospitals in Calgary', \n    fontsize=10,\n    loc = \"left\"\n).set_color('LightGray')\n\nax.set_axis_off()\n\nfig.savefig('images/calgary-hospital-map-all-python-export.png')\n\n\n\n\n\n\n\n\nThe above map views nicely, but it can be a bit difficult to parse out travel to any specific hospital. One solution to this are small multiples, where we create 5 smaller plots, each focusing on a specific hospital.\n\nfig, ax = plt.subplots(ncols=5, nrows=1, figsize=(15,5), facecolor='black')\n\nfor hospital in [0,1,2,3,4]:\n    \n    # data layers\n    \n    odp = odm[odm[\"hospital_id\"] == hospital]\n    da_poly.plot(\n        ax = ax[hospital],\n        edgecolor = '#484848',\n        linewidth = 2,\n        color = \"Black\"\n    )\n    da_poly.plot(\n        ax = ax[hospital],\n        edgecolor = '#2B2B2B',\n        linewidth = 0.5,\n        color = \"Black\"\n    )\n    odp.plot(\n        ax=ax[hospital], \n        linewidth=1, \n        alpha=odp[\"opacity\"], \n        color=odp['colour'], \n        zorder = 1\n    )\n    hp[hp[\"id\"] == hospital].plot(\n        ax = ax[hospital],\n        color = 'White',\n        markersize = 10,\n        zorder = 3\n    ).set_axis_off()\n    \n    # titles\n    \n    ax[hospital].set_title(\n        hp.loc[hospital, 'facility_name'], \n        fontsize=8,\n        loc='center'\n    ).set_color('White')\n\n# overall title and save to file\n    \nfig.suptitle(\n    'Travel to Hospitals in Calgary', \n    fontsize=13\n).set_color('White')\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\nfig.subplots_adjust(top=0.9, bottom=0.1)\n\nfig.savefig('images/calgary-hospital-map-multipes-python-export.png')",
    "crumbs": [
      "Urban Data Visualization",
      "Flow maps"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Urban Data Storytelling",
    "section": "",
    "text": "Welcome to the Urban Data Storytelling online textbook, written and created by the School of Cities at the University of Toronto.\nUrban data storytelling is the process and practice of using data to craft compelling narratives about cities, in order to communicate key insights, inform policy-making, build public will, or advocate for change.\nUrban data storytelling combines data analytics, data visualization, and narrative techniques to make complex urban trends understandable and engaging for specific audiences, such as policymakers, funders, or community members.\nThis online textbook has three core modules, each composed of one or more notebooks or tutorials."
  },
  {
    "objectID": "index.html#contributors-and-citing",
    "href": "index.html#contributors-and-citing",
    "title": "Urban Data Storytelling",
    "section": "Contributors and citing",
    "text": "Contributors and citing\nThis online textbook was compiled by Jeff Allen using Quarto with content contributions from (in alphabetical order by last name) Jeff Allen, Karen Chapple, Isabeaux Graham, Julia Greenberg, Aniket Kali, Evelyne St.¬†Louis, Nate Wessel, and Michelle Zhang. Each page lists its authors.\nIf you want to cite this online textbook, here is the recommended citation:\nAllen, J. (Eds.). (2025). Urban Data Storytelling. School of Cities, University of Toronto.\n@book{allen2025urbandatastorytelling,\n  editor    = {Allen, Jeff},\n  title     = {Urban Data Storytelling},\n  year      = {2025},\n  publisher = {School of Cities, University of Toronto},\n  url       = {https://schoolofcities.github.io/urban-data-storytelling/}\n}\nIf you want to cite a specific page, here is an example of a recommended citation:\nGreenberg, J. & St. Louis, E. (2025). Exploratory data visualization. In Allen, J. (Eds.), Urban Data Storytelling. School of Cities, University of Toronto.\n@incollection{greenberg2021dataviz,\n  author    = {Greenberg, Julia and St. Louis, Evelyne},\n  title     = {Exploratory data visualization},\n  booktitle = {Urban Data Storytelling},\n  editor    = {Allen, Jeff},\n  publisher = {School of Cities, University of Toronto},\n  year      = {2025},\n  url       = {https://schoolofcities.github.io/urban-data-storytelling/urban-data-visualization/exploratory-data-visualization/exploratory-data-visualization.html}\n}"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Urban Data Storytelling",
    "section": "License",
    "text": "License\nThis online textbook and its notebooks are licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License.\nYou are free to:\n\nShare ‚Äî copy and redistribute the material in any medium or format\nAdapt ‚Äî remix, transform, and build upon the material\n\nUnder the following terms:\n\nAttribution ‚Äî You must give appropriate credit, provide a link to the license, and indicate if changes were made.\nNonCommercial ‚Äî You may not use the material for commercial purposes.\nShareAlike ‚Äî If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original."
  },
  {
    "objectID": "urban-data-analytics/us-census-data/us-census-data.html",
    "href": "urban-data-analytics/us-census-data/us-census-data.html",
    "title": "Overview of U.S. census data",
    "section": "",
    "text": "Evelyne St.¬†Louis\n\n\n\n\nIn the U.S., the U.S. Census Bureau is responsible for conducting the census every ten years. The last decennial census was conducted in 2020. The census is intended to count every person in the U.S. and U.S. territories. It covers a relatively short set of questions, including basic demographic information such as age, sex, race, Hispanic origin, and owner/renter status.\nThe Census Bureau also conducts additional surveys and programs. One of the most commonly used is the American Community Survey (ACS), which is an annual demographics survey that covers a much wider range of questions typically not included in the full census, such as education, employment, internet access, or transportation. It is important to remember that the ACS is sent to a sample of about 3.5 million household addresses every year. As such, the ACS is reported in estimates for different time periods, most typically 1-year, 3-year, and 5-year periods. For example, you can use the 2013-2018 5-year ACS estimates to describe demographic characteristics for 2018.\nTo learn more about the Census, check out these two video series by the U.S. Census Bureau:\n\nIntroduction to Census Data\nCensus Academy: Discovering the American Community Survey\n\nAs you begin using census data, you will also want to learn more about the different census geographies. Here is an overview of select geographic entities commonly used in the ACS and how they relate to each other. For city or regional-level analyses, commonly used geographies include Counties, Places (which often overlap with a city boundary), Census Tracts, and Census Block Groups.\n\n\n\nU.S. Census Bureau, Geography and the American Community Survey: What Data Users Need to Know (2020) (Source)\n\n\nFor beginners looking to get started with exploring, downloading, or using the census and ACS data, check out these tutorials:\n\ndata.census.gov for Beginners: How to Get Started\nHow to Export and Download Tables in data.census.gov\n\nFor more advanced users, or those seeking to access the ACS programmatically, the Census Bureau API webpage (Application Programming Interface) provides an easy way to search and download data more efficiently:\n\nCensus data API user guide\n\nFinally, note that certain universities and institutions also provide their students or employees access to Social Explorer, a very user-friendly platform for downloading and visualizing Census data. It‚Äôs worth looking into whether your organization would provide you with an account!",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of U.S. census data"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html",
    "href": "urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "Jeff Allen\nA lot of urban datasets are directly linked to specific places, e.g.¬†addresses, streets, neighbourhoods, political or administrative boundaries, etc.\nData that include place-based information are often called spatial, geographic, or geospatial data Geographic Information Systems (GIS) are tools and software for analyzing, processing, and visualizing spatial data.\nThis page will cover the basics of spatial data and how we can view and interact with these data in the software QGIS. You will need to download QGIS to work on the hands-on part of this tutorial. We recommend the Long-Term Release (LTR) version.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#spatial-data",
    "href": "urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#spatial-data",
    "title": "Spatial data and GIS",
    "section": "Spatial data",
    "text": "Spatial data\nA spatial dataset is a combination of‚Ä¶\n\nattribute data (the what)\nlocation data and spatial dimensions (the where)\n\nSpatial data, this combination of attribute and location data, can be organized and represented in a number of different formats.\nFor example, a city can be represented on a map via a single point with a label (e.g.¬†based on latitude and longitude coordinates). Or a city can be represented as a polygon, based on on it‚Äôs administrative boundary\nImportantly, there is always uncertainty about the level of accuracy, precision, and resolution of spatial data. Spatial data are abstractions of reality, and thus have some loss of information when used for visualization and analysis. Any analysis can only be as good as the available data.\nThe two most common forms of spatial data are vector data and raster data.\n\n\n\nExamples of spatial data\n\n\n\nVector data\nVector data uses geographic coordinates, or a series of coordinates, to create points, lines, and polygons representing real-world features.\ne.g.¬†in the map below (a screenshot of OpenStreetMap), lines are used to represent roads and rail, points for retail, polygons for parks and buildings, etc.\n\n\n\nScreenshot of OpenStreetMap\n\n\nSpatial data can be stored as columns in traditional table-based formats (e.g.¬†.csv), but there are also a wide range of data formats specific to spatial data. These are some of the most common:\n\nGeoJSON .geojson\nGeoPacakge .gpkg\nShapefile .shp\nGeodatabase .gdb\n\nHere is an example of a single point location of a city stored in a .geojson file. .geojson is a standardized data schema .json format for spatial data.\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [-80.992, 46.490]\n            },\n            \"properties\": {\n                \"city_name\": \"Sudbury\"\n            }\n        }\n    ]\n}\n\n\nRaster data\nRaster data represents space as a continuous grid with equal cell sizes. Each cell contains a value pertaining to the type of feature it represents. These values can be quantitative (e.g.¬†elevation) or categorical (e.g.¬†type of land use). Common examples of raster data include Digital Elevation Models (DEMs), satellite imagery, and scanned images like historical maps.\ne.g.¬†the map below shows a DEM for Toronto at two different scales.\n\n\n\nDEM of Toronto",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#geographic-information-systems-gis",
    "href": "urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#geographic-information-systems-gis",
    "title": "Spatial data and GIS",
    "section": "Geographic Information Systems (GIS)",
    "text": "Geographic Information Systems (GIS)\nWe use Geographic Information Systems (GIS) to analyze, manipulate, and visualize spatial data.\nWhy is GIS useful?\n\nexplore spatial patterns and relationships in data\ncreate maps for publications\ngenerate new data, either ‚Äúby hand‚Äù or via spatial relationships from other data (e.g.¬†through spatial queries)\nperform spatial analysis (i.e.¬†statistical methods applied to spatial data)\n\nGIS is often thought of as more than just a tool or piece of software. It can refer to all aspects of managing and analyzing digital spatially referenced data.\nThe power of GIS software and tools is the ability to work with data stored in different layers (e.g.¬†a layer for roads, another for buildings, and so on) in conjunction with each other. These layers can be visualized and analyzed relative to each other based on their spatial relationships.\n\n\n\nSimple AI generated schematic of layers\n\n\nGIS software usually links to data stored elsewhere on a computer, rather than in a project file. If the source location of the data (i.e.¬†which folder it‚Äôs in) changes, then this will have to be updated in the GIS project. If data are edited in GIS, it will update the data in its source location.\nThe open-source QGIS and the proprietary ESRI ArcGIS are the two most used desktop GIS software. ESRI‚Äôs suite of tools are often used by larger corporate and government organizations while QGIS is often used by small consultants and freelancers, non-profits, and academia. Many spatial data processing, visualization, and analyses steps also have equivalents in Python, R, SQL and other programming languages via specific libraries\nWe‚Äôll be working with QGIS and Python, because they free and work across multiple operating systems, and are commonly used across different areas of work and research. But pretty much everything we‚Äôll show can be accomplished across different software options, the buttons and steps will just be a bit different.\nLearning the core analytical and visualization concepts and ideas are much more important than exactly where to click or what specific functions to run.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#coordinate-reference-systems-crs-and-map-projections",
    "href": "urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#coordinate-reference-systems-crs-and-map-projections",
    "title": "Spatial data and GIS",
    "section": "Coordinate Reference Systems (CRS) and map projections",
    "text": "Coordinate Reference Systems (CRS) and map projections\nA CRS is a framework for describing the geographic position of location-based data. It tells your GIS software where your data are located on the Earth. Without a CRS, your data would just be a bunch of numbers without an ability to place them on a map and relate them to other data.\nThere are thousands of CRSs. Each CRS has specific attributes about its position and one main set of spatial units (e.g.¬†a CRS will measure location in degrees longitude/latitude, metres, miles, etc.).\nProbably the most common CRS is WGS84, which uses longitude/latitude to link data to the Earth‚Äôs surface.\nWhen doing spatial analyses and processing where we are comparing two or more datasets to each other, it is important that they are in the same CRS.\n\nMap projections\nCRS are often paired with map projections. This is sometimes called a Projected CRS.\nThe earth is a 3D globe, but most maps are represented on 2D surfaces (e.g.¬†on a screen, piece of paper). Map projections are mathematical models to flatten the earth to create a 2D view.\nEvery projection distorts shape, area, distance, or direction in some way or another. Different map projections distort the Earth in different ways. Here are some examples! For more check out this projection transition tool\n\n\n\nQGIS screenshot without data\n\n\nFor doing analysis at an urban scale, it is important that we choose a map projection that conserves area and distances in both X and Y directions (i.e.¬†space is not being distorted in one particular direction more than others).\nFor example, these are two aerial images of Toronto.The left image uses a local Mercator projection which does not distort data at a local scale, while the right image is a rectangular projection where degrees latitude are equal shown at the same scale as degrees longitude.\n\n\n\nScreenshots of OpenStreetMap of Toronto, left is in an area-preserving Mercator projection, right is un-projected and distorted\n\n\nA general recommendation for urban-scale analyses is to use a Universal Transverse Mercator (UTM) projected CRS, which conserves area, distance, and direction along bands of the earth. This is a good tool for finding which UTM zone your region is in.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#working-with-spatial-data-in-qgis",
    "href": "urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#working-with-spatial-data-in-qgis",
    "title": "Spatial data and GIS",
    "section": "Working with spatial data in QGIS",
    "text": "Working with spatial data in QGIS\nLet‚Äôs use QGIS to quickly view a few spatial datasets. When you open up QGIS, it should look something like this:\n\n\n\nQGIS screenshot without data\n\n\nThe Browser panel is used for finding and loading data on your computer or from external sources, each line and symbol is for a different data source. The Layer panel will list all the datasets that are loaded into your project. The big white space will populate with data once it is loaded. The buttons at the top include a variety of options for loading data, saving your project, exploring your map, and various common processing and analytical tools.\n\nLoading data\nLet‚Äôs begin by adding in a basemap. Basemaps are often used to provide geographic reference for other data that we load into QGIS.\nWe‚Äôll add a basemap of OpenStreetMap, a free crowd-sourced map of the world, available as XYZ raster tiles. To do this, go to Layer - Data Source Manager or simply click on the Data Source Manager button highlighted below. When you open the Data Source Manager you have options for a wide variety of data types to add to your map.\n\n\n\nData source manager location in QGIS\n\n\n\n\n\nAdding an XYZ tile\n\n\nHere is the URL to paste into QGIS for adding an OpenStreetMap basemap: https://tile.openstreetmap.org/{z}/{x}/{y}.png.\nWhen you click ‚ÄòAdd‚Äô it should be added to the map. You can use the navigation buttons or just scroll on your mouse to zoom to your city.\n\n\n\nQGIS with OpenStreetMap\n\n\nLet‚Äôs now add some vector data to the map. We‚Äôve pre-downloaded two datasets from the City of Toronto‚Äôs Open Data Portal\n\nCity Wards (polygons)\nLibrary Locations (points)\n\nThese can be added via the Data Source Manager - Vector, or simply via drag-and-drop from the folder of where they are located on your computer.\n\n\n\nQGIS with Toronto data\n\n\n\n\nWorking with layers\nThe data added to the project will be listed in the Layers panel. The order of the layers determine the order of which they stack on top of each other on the map. In the example above, the libraries are listed at the top of the other two layers, and it thus appears on top of the other two layers on the map.\nTo change the layer order, you can drag individual layers to be above or below any other layer in the Layers panel.\nMost vector data has associated attribute data, e.g.¬†the number of a ward, the name of a library, etc. To view this data, right-click on a layer and then click on Open Attribute Table. In GIS, column names are often called ‚ÄúFields‚Äù\n\n\nStyling data\nWhen you load vector data like this into QGIS, the layers have default styling (e.g.¬†colours, sizes, line-widths, etc.).\nThere are tons of options in QGIS to change these initial styles (e.g.¬†colours, size, and symbols). To do so, right-click on a layer, go to Properties, and then Symbology.\nFor example, in the screenshot below, the sizes of the libraries and the line-widths of the wards are increased a bit, the wards are given a transparent fill, and the library symbols are converted to squares.\n\n\n\nQGIS with Toronto data\n\n\nTip! you get extra options if you click on the sub-component of the symbol. e.g.¬†in this example I clicked on Simple Fill to find additional styling options like pattern-based fill styles.\n\n\n\nQGIS symbology example\n\n\nWe‚Äôll explore data-driven styling, where colours or other style options are based on data values, in a future tutorial.\n\n\nChanging projections and CRS in QGIS\nYou can change the project CRS in QGIS by going to Project - Properties - CRS.\n\n\nExporting data\nYou can export and create copies of any of your data layers. This may be useful if you want to change the file format, the CRS, subset the attribute table, or simply just make a copy of the data.\nTo do this, go to Export - Save Features As, and you can adjust the Format, CRS, and other options as needed.\n\n\nSaving a QGIS project\nYou can save a project in QGIS by going to Project and then Save or Save As.\nIt will get saved to a .qgis file. These files save styling, filters, layer orders, map scale and location, and layout settings.\nHowever, .qgis files do not save datasets directly in the file, only paths and links to where the data is stored, either on your computer or from a URL like the OpenStreetMap basemap. Saving only links to data is common in other tools and software.\nTherefore, if you share your project, you‚Äôll need to also share any local datasets it is linking to.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html",
    "href": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html",
    "title": "Introduction to urban data",
    "section": "",
    "text": "Karen Chapple, Julia Greenberg, Jeff Allen",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#where-does-data-come-from",
    "href": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#where-does-data-come-from",
    "title": "Introduction to urban data",
    "section": "Where does data come from?",
    "text": "Where does data come from?\nIn data analysis, understanding the type and origin of data is essential to choosing the right methods for analysis, limitations in the data, and interpreting results. The following categories outline common types of data sources that we often encounter. These sources vary in how and why they are collected.\n\n\n\n\n\n\n\n\nData source type\nDescription\nExamples\n\n\n\n\nDesigned/Survey\nData collected through surveys, experiments, or designed methods.\nCensus data (historical or current), research surveys, opinion polls\n\n\nAdministrative\nData from routine operations or official records. Often open or public when government-run.\nCity records (public housing locations, public transit data), tax records, healthcare data (historical and current), school enrollment data\n\n\nCrowdsourced\nData contributed by the public or community-driven platforms.\nOpenStreetMap (OSM), Wikipedia, social media data, 311 data, Google reviews\n\n\nEvent-Driven/Real-Time\nData generated from sensors, transactions, or interactions. Often continuous and time-sensitive.\nSatellite data, mobile phone GPS, IoT sensors, e-commerce transactions, website clicks\n\n\nDerived data\nData created by transforming or calculating metrics from existing data or simulations.\nIndices like low-income prevalence or social deprivation, environmental quality scores, simulated datasets for testing or model training\n\n\n\nSometimes data can be a combination, for example, both the United States Census Bureau and Statistics Canada collect a combination of survey data and administrative data.\n\n\n\n\nMap of average household income in Toronto using data from Statistics Canada (2020)",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-availability",
    "href": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-availability",
    "title": "Introduction to urban data",
    "section": "Data availability",
    "text": "Data availability\nData availability refers to how accessible data is and the conditions under which it can be used. Key categories include:\nPublic / Open data - Data that is freely accessible to anyone and can be reused without restrictions. Open data is often provided by governments, research institutions, and public organizations. It is typically non-sensitive and available in machine-readable formats. Examples include aggregated census data, municipal open data, and OpenStreetMap.\nRestricted data Data that is available but comes with limitations due to privacy, security, or legal concerns. This includes sensitive datasets such as health data, dis-aggregate census data, and government records with personal identifiers that are redacted or protected.\nProprietary data - Data that is owned by a specific entity (e.g., a corporation or private organization) and is not freely available. Access is typically granted through licenses, paid subscriptions, or agreements. For example, cell phone mobility data from Spectus can be used to measure post-pandemic downtown recovery trends, real estate data from Costar can be used to assess vacancy rates or rent prices, and consumer data from Data Axle can be used to study the impact of new housing on migration patterns.\n\n\n\n\nCrowdsourced OpenStreetMap data in Vancouver",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-sources-for-urban-analysis",
    "href": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-sources-for-urban-analysis",
    "title": "Introduction to urban data",
    "section": "Data sources for urban analysis",
    "text": "Data sources for urban analysis\nThe table below lists a handful of websites where you can find publicly available data about cities, the environment, land use, transportation, Indigenous communities, housing and homelessness. This is a non-exhaustive list; there are many other great data sources available. Note that municipalities‚Äô open data portals typically contain information on all or most of these topics.\n\n\n\nTopic\nData sources (Global/Other)\nData sources (Canada-specific)\nData sources (U.S.-specific)\n\n\n\n\nDemographic\n\n- Canadian census data\n- U.S. census data  (this is a user-friendly census tool; some universities and institutions have licenses)\n\n\nMunicipal\n\n- Open data portals (e.g., Toronto, Montreal, or Vancouver)\n- Open data portals (e.g., Boston, New York, or San Francisco)\n\n\nEnvironment and health\n\n- NASA‚Äôs Earth Science Data Systems (ESDS) Program - The Canadian Urban Environmental Health Research Consortium - Natural Resources Canada - Environment and Climate Change Canada\n- Center for Disease Control (CDC) PLACES data - U.S. Environmental Protection Agency  - U.S. Geological Survey - Elevation data for the entire U.S. - U.C. Berkeley Earth Sciences & Map Library - U.C. Berkeley Geospatial Innovation Facility\n\n\nLand use and built environment\n- OpenStreetMap - OSMNx - tool for extracting Open Street Map data for anywhere in the world - Google Maps data access\n- Land cover of Canada - Municipal-level zoning maps (e.g., in Toronto)\n- OSMNx-derived downloads for the U.S. - Building footprints for the entire U.S. - National Zoning Atlas - U.C. Berkeley Geospatial Innovation Facility\n\n\nTransportation\n- Transitland\n- Metrolinx Open Data for the Greater Toronto Area - Canadian Urban Transit Association - Mobilizing Justice Hub\n- National Transit Database - National Household Travel Survey - Bike share data (e.g.¬†from Bay Wheels in San Francisco) - Transportation crash/injury data (e.g.¬†MassDOT, California)\n\n\nIndigenous communities\n- Native Land Digital\n- First Nations Data Centre\n\n\n\nHousing and homelessness\n- Airbnb data (Inside Airbnb)\n- Housing - Statistics Canada - Housing data from Canada Mortgage and Housing Corporation\n- U.S. Department of Housing and Urban Development - Federal Housing Finance Agency - Zillow housing data - Urban Displacement data\n\n\nEconomy and commerce\n- Yelp open data\n\n- U.S. Bureau of Labor Statistics - Longitudinal Employer-Household Dynamics (LEHD) - Federal Reserve Economic Data (FRED) - Annual Business Survey (ABS) - County Business Patterns\n\n\nOther\n- Google Dataset Search - ArcGIS Open Data Portal\n- Open Government Canada (federal open data portal)\n- U.S. Data.gov Open Data Portal - Harvard Dataverse\n\n\n\nWeb scraping, or extracting information from the internet, is another method for creating datasets. Since the data do not already exist and must be created, this can be more time-intensive than using existing datasets. However, packages like beautifulsoup or selenium in Python make this process easier.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-formats",
    "href": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-formats",
    "title": "Introduction to urban data",
    "section": "Data formats",
    "text": "Data formats\nData format refers to how data is stored and structured. In practice, this is most relevant when loading and saving data. The data format you choose to use depends on the data‚Äôs size, structure, use, how it is being stored, and whether it is spatial (has a geometry column) or not.\nSome of the most common data formats for non-spatial data are:\n\nCSV (comma separated values) .csv\nExcel .xlsx\nJSON (JavaScript Object Notation) .json\nXML (Extensible Markup Language) .xml\n\nSome of the most common data formats for spatial data are (see Spatial data & GIS for more information):\n\nGeoJSON .geojson\nGeoPackage .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nWhile the file formats above suffice for relatively small or simple datasets, very large or complex datasets require more efficient storage via formats like Parquet (see instructions for Python). Relational databases are another commonly used data storage format for ‚Äúbig data‚Äù because they are more efficient, faster to query, more secure, and can be accessed by multiple users.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-types",
    "href": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-types",
    "title": "Introduction to urban data",
    "section": "Data types",
    "text": "Data types\nIt‚Äôs important to make sure that each variable in your dataset is in the right format so the computer interprets it correctly. For example, if you load a .csv file with a column representing the population of a neighbourhood, you would want to make sure this variable is interpreted as a number and not a string of characters so you can easily use this column to calculate additional statistics (e.g.¬†sum of population in all neighbourhoods, population density in each neighbourhood).\nSee the table below for a list of common data types used in data analysis software.\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample\n\n\n\n\nInteger (int)\nWhole numbers without decimal points.\n5, -3, 42\n\n\nFloat\nNumbers with decimal points, representing real numbers.\n3.14, -0.001, 2.718\n\n\nString (str)\nA sequence of characters, used for textual data.\n\"Hello\", \"Data analysis\", \"123\"\n\n\nBoolean (bool)\nRepresents binary values: True or False.\nTrue, False\n\n\nList\nOrdered collection of items, can contain different data types.\n[1, 2, 3], [\"apple\", \"banana\"]\n\n\nTuple\nImmutable sequence of items, like lists but cannot be modified.\n(1, 2, 3), (\"apple\", \"banana\")\n\n\nDictionary (dict)\nCollection of key-value pairs, often used for mapping.\n{\"country\": \"Alice\", \"age\": 30}\n\n\nDateTime\nUsed for representing date and time.\n2025-04-09 14:32:00, 2021-01-01\n\n\nSet\nUnordered collection of unique items.\n{1, 2, 3}, {\"apple\", \"banana\"}\n\n\nNoneType\nRepresents the absence of a value, or null.\nNone\n\n\n\nDifferent software (e.g.¬†Excel, Python, R, QGIS, etc.) might have slightly different names for each of the above. For example, this page provides a nice summary of data types in Python.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#levels-of-measurement",
    "href": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#levels-of-measurement",
    "title": "Introduction to urban data",
    "section": "Levels of measurement",
    "text": "Levels of measurement\nWhile data types specify what kind of data a variable can hold, levels of measurement describe how data is structured and the relationships between different values. They refer to how we can classify and interpret the data in terms of its inherent ordering, spacing, and possible mathematical operations.\n\nNominal: Categorical data with no inherent order (e.g., colors, countries, land-use types e.g.¬†Urban, Wetlands, Forest, etc.).\nOrdinal: Data with a meaningful order, but unknown or not always equal differences between values (e.g., movie ratings like Good,Okay, or Bad, or levels of education e.g.¬†High School, Bachelors, Masters ).\nInterval: Ordered data with equal intervals between values but no true zero (e.g., temperature in Celsius or Fahrenheit, datetime).\nRatio: Ordered data with equal intervals and a true zero point, allowing for meaningful ratios (e.g., length, area, income).",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#software-and-tools",
    "href": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#software-and-tools",
    "title": "Introduction to urban data",
    "section": "Software and tools",
    "text": "Software and tools\nThere are hundreds of software and tools for processing, analyzing, and visualizing data.\nWhen choosing which software to use to analyze or visualize data, one of the main considerations is whether the software is open source or proprietary. Open source software has its source code publicly available and can be modified by anyone on the internet. Proprietary software‚Äôs source code is not publicly available, is typically developed and updated by a closed group, and is licensed to users in exchange for payment. Read this article if you‚Äôre interested in learning more about the difference between the two.\nSpreadsheet software, like Excel, Google Sheets, and LibreOffice calc, can be great for viewing and exploring data and doing quick analyses. However, they can be very limited for serious data analysis. They struggle with large datasets, lack robust tools for cleaning and transforming complex data, have very limited options for spatial/geographic data, and make reproducibility nearly impossible. Plus, it‚Äôs easy to introduce silent errors with formulas or copy-pasting. For anything beyond basic summaries or charts, scripting languages like R or Python are far more powerful, reliable, and scalable.\nGeographic information systems (GIS) are tools and software specifically designed for analyzing, processing, and visualizing spatial data. There are desktop point-and-click software like QGIS that are great for exploratory and smaller-scale analyses and visualization, and programming languages like Python have many libraries for working with spatial data.\nIn this course, other than common spreadsheet software, we will focus on open source software because it is free and available to everyone. Below is a list of some of the main open source programming languages, software, and tools that we recommend using for data analysis and mapping.\n\n\n\nPurpose\nSoftware/Tools\n\n\n\n\nAnalyzing and visualizing data\nPython, R, SQL\n\n\nMaking pretty graphics\nInkscape, GIMP\n\n\nWeb development\nHTML, CSS, Javascript\n\n\nWeb-based maps and visualization\nD3, MapLibre, Protomaps\n\n\nHosting / project management\nGitHub",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-analysis-process",
    "href": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-analysis-process",
    "title": "Introduction to urban data",
    "section": "Data analysis process",
    "text": "Data analysis process\nWhile is no set of specific step-by-step instructions for data analysis ‚Äì each project involves unique data sources, variables, methodologies, and outputs ‚Äì there is a general framework that we recommend following:\n\nDefine the problem or research question. What question are you trying to answer with data? Is data analysis the best way to answer that question? Who is the audience for your data analysis, and what do they want to know?\nCollect data. What kind of data do you need to answer your research question, and where can you find it? Does it exist? In what format?\nClean data. Make sure the data has appropriate variable names, does not have misspellings or other errors, and the variables are the correct data types. Get rid of any redundant or irrelevant data that you don‚Äôt need, and determine a method for dealing with any missing values.\nAnalyze data. Start by exploring the data to understand its structure and any statistical patterns. Then perform your analysis ‚Äì for example, are you trying to uncover trends, or measure relationships among variables?\nVisualize data. Create plots, maps, or other visual representations that illustrate the structure, trends, or relationships present in your data.\nPresent data. Clearly communicate your results to your intended audience. This could involve writing a report, or creating a presentation or interactive dashboard. Whatever gets your message across!\n\nIt is important to iterate through some of these steps and make updates as needed. For example, if step 5 (visualize your data) reveals that you have an imbalanced dataset, you may need to go back to step 3 or 4 to address this. And of course once you hit steps 5 and 6, you might discover something so interesting that you‚Äôll want to collect more data and repeat the process all over again! :)",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#getting-help",
    "href": "urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#getting-help",
    "title": "Introduction to urban data",
    "section": "Getting help",
    "text": "Getting help\nLearning new software for data analysis or mapping can be confusing and frustrating. Luckily, there are a lot of great resources that can help!\nThe first place you should look when you‚Äôre confused about how to do something is the official documentation. For example, if you‚Äôre having trouble loading a CSV file in Python using the pandas package, take a look at the documentation for .read_csv on the pandas website. Or if you‚Äôre not sure how to create a spatial buffer in QGIS, check out the QGIS buffer operations page.\nIf you‚Äôre still stuck on a question, Google it! Chances are, someone else has dealt with a similar issue, and there is likely a community of people helping them out. For example, one of the most popular resources for coding is Stack Overflow, a website where programmers ask and answer questions. Responses with the most votes are shown at the top, making it easy to find helpful code snippets and explanations that you can adapt for your own needs. The website is so widely used that Stack Overflow posts will usually show up towards the top when you Google search coding questions.\nThere are also websites like W3Schools and GeeksforGeeks that offer online courses and tutorials covering everything from sorting a list in Python to building complicated statistical models. These websites show up often as results for relevant Google searches.\nAnd of course there are AI chatbots like ChatGPT. These tools can be extremely helpful for debugging code, writing code, or providing instructions for GIS. However, be careful and don‚Äôt trust them blindly, as they are often wrong, and sometimes make up packages or functions that don‚Äôt exist. Also, if you use chatbots for help, make sure you understand what they are telling you. Asking for guidance or hints about specific, discrete questions is much better than asking the chatbot to write an entire Python script for you.\nThe more you rely on chatbots, the less you will learn, and the less you will be able to do on your own. Learning data analysis or coding in particular can feel like an uphill battle, but if you start with a solid understanding of what you‚Äôre doing, you‚Äôll be better equipped to prompt and efficiently use chatbots later on. Most importantly, you will have a stronger foundation of knowledge and will be able to tackle complicated problems in the future.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html",
    "href": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html",
    "title": "Spatial data in Python",
    "section": "",
    "text": "Jeff Allen, Aniket Kali\nDownload this notebook and data\nThere are a variety of libraries, methods, and functions for working with spatial data in Python.\nGeoPandas is a library for working with spatial (typically geographic) data. It is used a lot since it extends the functionality of pandas to support spatial data types and operations, making it easier to analyze, visualize, and manipulate spatial data.\nMany of the tasks that are typically done within a GIS can be done via geopandas. And often, it is preferable to work in geopandas versus point-and-click GIS (e.g.¬†QGIS) since it‚Äôs easier to chain together and automate processes, have access to the full capabilities of pandas and other Python libraries, as well as can be better scaled for handling very large datasets.\nThis tutorial provides an introduction to geopandas, its geometry data types, how to quickly plot spatial data, and methods for spatial calculations (e.g.¬†computing areas, bounding boxes, etc.). The last section of this tutorial provides direction for next steps, including libraries and examples for doing advanced spatial data processing, analyses, and visualizations.\nHere are the links to download this notebook and example data:\nThis tutorial assumes you have base knowledge of working with spatial data (e.g.¬†in GIS) and working with dataframes in pandas. Check out the following if you want a refresher:\nWe‚Äôll begin this tutorial by loading in pandas, geopandas, and matplotlib (the latter for showing how to quickly plot and view data). If you don‚Äôt have these installed already, you‚Äôll have to install them via pip or conda.\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#loading-and-exploring-geometric-data",
    "href": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#loading-and-exploring-geometric-data",
    "title": "Spatial data in Python",
    "section": "Loading and exploring geometric data",
    "text": "Loading and exploring geometric data\nGeospatial data represents real-world features using three primary geometric types:\n\nPoints: Single (x,y) coordinates for discrete locations like transit stops or landmarks.\nLines: Connected sequences of points forming paths, such as roads or rivers.\nPolygons: Closed shapes defining areas like census tracts or property boundaries.\n\n\ntransit_stops = gpd.read_file(\"data/ttc_stops.geojson\")\ntransit_routes = gpd.read_file(\"data/ttc_routes.geojson\")\nwards = gpd.read_file(\"data/city-wards.geojson\")\n\nIn geopandas, we typically load data with a more agnostic read_file() function. For this workshop, we‚Äôre going to use four sources of data:\n\ntransit_stops: each of the stops for the TTC\ntransit_routes: each of the lines for the TTC\nwards: polygons of City of Toronto wards (i.e.¬†council districts)\n\nLet‚Äôs take a look at the first layer, the transit stops. We have two columns with text, and a third with geometry data.\nHere the data are coded as a MULTILINESTRING, essentially a combination of lines that combine into one object.\n\ntransit_routes\n\n\n\n\n\n\n\n\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nExisting\nSubway\nLine 4: Sheppard Subway\nMULTILINESTRING ((-79.41092 43.76152, -79.4096...\n\n\n1\nExisting\nSubway\nLine 1: Yonge-University Subway\nMULTILINESTRING ((-79.52727 43.79351, -79.5261...\n\n\n2\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nMULTILINESTRING ((-79.26453 43.73227, -79.2669...\n\n\n3\nIn Delivery\nLRT / BRT\nEglinton Crosstown LRT\nMULTILINESTRING ((-79.26453 43.73227, -79.2679...\n\n\n4\nIn Delivery\nLRT / BRT\nEglinton Crosstown West Extension\nMULTILINESTRING ((-79.48726 43.68739, -79.4901...\n\n\n5\nIn Delivery\nSubway\nScarborough Subway Extension\nMULTILINESTRING ((-79.26453 43.73227, -79.2630...\n\n\n6\nIn Delivery\nLRT / BRT\nFinch West LRT\nMULTILINESTRING ((-79.49099 43.76349, -79.4922...\n\n\n7\nIn Delivery\nSubway\nOntario Line\nMULTILINESTRING ((-79.35168 43.69644, -79.3414...\n\n\n8\nIn Delivery\nSubway\nYonge North Subway Extension\nMULTILINESTRING ((-79.41557 43.77977, -79.4157...\n\n\n\n\n\n\n\nThe data can be manipulated like a regular pandas data frame. For example, if we want to filter out routes that currently do not operate, like the incomplete ‚ÄúEglinton Crosstown LRT‚Äù and defunct ‚ÄúScarborough RT‚Äù, we can do so by filtering the STATUS column.\n\ntransit_routes = transit_routes.loc[transit_routes[\"STATUS\"] == \"Existing\"]\ntransit_routes\n\n\n\n\n\n\n\n\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nExisting\nSubway\nLine 4: Sheppard Subway\nMULTILINESTRING ((-79.41092 43.76152, -79.4096...\n\n\n1\nExisting\nSubway\nLine 1: Yonge-University Subway\nMULTILINESTRING ((-79.52727 43.79351, -79.5261...\n\n\n2\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nMULTILINESTRING ((-79.26453 43.73227, -79.2669...\n\n\n\n\n\n\n\nLet‚Äôs do the same for the stops.\nThe geometry for the stops are simpler, just POINT.\n\ntransit_stops = transit_stops.loc[transit_stops[\"STATUS\"] == \"Existing\"]\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nKipling\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.53628 43.63694)\n\n\n1\nIslington\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.5246 43.64533)\n\n\n2\nRoyal York\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.51129 43.64812)\n\n\n3\nOld Mill\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.49509 43.65008)\n\n\n4\nJane\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.48446 43.6498)\n\n\n...\n...\n...\n...\n...\n...\n\n\n69\nVaughan Metropolitan Centre\nExisting\nSubway\nLine 1: Yonge-University Subway\nPOINT (-79.52727 43.79351)\n\n\n70\nSheppard-Yonge\nExisting\nSubway\nLine 4: Sheppard Subway\nPOINT (-79.41092 43.76152)\n\n\n71\nSpadina\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.40397 43.66729)\n\n\n72\nSt. George\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.39931 43.66828)\n\n\n73\nBloor-Yonge\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.38572 43.671)\n\n\n\n\n74 rows √ó 5 columns\n\n\n\nBefore we go on, it‚Äôs important to have an idea of the metadata of geometric files that we work with. There‚Äôs two key parts to this.\n\nCRS (Coordinate Reference System): The crs attribute defines a geodataset‚Äôs spatial ‚Äúcoordinate system‚Äù (e.g., latitude/longitude, meters-based projections). We can use it to ensure layers align‚Äîfor example, combining Toronto census tracts (EPSG:3347) with a Web Mercator basemap (EPSG:3857).\nTotal Bounds: The total_bounds attribute returns the min/max coordinates (xmin, ymin, xmax, ymax) of your data‚Äôs extent. It‚Äôs useful for setting map zoom levels or clipping other datasets to the same area‚Äîlike focusing a transit map on Toronto‚Äôs downtown core.\n\n\ntransit_stops.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ntransit_stops.total_bounds\n\narray([-79.53628,  43.63694, -79.26453,  43.79351])",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#plotting-geographic-data",
    "href": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#plotting-geographic-data",
    "title": "Spatial data in Python",
    "section": "Plotting geographic data",
    "text": "Plotting geographic data\nWe explore geometry simply by plotting using .plot(). We can do this for any row, or the entire GeoDataFrame\n\ntransit_stops.plot()\n\n\n\n\n\n\n\n\nThis is the default plot, but we can tweak the colours, add multiple layers, and change some of the layout options using matplotlib, a commonly used Python plotting library. Here‚Äôs a very simple schematic of rapid transit in Toronto.\n\nfig, ax = plt.subplots(ncols = 1, figsize=(4, 4))\n\nwards.plot(\n    linewidth = 1,\n    color=\"LightGray\",\n    edgecolor=\"White\",\n    ax = ax\n)\n\ntransit_stops.plot(\n    color=\"Black\",\n    markersize = 6,\n    ax = ax\n)\n\ntransit_routes.plot(\n    linewidth = 1,\n    color=\"Black\",\n    ax = ax\n).set_axis_off()",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#interactive-exploration",
    "href": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#interactive-exploration",
    "title": "Spatial data in Python",
    "section": "Interactive Exploration",
    "text": "Interactive Exploration\nGeoPandas‚Äô explore() function generates an interactive Leaflet map (like Google Maps) from your geodata. We can use it to better understand the data we are working with and how it might be viewed from the user side on a web application (e.g., Svelte).\nYou‚Äôll need to install a couple libraries in order for this to work - matplotlib, folium, and mapclassify. This can be done in the environment that you‚Äôre working in with a command like pip install folium matplotlib mapclassify.\n\ntransit_stops.explore(\n    column='NAME',\n    tiles=\"CartoDB Positron\", \n    marker_kwds={\"radius\": 7}\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#geometery-properties",
    "href": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#geometery-properties",
    "title": "Spatial data in Python",
    "section": "Geometery properties",
    "text": "Geometery properties\ngeopandas has a number of functions you can run on geometry columns that can be super useful for analyzing and summarizing data. Here‚Äôs a list of a few examples of using these to create a new column of data (more can be found in the official documentation)\n\n\n\n\n\n\n\n\nProperty\nDescription\nExample\n\n\n\n\n.area\nArea of the geometry (in CRS units)\ngdf[\"area\"] = gdf.geometry.area\n\n\n.length\nLine length (perimeter length for polygons)\ngdf[\"length\"] = gdf.geometry.length\n\n\n.is_valid\nGeometry validity check\ngdf[\"valid\"] = gdf.geometry.is_valid\n\n\n.is_empty\nWhether geometry is empty\ngdf[\"empty\"] = gdf.geometry.is_empty\n\n\n.is_simple\nNo self-intersections\ngdf[\"simple\"] = gdf.geometry.is_simple\n\n\n.type\nType of geometry (e.g., Polygon)\ngdf[\"geom_type\"] = gdf.geometry.type\n\n\n.bounds\nBounding box as (minx, miny, maxx, maxy)\ngdf[\"bounds\"] = gdf.geometry.bounds",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#processing-and-analyzing-spatial-data",
    "href": "urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#processing-and-analyzing-spatial-data",
    "title": "Spatial data in Python",
    "section": "Processing and analyzing spatial data",
    "text": "Processing and analyzing spatial data\nThis tutorial provided a quick overview of how we can work with spatial data in geopandas. However, this just scratched the surface of what is possible\nIf you‚Äôre interested the why and how of spatial data processing, i.e.¬†about converting spatial data from one format to another (e.g.¬†generating centroids of polygons, buffers around points, joining and linking multiple spatial datasets to each other, etc.), check out our Processing spatial data tutorial.\nBeyond that, you can check out the following Python libraries for working with spatial data:\n\nShapely (wide variety of tools for manipulation of 2d geometry data, some of which geopandas uses.)\nPysal (spatial data analysis library, including various descriptive statistics and spatial modelling functions)\nRasterio (library for working with raster data)\nFolium (more interactive maps!)",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html",
    "href": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Aniket Kali, Jeff Allen\nDownload this notebook and data\nThis notebook provides an introduction for analyzing urban data. It will cover ‚Ä¶\nTo do this, we‚Äôll primarily be using pandas, a Python library for analyzing and organizing tabular data. For each section, we‚Äôll also link to relevant documentation for doing similar tasks in Microsoft Excel and Google Sheets.\nIf you haven‚Äôt installed pandas you‚Äôll have to install it by ‚Äúun-commenting‚Äù the line of code below (i.e., removing the # and following space at the beginning of the line).\n# !pip install pandas\nNext, let‚Äôs import the pandas library using the pd alias. An alias in Python is an alternate name for a library that‚Äôs typically shorter and easier to reference in the code later on.\nimport pandas as pd\npandas is the most common library for working with both big and small datasets in Python, and it is also the basis for working with more analytical packages (e.g.¬†numpy, scipy, scikit-learn) and analyzing geographic data (e.g.¬†geopandas). For each section, we‚Äôll also link to relevant documentation for doing similar tasks in Microsoft Excel and Google Sheets.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#tables-dataframes",
    "href": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#tables-dataframes",
    "title": "Processing and analyzing data",
    "section": "Tables & DataFrames",
    "text": "Tables & DataFrames\nA very common way of structuring and analyzing a dataset is in a 2-dimensional table format, where rows are individual records or observations, while columns are attributes (often called variables) that describe those observations. For example, each row could represent a city and the columns could indicate the population (column 1 = population) for different time periods (column 2 = year). Data stored in spreadsheets often take this format.\nIn pandas, a DataFrame is a tabular data structure similar to a spreadsheet, where data is organized in rows and columns. Columns can contain different kinds of data, such as numbers, strings, dates, and so on. When we load data in pandas, we typically load it into the structure of a DataFrame.\nLet‚Äôs first take a look at a small dataset, which includes Census data about Canadian municipalities and their populations in 2021 and 2016. In Statistics Canada lingo, these are called Census Subdivisions. This dataset only includes municipalities with a population greater than 25,000 in 2021.\nThe main method for loading csv data is to use the read_csv function, but pandas can also read and write (export) many other data formats.\n\ndf = pd.read_csv(\"data/cities.csv\")\n\nGreat! Now our data is stored in the variable df and has the structure of a DataFrame.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#viewing-data",
    "href": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#viewing-data",
    "title": "Processing and analyzing data",
    "section": "Viewing data",
    "text": "Viewing data\nIn spreadsheet software, when we open a data file, we will see the top rows of the data right away.\nIn pandas, we can simply type the name of the DataFrame, in this case df, in the cell to view it. By default, it will print the top and bottom rows in the DataFrame\n\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nWinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nWe can specify which rows we want to view.\nLet‚Äôs explore what this data frame looks like. Adding the function .head(N) or .tail(N) prints the top or bottom N rows of the DataFrame. The following prints the first 10 rows.\nTry to edit this to print the bottom 10 rows or a different number of rows.\n\ndf.head(10)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n7\nBlainville\nQue.\n59819.0\nNaN\n\n\n8\nBoisbriand\nQue.\n28308.0\n26884.0\n\n\n9\nBoucherville\nQue.\n41743.0\n41671.0\n\n\n\n\n\n\n\nNotice that each column has a unique name. We can view the data of this column alone by using that name, and see what unique values exist using .unique().\nTry viewing the data of another column. Beware of upper and lower case ‚Äì exact names matter!\n\ndf['Prov/terr'].head(10)  # Top 10 only\n\n0     B.C.\n1    Alta.\n2     Ont.\n3     Que.\n4     Ont.\n5     Ont.\n6     Ont.\n7     Que.\n8     Que.\n9     Que.\nName: Prov/terr, dtype: object\n\n\n\ndf['Prov/terr'].unique() # Unique values for the *full* dataset\n\narray(['B.C.', 'Alta.', 'Ont.', 'Que.', 'Man.', nan, 'N.S.', 'P.E.I.',\n       'N.L.', 'N.B.', 'Sask.', 'Y.T.'], dtype=object)\n\n\nWhat happens if you do df['Prov/terr'].head(10).unique() instead?",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#filtering-data",
    "href": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#filtering-data",
    "title": "Processing and analyzing data",
    "section": "Filtering data",
    "text": "Filtering data\nWe often want to look at only a portion of our data that fit some set of conditions (e.g.¬†all cities in a province that have a population more than 100,000). This is often called filtering, querying, or subsetting a dataset.\nLet‚Äôs try to do some filtering in pandas with our data of Canadian cities.\nCheck out these links for filtering and sorting in spreadsheet software:\n\nFiltering in Excel\nFiltering in Google Sheets\n\nWe can use the columns to identify data that we might want to filter by. The line below shows data only for Ontario, but see if you can filter for another province or territory.\n\ndf.loc[df['Prov/terr'] == 'Ont.']\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n10\nBradford West Gwillimbury\nOnt.\n42880.0\n35325.0\n\n\n...\n...\n...\n...\n...\n\n\n171\nWhitby\nOnt.\n138501.0\n128377.0\n\n\n172\nWhitchurch-Stouffville\nOnt.\n49864.0\n45837.0\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n67 rows √ó 4 columns\n\n\n\nPandas allows us to use other similar mathematical concepts to filter for data. Previously, we asked for all data in Ontario.\nNow, filter for all cities which had a population of at least 100,000 in 2021.\nHINT: in Python, ‚Äúgreater than or equals to‚Äù (i.e., ‚Äúat least‚Äù) is represented using the syntax &gt;=.\nPandas also allows us to combine filtering conditions.\nUse the template below to select for all cities in Ontario with a population of over 100,000 in 2021.\n\ndf.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n\nNow let‚Äôs count how many cities actually meet these conditions. Run the line below to see how many cities there are in this data set in Ontario.\n\ndf.loc[df['Prov/terr'] == 'Ont.'].count()\n\nName                66\nProv/terr           67\nPopulation, 2021    66\nPopulation, 2016    65\ndtype: int64\n\n\nThe function .count() tells us how much data there is for each column - but if we wanted to just see one column, we could also filter for that individual column using df[COL_NAME].\nTry a different condition and count the amount of data for it.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#sorting-data",
    "href": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#sorting-data",
    "title": "Processing and analyzing data",
    "section": "Sorting data",
    "text": "Sorting data\nYou might have noticed that these cities are in alphabetical order - what if we wanted to see them in the order of population? In pandas, we do this using the sort_values function. The default is to sort in ascending order, so we set this to be False (i.e.¬†descending) so the most populous cities are at the top.\nCheck out these links for sorting data in Excel and Google Sheets - Sorting in Excel - Sorting in Google Sheets\n\ndf.sort_values(by='Population, 2021', ascending=False)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n\n\n89\nMontr√©al\nQue.\n1762949.0\n1704694.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n\n\n...\n...\n...\n...\n...\n\n\n115\nPrince Edward County\nOnt.\n25704.0\n24735.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n40\nDrummondville\nQue.\nNaN\n75423.0\n\n\n109\nPeterborough\nOnt.\nNaN\nNaN\n\n\n169\nWest Kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nLet‚Äôs put some of this together now.\nFilter the data to show all cities which are in the province of Quebec with at least a population of 50,000 in 2016, and then try to sort the cities by their 2016 population.\nHINT: You can do this in two steps (which is more readable) by storing the data that you filter into a variable called df_filtered, then running the command to sort the values on df_filtered.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#exporting-data",
    "href": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#exporting-data",
    "title": "Processing and analyzing data",
    "section": "Exporting data",
    "text": "Exporting data\nOnce we have processed and analyzed our data, we may want to save it for future use or share it with others. pandas makes it easy to export data to various formats, such as CSV or Excel files.\nBelow, we demonstrate how to export a DataFrame to both CSV and Excel formats. This is particularly useful for sharing results or viewing the data in other tools like spreadsheet software.\n\n# Save to CSV\ndf_filtered.to_csv('df_filtered.csv', index=False)\n\n# Save to Excel\ndf_filtered.to_excel('df_filtered.xlsx', index=False)",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#updating-and-renaming-columns",
    "href": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#updating-and-renaming-columns",
    "title": "Processing and analyzing data",
    "section": "Updating and renaming columns",
    "text": "Updating and renaming columns\nOften, the data we have might not be in the condition we want it to be in. Some data might be missing, and other data might have odd naming conventions. In Excel or Google Sheets, you can simply change a column name by typing the edits you want in the appropriate cell. However, Python requires different steps.\nHere‚Äôs a simple example. We might want all city names to be in lowercase - the code below does this for us.\n\ndf['Name'] = df['Name'].str.lower()\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nabbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nairdrie\nAlta.\n74100.0\n61581.0\n\n\n2\najax\nOnt.\n126666.0\n119677.0\n\n\n3\nalma\nQue.\n30331.0\n30771.0\n\n\n4\naurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nwindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nwinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nwood buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nwoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nwoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nDataFrames are easily modifiable. pandas has a number of methods like str.lower() to do so (see the full documentation). The important thing to note here is that we directly modified the existing values of a column. We might not always want to do this, but it is often a good way of saving computer memory by not duplicating data.\nLikewise, we might want better names for the columns we have. Take a look at the documentation for .rename() and modify the data frame df such that we rename the column Name to City. Pandas has more methods than we could ever remember, so learning to navigate the documentation is a crucial part of using the library.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#handling-missing-data",
    "href": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#handling-missing-data",
    "title": "Processing and analyzing data",
    "section": "Handling missing data",
    "text": "Handling missing data\nUnfortunately, it is pretty common that the datasets we work with will be missing data values for some rows and columns. This can create complications when we want to produce summary statistics or visualizations. There are different strategies for dealing with this (i.e., imputing data), but the easiest is simply to remove missing values. But first let‚Äôs check how much data is missing.\n\ndf.isnull().sum()\n\nName                3\nProv/terr           4\nPopulation, 2021    3\nPopulation, 2016    4\ndtype: int64\n\n\nIt seems that each column has a couple of data points missing. Let‚Äôs take a look at which rows these occur in. Similar to how we created a condition to filter for certain data, the code below creates a condition to filter for rows with missing data.\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n7\nblainville\nQue.\n59819.0\nNaN\n\n\n18\ncaledon\nNaN\n76581.0\n66502.0\n\n\n30\nNaN\nOnt.\n101427.0\n92013.0\n\n\n40\ndrummondville\nQue.\nNaN\n75423.0\n\n\n51\ngrimsby\nOnt.\n28883.0\nNaN\n\n\n64\nla prairie\nNaN\n26406.0\n24110.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n86\nmission\nNaN\n41519.0\n38554.0\n\n\n109\npeterborough\nOnt.\nNaN\nNaN\n\n\n131\nNaN\nQue.\n29954.0\n27359.0\n\n\n157\ntimmins\nNaN\n41145.0\n41788.0\n\n\n169\nwest kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n\n\n\nYou can see that some rows are missing multiple values, while others are just missing one. We can remove rows which have missing data using the function dropna and assign it to df so that we will be working only with complete data going forward. Try to modify the code below to drop rows that contain empty values in one of the two population columns. Put another way, we want to keep rows with population values, even if the name or province values are missing. Look at the documentation to figure this out, specifically the argument subset for .dropna()\n\ndf = df.dropna()\n\nGreat. Now let‚Äôs reset to our original data frame and exclude any rows with missing values.\n\ndf = pd.read_csv(\"data/cities.csv\")\ndf = df.dropna()\n\nInstead of dropping (i.e.¬†removing) rows with missing values, we can instead replace them with specific values with fillna. For example, df.fillna(0) would replace all missing data values with a 0. This wouldn‚Äôt make sense in our data of Canadian cities, but can be useful in other contexts.\nSimilarly we can programatically find and replace data in any other column or across our dataset. For example, if we wanted to rename 'Y.T.' to 'Yukon' we would run the replace function as so df = df.replace('Y.T.', 'Yukon')\nIf you are using spreadsheet software such as Excel or Google Sheets, there are different ways of approaching this problem. Explore some of the common ways to clean data and handle missing values in the following resources:\n\nTop ten ways of cleaning your data in Excel\nFind missing values in Excel\n\n\nCreating new columns\nWe can add or delete columns as needed. Let‚Äôs first add a column which shows the change in population between 2021 and 2016 and then sort by the cities that experienced the greatest population loss. We can calculate this via a simple subtraction as follows.\n\ndf[\"pop_change\"] = df[\"Population, 2021\"] - df[\"Population, 2016\"]\ndf.sort_values(\"pop_change\", ascending = False).head(5)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\npop_change\n\n\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n83206.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n77811.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n67564.0\n\n\n11\nBrampton\nOnt.\n656480.0\n593638.0\n62842.0\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n62785.0\n\n\n\n\n\n\n\nPandas supports mathematical equations between columns, just like the subtraction we did above. Create a new column called pct_pop_change that computes the percentage change in population between 2016 and 2021, and sort by the cities with the greatest increase.\nHINT: the way to compute percent change is 100 * (Y - X) / X.\nNow let‚Äôs clear these new columns out using drop to return to what we had originally.\n\ndf = df.drop(columns=['pop_change', 'pct_pop_change'])\n\nIn Excel or Google Sheets, you can easily create a new column by following these steps, then typing out the mathematical formula you wish to use to define the values in your new column in the top-most cell of that column, and then dragging that cell all the way down the length of your new column.\n\n\nConcatenating and joining tables\nMuch of the time, we are working with multiple sets of data which may overlap in key ways. Perhaps we want to include measures of income in cities, or look at voting patterns - this may require us to combine multiple data frames so we can analyze them.\nPandas methods to combine data frames are quite similar to that of database operations - if you‚Äôve worked with SQL before, this will come very easily to you. There‚Äôs an extensive tutorial on this topic, but we‚Äôll focus on simple cases of pd.concat() and pd.merge().\nFirst, we can use pd.concat() to stack DataFrames vertically when new data has the same columns but additional rows (e.g., adding cities from another region). This is like adding new entries to a database table.\n\ndf_ny_cities = pd.read_csv(\"./data/new_york_cities.csv\")\ncombined = pd.concat([df, df_ny_cities], ignore_index=True)\nprint(\"Original rows:\", len(df), \"| After append:\", len(combined))\ndisplay(combined.tail(5))  # Show new rows\n\nOriginal rows: 167 | After append: 169\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n164\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n165\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n166\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n167\nNew York City\nN.Y.\n8804190.0\n8537673.0\n\n\n168\nBuffalo\nN.Y.\n278349.0\n258071.0\n\n\n\n\n\n\n\nNext, we can use pd_merge() (or pd.concat(axis=1)) to combine DataFrames side-by-side when they share a key column (e.g., city names). Here, we‚Äôll add a column denoting whether the city is a provincial capital by matching city names.\nLet‚Äôs first load this data:\n\ndf_capitals = pd.read_csv('./data/capitals.csv')\ndf_capitals\n\n\n\n\n\n\n\n\nName\nIs_Capital\n\n\n\n\n0\nToronto\nTrue\n\n\n1\nQu√©bec\nTrue\n\n\n2\nVictoria\nTrue\n\n\n3\nEdmonton\nTrue\n\n\n4\nWinnipeg\nTrue\n\n\n5\nFredericton\nTrue\n\n\n6\nHalifax\nTrue\n\n\n7\nCharlottetown\nTrue\n\n\n8\nSt. John's\nTrue\n\n\n9\nRegina\nTrue\n\n\n10\nWhitehorse\nTrue\n\n\n\n\n\n\n\n\ndf_with_capitals = pd.merge(\n    df,  # Original data\n    df_capitals,  # New data\n    on=\"Name\",  # The same column \n    how=\"left\"  # Keep all data from the \"left\", ie. original\n)\n\n# Set non-capitals to False\ndf_with_capitals[\"Is_Capital\"] = df_with_capitals[\"Is_Capital\"].astype('boolean').fillna(False)\n\n# Verify: Show capitals and counts\nprint(f\"Found {df_with_capitals['Is_Capital'].sum()} capitals:\")\ndf_with_capitals[df_with_capitals[\"Is_Capital\"]]\n\nFound 11 capitals:\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\nIs_Capital\n\n\n\n\n23\nCharlottetown\nP.E.I.\n38809.0\n36094.0\nTrue\n\n\n38\nEdmonton\nAlta.\n1010899.0\n933088.0\nTrue\n\n\n41\nFredericton\nN.B.\n63116.0\n58721.0\nTrue\n\n\n49\nHalifax\nN.S.\n439819.0\n403131.0\nTrue\n\n\n108\nQu√©bec\nQue.\n549459.0\n531902.0\nTrue\n\n\n111\nRegina\nSask.\n226404.0\n215106.0\nTrue\n\n\n139\nSt. John's\nN.L.\n110525.0\n108860.0\nTrue\n\n\n147\nToronto\nOnt.\n2794356.0\n2731571.0\nTrue\n\n\n154\nVictoria\nB.C.\n91867.0\n85792.0\nTrue\n\n\n161\nWhitehorse\nY.T.\n28201.0\n25085.0\nTrue\n\n\n163\nWinnipeg\nMan.\n749607.0\n705244.0\nTrue\n\n\n\n\n\n\n\nIf you are curious about how to do this in spreadsheet software like Excel or Google Sheets, check out this tutorial, which lays out a few different options. One of the most common approaches for joining two separate datasets is the =VLOOKUP formula.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#summary-statistics",
    "href": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#summary-statistics",
    "title": "Processing and analyzing data",
    "section": "Summary statistics",
    "text": "Summary statistics\nA dataset is only as good as its ability to help us understand/describe the world around us. There are some basic methods in pandas we can use to get an idea of what the data says.\nThe most basic function you can use to get an idea of what‚Äôs going on is .describe(). It shows you how much data there is, and a number of summary statistics.\nModify the code below to report summary statistics for cities in Quebec only.\n\ndf.describe()\n\n\n\n\n\n\n\n\nPopulation, 2021\nPopulation, 2016\n\n\n\n\ncount\n1.670000e+02\n1.670000e+02\n\n\nmean\n1.573169e+05\n1.487358e+05\n\n\nstd\n3.074452e+05\n2.959960e+05\n\n\nmin\n2.570400e+04\n2.378700e+04\n\n\n25%\n3.770050e+04\n3.449850e+04\n\n\n50%\n6.414100e+04\n6.316600e+04\n\n\n75%\n1.348910e+05\n1.255940e+05\n\n\nmax\n2.794356e+06\n2.731571e+06\n\n\n\n\n\n\n\nInstead of picking out and examining a subset of the data one by one, we can use the function .groupby() to examine several subsets concurrently. Given a column name, it will group rows which have the same value. In the example below, that means grouping every row which has the same province name. We can then apply a function to this (or multiple functions using .agg()) to examine different aspects of the data.\n\n# Group by province and calculate total population\nprovince_pop = df.groupby('Prov/terr')['Population, 2021'].sum()\nprint(\"Total population by province:\")\nprovince_pop.sort_values(ascending=False)\n\nTotal population by province:\n\n\nProv/terr\nOnt.      11598308.0\nQue.       5474109.0\nB.C.       3662922.0\nAlta.      3192892.0\nMan.        800920.0\nSask.       563966.0\nN.S.        533513.0\nN.B.        240595.0\nN.L.        137693.0\nP.E.I.       38809.0\nY.T.         28201.0\nName: Population, 2021, dtype: float64\n\n\n\n# Multiple aggregation statistics\nstats = df.groupby('Prov/terr')['Population, 2021'].agg(['count', 'mean', 'max', 'sum'])\nstats\n\n\n\n\n\n\n\n\ncount\nmean\nmax\nsum\n\n\nProv/terr\n\n\n\n\n\n\n\n\nAlta.\n17\n187817.176471\n1306784.0\n3192892.0\n\n\nB.C.\n29\n126307.655172\n662248.0\n3662922.0\n\n\nMan.\n2\n400460.000000\n749607.0\n800920.0\n\n\nN.B.\n4\n60148.750000\n79470.0\n240595.0\n\n\nN.L.\n2\n68846.500000\n110525.0\n137693.0\n\n\nN.S.\n2\n266756.500000\n439819.0\n533513.0\n\n\nOnt.\n64\n181223.562500\n2794356.0\n11598308.0\n\n\nP.E.I.\n1\n38809.000000\n38809.0\n38809.0\n\n\nQue.\n41\n133514.853659\n1762949.0\n5474109.0\n\n\nSask.\n4\n140991.500000\n266141.0\n563966.0\n\n\nY.T.\n1\n28201.000000\n28201.0\n28201.0\n\n\n\n\n\n\n\nBelow, we‚Äôve added a column which shows the percent growth for each city. Use .groupby('Prov/terr') and use the function .median() (just as we used .sum() above) to observe the median growth rate per province or territory. Make sure to use sort_values() and set ascending to False.\n\ndf['Percent_Growth'] = 100 * (df['Population, 2021'] - df['Population, 2016']) / df['Population, 2016'] \n\nIn spreadsheet software like Excel or Google Sheets, you can calculate the descriptive statistics for your dataset by using individual formulas that you apply to the specific columns you wish to analyze, such as =COUNT (count), =MIN (minimum), =MAX (maximum), =AVERAGE (mean), =MEDIAN (median), =PERCENTILE (percentiles, which can be used for quartiles, quantiles, or any percentile you wish), =STDEVA (standard deviation) functions. If it‚Äôs available to you in your Excel subscription service, you can also explore Excel‚Äôs Analysis Toolpak add-in, which allows for more efficient and advanced statistical analysis.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#cross-tabulations-and-pivot-tables",
    "href": "urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#cross-tabulations-and-pivot-tables",
    "title": "Processing and analyzing data",
    "section": "Cross tabulations and pivot tables",
    "text": "Cross tabulations and pivot tables\nA cross tabulation, also called a frequency table or a contingency table, is a table that summarizes two categorical variables by displaying the number of occurrences in each pair of categories.\nHere‚Äôs an example. Let‚Äôs count the number of cities in each province by city size.\nWe will need two tools to do this: - cut, which bins continuous numbers (like population) into categories (e.g., ‚ÄúSmall‚Äù/‚ÄúMedium‚Äù/‚ÄúLarge‚Äù), turning numbers into meaningful groups. - crosstab, which counts how often these groups appear together‚Äîlike how many ‚ÄúMedium‚Äù cities exist per province‚Äîrevealing patterns that raw numbers hide.\n\n# Create a size category column\ndf['Size'] = pd.cut(df['Population, 2021'],\n                    bins=[0, 50000, 200000, float('inf')],\n                    labels=['Small', 'Medium', 'Large'])\n\n# Cross-tab: Province vs. Size\nsize_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\nsize_table\n\nRecall that we just created the column 'Percent_Growth' as well. Use these two functions to create different bins for different levels of growth and cross tabulate them.\nIf you‚Äôve worked with Excel or Google Sheets, this is very similar to doing a Pivot Table. Here are a couple tutorials for spreadhseet software:\n\nPivot tables in Excel\nPivot tables in Google Sheets\n\nPivot tables are able to summarize data across several categories, and for different types of summaries (e.g.¬†sum, mean, median, etc.)\nThe pivot_table function in pandas to aggregate data, and has more options than the crosstab function. Both are quite similar though. Here‚Äôs an example where we are using pivot_table to sum the population in each province by the 3 size groups.\nTry to edit this to compute the mean or median city Percent_Growth\n\npd.pivot_table(data = df, values = ['Population, 2021'], index = ['Prov/terr'], columns = ['Size'], aggfunc = 'sum', observed=True)\n\n\n\n\n\n\n\n\nPopulation, 2021\n\n\nSize\nSmall\nMedium\nLarge\n\n\nProv/terr\n\n\n\n\n\n\n\nAlta.\n234664.0\n640545.0\n2317683.0\n\n\nB.C.\n330537.0\n1642753.0\n1689632.0\n\n\nMan.\nNaN\n51313.0\n749607.0\n\n\nN.B.\n28114.0\n212481.0\nNaN\n\n\nN.L.\n27168.0\n110525.0\nNaN\n\n\nN.S.\nNaN\n93694.0\n439819.0\n\n\nOnt.\n930812.0\n2925641.0\n7741855.0\n\n\nP.E.I.\n38809.0\nNaN\nNaN\n\n\nQue.\n806267.0\n1371544.0\n3296298.0\n\n\nSask.\n71421.0\nNaN\n492545.0\n\n\nY.T.\n28201.0\nNaN\nNaN\n\n\n\n\n\n\n\nThere are often multiple ways to achieve similar results. In the previous section we looked at the groupby function to summarize data by one column, while above we used crosstab or pivot_table. However, we could also use the groupby function for this purpose. Check out the example below.\nYou should notice that it has created a long-table format rather than a wide-table format. Both formats can be useful. Wide-table formats are better for viewing data with only 2 categories, while long-table formats are often used for inputting data into modelling or visualization libraries.\n\ndf.groupby(['Prov/terr', 'Size'], observed=False)['Population, 2021'].agg(['sum'])\n\n\n\n\n\n\n\n\n\nsum\n\n\nProv/terr\nSize\n\n\n\n\n\nAlta.\nSmall\n234664.0\n\n\nMedium\n640545.0\n\n\nLarge\n2317683.0\n\n\nB.C.\nSmall\n330537.0\n\n\nMedium\n1642753.0\n\n\nLarge\n1689632.0\n\n\nMan.\nSmall\n0.0\n\n\nMedium\n51313.0\n\n\nLarge\n749607.0\n\n\nN.B.\nSmall\n28114.0\n\n\nMedium\n212481.0\n\n\nLarge\n0.0\n\n\nN.L.\nSmall\n27168.0\n\n\nMedium\n110525.0\n\n\nLarge\n0.0\n\n\nN.S.\nSmall\n0.0\n\n\nMedium\n93694.0\n\n\nLarge\n439819.0\n\n\nOnt.\nSmall\n930812.0\n\n\nMedium\n2925641.0\n\n\nLarge\n7741855.0\n\n\nP.E.I.\nSmall\n38809.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0\n\n\nQue.\nSmall\n806267.0\n\n\nMedium\n1371544.0\n\n\nLarge\n3296298.0\n\n\nSask.\nSmall\n71421.0\n\n\nMedium\n0.0\n\n\nLarge\n492545.0\n\n\nY.T.\nSmall\n28201.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html",
    "href": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html",
    "title": "Spatial data processing",
    "section": "",
    "text": "Jeff Allen, Aniket Kali\nDownload this notebook and data\nSpatial data processing (or geoprocessing) is the use of tools and functions to analyze and manipulate geographic data, often to create new data or information.\nIt‚Äôs like doing applied math on map data ‚Äî combining, measuring, or filtering spatial data to get new information. Here are a couple of examples:\nThese help answer spatial questions like ‚ÄúWhat‚Äôs within walking distance?‚Äù or ‚ÄúHow much green-space is inside this area?‚Äù or ‚ÄúHow many libraries are in each neighbourhood?‚Äù\nThere are many ways to interact with spatial data - ranging from getting geometric points from addresses, to intersecting different spatial datasets. In this notebook, we‚Äôll cover a number of commonly used spatial data processing functions.\nWe‚Äôll show the examples using geopandas, however every one of these examples can easily be done in GIS software and many other libraries for working with spatial data.\nIn QGIS, many of the functions that we show below (e.g.¬†Buffer, Spatial Joins, etc.) can be found under the Vector or Raster dropdowns at the top QGIS panel. These dropdowns include some of the most common functions. For a full, searchable, list of all the spatial data processing functions in QGIS, go to the Processing dropdown and then click Toolbox. This should display a panel with all the functions available in QGIS by category and with a search bar.\nGenerally, when processing spatial data, using QGIS is great for smaller tasks that we only want to compute something once, while using Python is better suited for larger datasets, automating repetitive tasks, chaining processes together, and allows for greater reproducibility.\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nLet‚Äôs remind ourselves of some key concepts to start with. For a broader overview, check out our notebook on Spatial data and GIS.\nFirst, there are two major types of spatial data.\nIn this notebook, we‚Äôll mostly be working with vector data, which consists of three common types of geometry.\nThese can be projected into different Coordinate Reference Systems (CRS), which define how spatial data maps to the Earth‚Äôs surface. For example, WGS84 (EPSG:4326) is common for global latitude/longitude coordinates, while UTM zones (e.g., EPSG:32617) minimize distortion for local measurements.\nChoosing the right CRS ensures accurate distances, areas, and spatial relationships.\nWhen we are comparing and relating multiple spatial datasets to each other, we have to ensure that they are in the same CRS.\nIn order to process spatial data we will be working with three datasets today (you can also download them by clicking on the link):\ndf_tpl = pd.read_csv('./data/tpl-branch-general-information-2023.csv')\ngdf_wards = gpd.read_file('./data/city-wards.gpkg')\ngdf_regions = gpd.read_file('./data/toronto-regions.gpkg')",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#geocoding",
    "href": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#geocoding",
    "title": "Spatial data processing",
    "section": "Geocoding",
    "text": "Geocoding\nGeocoding converts addresses (e.g.¬†\"100 Queen St, Toronto, ON M5H 2N2\") into geographic coordinates (e.g.¬†[-79.3840, 43.6536]) that can be plotted on a map and analyzed in GIS or Python.\nOf course we could simply input the string for an address into Google Maps, search for it, and manually record the coordinate location. However, this wouldn‚Äôt be practical if we had a big list of addresses. Let‚Äôs take the .csv data that we loaded for library locations. There are 112 in Toronto!\n\ndf_tpl[[\"BranchName\",\"Address\"]]\n\n\n\n\n\n\n\n\nBranchName\nAddress\n\n\n\n\n0\nAlbion\n1515 Albion Road, Toronto, ON, M9V 1B2\n\n\n1\nAlbert Campbell\n496 Birchmount Road, Toronto, ON, M1K 1N8\n\n\n2\nAlderwood\n2 Orianna Drive, Toronto, ON, M8W 4Y1\n\n\n3\nAgincourt\n155 Bonis Avenue, Toronto, ON, M1T 3W6\n\n\n4\nArmour Heights\n2140 Avenue Road, Toronto, ON, M5M 4M7\n\n\n...\n...\n...\n\n\n107\nWoodview Park\n16 Bradstock Road, Toronto, ON, M9M 1M8\n\n\n108\nWoodside Square\nWoodside Square Mall, 1571 Sandhurst Circle, T...\n\n\n109\nWychwood\n1431 Bathurst Street, Toronto, ON, M5R 3J2\n\n\n110\nYorkville\n22 Yorkville Avenue, Toronto, ON, M4W 1L4\n\n\n111\nYork Woods\n1785 Finch Avenue West, Toronto, ON, M3N 1M6\n\n\n\n\n112 rows √ó 2 columns\n\n\n\nA geocoding function, either in Python or GIS, typically relies on an external geocoder, a service that turns addresses or place names into map coordinates. Large companies like Google, Apple, and Mapbox provide geocoding services. However, these require an API key and often have a tiered pricing model. There are free tools as well, for example Nominatim which queries OpenStreetMap‚Äôs database. This is what we will use in this notebook.\nIn geopandas, the function for geocoding is geopandas.tools.geocode(). This transforms a DataFrame of addresses into a GeoDataFrame with Point geometries, enabling spatial analysis.\nThere are similar tools in QGIS which make use of geocoders like Nominatim and others.\nFree geocoding services usually allow about one query per second, regardless of whether you are accessing them in QGIS or Python. Try running the code below. Since we have over 100 library branches, you‚Äôll have to wait a little bit! :)\n\ngdf_tpl = gpd.tools.geocode(\n    df_tpl[\"Address\"],  \n    provider=\"nominatim\",      \n    user_agent=\"tpl-workshop\",\n    timeout=10\n)\n\n# Add in and update columns\ngdf_tpl['Address'] = df_tpl['Address']\ngdf_tpl['BranchName'] = df_tpl['BranchName']\ngdf_tpl = gdf_tpl.drop(columns=['address'])\ngdf_tpl = gdf_tpl.to_crs(4326)\n\n\n## In case you don't want to run and wait for the geocoding to work, we've precomputed it and you can load\ngdf_tpl = gpd.read_file('./data/tpl.gpkg')\n\n\ngdf_tpl.head()\n\n\n\n\n\n\n\n\nAddress\nBranchName\ngeometry\n\n\n\n\n0\n1515 Albion Road, Toronto, ON, M9V 1B2\nAlbion\nPOINT (-79.58481 43.73987)\n\n\n1\n496 Birchmount Road, Toronto, ON, M1K 1N8\nAlbert Campbell\nPOINT (-79.2693 43.70806)\n\n\n2\n2 Orianna Drive, Toronto, ON, M8W 4Y1\nAlderwood\nPOINT (-79.54724 43.60191)\n\n\n3\n155 Bonis Avenue, Toronto, ON, M1T 3W6\nAgincourt\nPOINT (-79.29345 43.78527)\n\n\n4\n2140 Avenue Road, Toronto, ON, M5M 4M7\nArmour Heights\nPOINT (-79.42171 43.73927)\n\n\n\n\n\n\n\nOnce we‚Äôve geocoded the data, we can then map and analyze this data relative to other spatial data.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#buffer",
    "href": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#buffer",
    "title": "Spatial data processing",
    "section": "Buffer",
    "text": "Buffer\nBuffers create zones around geometries at a specified distance.\nFor example, if we wanted to map the area that is within 1km of the nearest library, we would compute a buffer with the point locations of libraries as the input using the function .buffer(N) (or accompanying methods in QGIS).\nBuffers can be super useful on their own, but they can also be important steps in analyses. For example, you could take buffers of library locations and then count the number of households within each buffer.\nTry running the code below to compute buffers for library locations. You can also test different buffer distances! :)\n\n# converts to UTM Zone 17, which is in metres\ngdf_tpl = gdf_tpl.to_crs(\"EPSG:32617\") \n\n# generate buffer at 1km (1000m)\ngdf_tpl[\"buffer_1km\"] = gdf_tpl.buffer(1000)\n\nGreat! let‚Äôs see what this looks like\n\n# Compare the wards and their center points\nax = gdf_wards.plot(alpha=0.25, edgecolor='black', color=\"pink\", linewidth=1)\ngdf_tpl.set_geometry(\"buffer_1km\").plot(ax=ax, color=\"lightblue\", edgecolor=\"blue\")",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#dissolve",
    "href": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#dissolve",
    "title": "Spatial data processing",
    "section": "Dissolve",
    "text": "Dissolve\nDissolve merges multiple features into one, either based on a shared attribute or as a single unified shape.\nYou might have noticed that many of the buffered library polygons overlapped each other. We can use the method .dissolve() from geopandas, to combine all of these polygons into a single polygon. Check out this tutorial for a similar function in QGIS.\n\ngdf_tpl_dissolved = gdf_tpl.set_geometry(\"buffer_1km\").dissolve()\n\nLet‚Äôs plot it! Notice how all buffers that were overlapping are now merged into one much larger geometry.\n\nax = gdf_wards.plot(alpha=0.25, edgecolor='black', color=\"pink\", linewidth=1)\ngdf_tpl_dissolved.plot(ax=ax, color=\"lightblue\", edgecolor=\"blue\")\n\nHaving this as one geometry can be useful for quick questions like asking how much of the city (in terms of area) is within 1km of a public library. If we did this calculation on just the buffered polygon, we would be double counting areas of overlap.\n\n# computing area of the dissolved polygon, dividing to covert to km2\ngdf_tpl_dissolved.buffer_1km.area / (1000 * 1000)",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#centroids",
    "href": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#centroids",
    "title": "Spatial data processing",
    "section": "Centroids",
    "text": "Centroids\nCentroids are the geometric center point of polygons.\nComputing centroids is useful for urban analyses because it gives a single representative point for each area, which is helpful for labeling, spatial joins, spatial selections, or other further processing of spatial data. For example, we could compute the centroid of each ward in Toronto, and then try to find the closest hospital to each.\nIn geopandas we can use the method .centroid to return a Point geometry for each feature. Here‚Äôs a quick example of finding the centroids of Toronto wards. QGIS also has a centroids function.\n\n# Convert to UTM Zone 17N (meters)  \ngdf_wards = gdf_wards.to_crs(\"EPSG:32617\")  \n\n# Compute centroids (as Point geometries)  \ngdf_wards[\"centroid\"] = gdf_wards.centroid  \n\n# Compare the wards and their center points\nax = gdf_wards.plot(alpha=0.25, edgecolor='black', color=\"pink\", linewidth=1)\ngdf_wards.set_geometry(\"centroid\").plot(ax=ax, color=\"purple\", markersize=10)",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#spatial-selections",
    "href": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#spatial-selections",
    "title": "Spatial data processing",
    "section": "Spatial Selections",
    "text": "Spatial Selections\nSpatial selections, or spatial queries, are ways we can filter datasets based on their locations relative to other spatial features and datasets.\nThis can have a wide variety of applications. For example, you can filter ‚Ä¶\n\nLibrary locations that are within a given neighbourhood\nSchools that are within a set distance of an expressway\nStreets that cross railroad tracks\nParks that contain baseball fields\nAnd more!\n\nBelow, we‚Äôll show an example with geopandas, selecting libraries that are within a neighbourhood.\n(Check out this video tutorial for doing similar selections in QGIS)\nLet‚Äôs begin by loading in the data, and convert to a common CRS so the data can be related to each other accurately.\n\ngdf_tpl = gpd.read_file('./data/tpl.gpkg').to_crs(\"EPSG:32617\") \ngdf_regions = gpd.read_file('./data/toronto-regions.gpkg').to_crs(\"EPSG:32617\") \n\nThe gdf_regions are the 6 former municipalities of Toronto prior to amalgamation.\n\ngdf_regions\n\n\n\n\n\n\n\n\nREGION_NAME\ngeometry\n\n\n\n\n0\nYORK\nMULTIPOLYGON (((621566.778 4833819.469, 621576...\n\n\n1\nNORTH YORK\nMULTIPOLYGON (((617962.54 4840496.385, 617956....\n\n\n2\nEAST YORK\nMULTIPOLYGON (((637991.822 4838842.675, 637885...\n\n\n3\nSCARBOROUGH\nMULTIPOLYGON (((637991.822 4838842.675, 637977...\n\n\n4\nETOBICOKE\nMULTIPOLYGON (((622510.211 4830645.811, 622497...\n\n\n5\nTORONTO\nMULTIPOLYGON (((629370.742 4831174.941, 629343...\n\n\n\n\n\n\n\nLets try to query our dataset of public library locations to find only those within SCARBOROUGH, via .intersects\n\n# Get Scarborough's polygon from regions  \nscarborough = gdf_regions[gdf_regions[\"REGION_NAME\"] == \"SCARBOROUGH\"].geometry.iloc[0]  \n\n# filter the library dataset via a spatial selection, specifically .intersects\ngdf_tpl[gdf_tpl.intersects(scarborough)]\n\n\n\n\n\n\n\n\nAddress\nBranchName\ngeometry\n\n\n\n\n1\n496 Birchmount Road, Toronto, ON, M1K 1N8\nAlbert Campbell\nPOINT (639436.969 4840903.999)\n\n\n3\n155 Bonis Avenue, Toronto, ON, M1T 3W6\nAgincourt\nPOINT (637314.699 4849438.607)\n\n\n10\n1515 Danforth Road, Toronto, ON, M1J 1H5\nBendale\nPOINT (641371.584 4845728.55)\n\n\n18\n1081 Progress Avenue, Toronto, ON, M1B 5Z6\nBurrows Hall\nPOINT (642055.158 4850504.516)\n\n\n19\n3017 Kingston Road, Toronto, ON, M1M 1P1\nCliffcrest\nPOINT (642523.048 4842918.563)\n\n\n21\n545 Markham Road, Toronto, ON, M1H 2A1\nCedarbrae\nPOINT (642971.873 4846476.537)\n\n\n43\n295 Alton Towers Circle, Toronto, ON, M1V 4P1\nGoldhawk Park\nPOINT (638458.503 4853772.851)\n\n\n44\nGuildwood Plaza, 123 Guildwood Parkway, Toront...\nGuildwood\nPOINT (644952.92 4845333.931)\n\n\n46\n3550 Ellesmere Road, Toronto, ON, M1C 3Z2\nHighland Creek\nPOINT (646818.591 4850167.812)\n\n\n63\n30 Sewells Road, Toronto, ON, M1B 3G5\nMalvern\nPOINT (643480.9 4852157.674)\n\n\n65\n2219 Lawrence Avenue East, Toronto, ON, M1P 2P5\nMcGregor Park\nPOINT (638759.861 4845443.433)\n\n\n71\n4279 Lawrence Avenue East, Toronto, ON, M1E 2S8\nMorningside\nPOINT (646611.487 4847925.024)\n\n\n83\n5450 Lawrence Avenue East, Toronto, ON, M1C 3B2\nPort Union\nPOINT (649672.109 4849078.088)\n\n\n92\n156 Borough Drive, Toronto, ON, M1P 4N7\nScarborough Civic Centre\nPOINT (640270.251 4848071.684)\n\n\n100\n1440 Kingston Road, Toronto, ON, M1N 1R3\nTaylor Memorial\nPOINT (639322.581 4838651.581)\n\n\n108\nWoodside Square Mall, 1571 Sandhurst Circle, T...\nWoodside Square\nPOINT (639171.567 4852212.822)\n\n\n\n\n\n\n\nIf we wanted to simply count the number of libraries in Scarborough, we can do that too!\n\nlen(gdf_tpl[gdf_tpl.intersects(scarborough)])\n\n16\n\n\n.intersects is just one of several spatial selection methods. Here‚Äôs a list of several options in .geopandas. QGIS and other GIS software all have the same options, but they might just have different names.\n\n\n\nMethod\nDescription\nDocs\n\n\n\n\nintersects()\nReturns True if geometries intersect\nintersects\n\n\ncontains()\nTrue if geometry completely contains the other\ncontains\n\n\nwithin()\nTrue if geometry is within another\nwithin\n\n\ncrosses()\nTrue if geometry crosses the other\ncrosses\n\n\noverlaps()\nTrue if geometries partially overlap\noverlaps\n\n\ntouches()\nTrue if geometries touch at boundary only\ntouches\n\n\nequals()\nTrue if geometries are exactly equal\nequals\n\n\ndisjoint()\nTrue if geometries have no overlap at all\ndisjoint\n\n\ncovers()\nTrue if geometry covers the other\ncovers\n\n\ncovered_by()\nTrue if geometry is covered by the other\ncovered_by\n\n\ndistance()\nReturns distance to the other geometry\ndistance",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#spatial-joins",
    "href": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#spatial-joins",
    "title": "Spatial data processing",
    "section": "Spatial Joins",
    "text": "Spatial Joins\nSpatial joins combine data from two spatial datasets based on their spatial (i.e.¬†geographic) relationship, allowing you to connect attributes from different layers. This is similar to a table join, but instead of joining data based tabular data (e.g.¬†matching ID columns), we are joining data based on spatial relationships.\nFor example, you can match points (libraries) to polygons (wards) to see which administrative district contains each facility. And further, count how many libraries are in each.\nThis is similar to doing spatial selections and queries, but instead of filtering one of our input datasets, we are creating a new dataset.\nWe‚Äôll show an example in geopandas but checkout this tutorial for doing spatial joins in QGIS\n\ncrs = \"EPSG:32617\"\ngdf_tpl = gpd.read_file('./data/tpl.gpkg').to_crs(crs) \ngdf_regions = gpd.read_file('./data/toronto-regions.gpkg').to_crs(crs) \n\nWe‚Äôll begin by trying to join region names to library locations.\nWe can do this using the geopandas method .sjoin() (see similar documentation for QGIS).\n\ngdf_tpl_with_regions = gpd.sjoin(\n    gdf_tpl, \n    gdf_regions[['REGION_NAME', 'geometry']], \n    how='left', \n    predicate='within'\n)\ngdf_tpl_with_regions.head()\n\n\n\n\n\n\n\n\nAddress\nBranchName\ngeometry\nindex_right\nREGION_NAME\n\n\n\n\n0\n1515 Albion Road, Toronto, ON, M9V 1B2\nAlbion\nPOINT (613956.686 4843954.713)\n4.0\nETOBICOKE\n\n\n1\n496 Birchmount Road, Toronto, ON, M1K 1N8\nAlbert Campbell\nPOINT (639436.969 4840903.999)\n3.0\nSCARBOROUGH\n\n\n2\n2 Orianna Drive, Toronto, ON, M8W 4Y1\nAlderwood\nPOINT (617250.27 4828684.997)\n4.0\nETOBICOKE\n\n\n3\n155 Bonis Avenue, Toronto, ON, M1T 3W6\nAgincourt\nPOINT (637314.699 4849438.607)\n3.0\nSCARBOROUGH\n\n\n4\n2140 Avenue Road, Toronto, ON, M5M 4M7\nArmour Heights\nPOINT (627091.776 4844124.918)\n1.0\nNORTH YORK\n\n\n\n\n\n\n\nThe parameter predicate= can be one of several options for spatial relationship noted in the above section (e.g.¬†.intersects, .within, .crosses, etc.). The parameter how= is for the type of joining (e.g.¬†left, inner, right, etc.)\nView more in the geopandas documentation page for spatial joins.\nDoing a spatial join can then lead to a range of subsequent analyses, for example counting the number of libraries in each region. If you had numeric data, for example the number of books in each branch, the following count could be expanded to compute sum, mean, etc. for each group.\n\ngdf_tpl_with_regions.groupby('REGION_NAME').size()\n\nREGION_NAME\nEAST YORK       4\nETOBICOKE      12\nNORTH YORK     19\nSCARBOROUGH    16\nTORONTO        35\nYORK            6\ndtype: int64",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#overlay-operations",
    "href": "urban-data-analytics/spatial-data-processing/spatial-data-processing.html#overlay-operations",
    "title": "Spatial data processing",
    "section": "Overlay Operations",
    "text": "Overlay Operations\nOverlay operations take two spatial datasets and produce a new one based on the spatial relationships between their geometries.\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nintersection\nFinds only the overlapping areas between two layers\n\n\nunion\nMerges all geometries from both layers into a single combined shape\n\n\nsymmetric_difference\nKeeps geometry that is in either layer but not both\n\n\ndifference\nExtracts parts of the first layer that are not covered by the second\n\n\n\nThe image below is adapted from the full tutorial in geopandas on this topic which you can look at for more details. QGIS also has similar functions available.\n\n\n\nimage.png\n\n\nIn geopandas these operations are performed using gpd.overlay()\nLet‚Äôs take a look at a quick example, where we find the portions of wards in Toronto that overlap with the Old Toronto region area.\n\ncrs = 32617\ngdf_wards = gpd.read_file('./data/city-wards.gpkg').to_crs(crs)\ngdf_regions = gpd.read_file('./data/toronto-regions.gpkg').to_crs(crs)\n\n\n# Get Old Toronto polygon\nold_toronto = gdf_regions[gdf_regions[\"REGION_NAME\"] == \"TORONTO\"]\n\n# Find intersecting wards\nintersection = gpd.overlay(gdf_wards, old_toronto, how=\"intersection\", keep_geom_type=False)\n\nintersection.plot()\n\n\n\n\n\n\n\n\nThis has returned the portions of wards that overlap with the Old Toronto polygon.\nBy overlaying all the wards, we can see below that the orange area has been cut out\n\nax = gdf_wards.plot(alpha=0.5, edgecolor='white', color=\"pink\", linewidth=1)\nintersection.plot(ax=ax, edgecolor='white', color=\"orange\", linewidth=1)\ngdf_wards.plot(ax=ax, edgecolor='blue', color=\"none\", linewidth=1)",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "urban-data-analytics/openstreetmap/openstreetmap.html",
    "href": "urban-data-analytics/openstreetmap/openstreetmap.html",
    "title": "OpenStreetMap",
    "section": "",
    "text": "Nate Wessel\nThis notebook is meant to introduce the reader to OpenStreetMap - what it is, how to use it, and when and why you might want to. This is written for an audience that has some familiarity with spatial data and its applications."
  },
  {
    "objectID": "urban-data-analytics/openstreetmap/openstreetmap.html#what-is-openstreetmap",
    "href": "urban-data-analytics/openstreetmap/openstreetmap.html#what-is-openstreetmap",
    "title": "OpenStreetMap",
    "section": "What is OpenStreetMap?",
    "text": "What is OpenStreetMap?\nOpenStreetMap (OSM) is a world-wide, collaborative mapping project spanning a huge range of subjects. You might think of it as Wikipedia, if Wikipedia were a map instead of a written body of text. Anyone with a computer can contribute edits and can also download and use that data, along with the contributions of millions of others, for a wide range of uses. Specifically:\n\nYou are free to copy, distribute, transmit and adapt our data, as long as you credit OpenStreetMap and its contributors. If you alter or build upon our data, you may distribute the result only under the same licence. The full legal code explains your rights and responsibilities.\n\nThis openness has led to a huge number of applications based on OSM, ranging from large companies offering maps or navigation services, to many thousands of smaller projects started by software developers, activists, academics, and hobbyists.\nAt the technical level, OSM is just a big spatial database run by the OpenStreetMap Foundation. It sits on a server somewhere and people access it over the web to make edits and download data. But OSM is also a community and a practice. Decisions about how it runs and what gets mapped are largely decentralized, and often local."
  },
  {
    "objectID": "urban-data-analytics/openstreetmap/openstreetmap.html#whats-on-the-map",
    "href": "urban-data-analytics/openstreetmap/openstreetmap.html#whats-on-the-map",
    "title": "OpenStreetMap",
    "section": "What‚Äôs on the map?",
    "text": "What‚Äôs on the map?\nThe types of things included on the map range widely, from the glaciers of Greenland to the cafe around the corner from you. Train routes are on there, as are buildings, hundreds of millions of them, all the way down to your apartment or your mom‚Äôs garden shed. However OpenStreetMap does not cover all subject matter. The basic rule is that data should be, in some sense, verifiable by a person on the ground in a real tangible way. This allows us to map sidewalks if they‚Äôre there, but not the fact that a census tract contains people 37.8% of whom spoke French at home in the year 2021. It‚Äôs not a map of everything.\nTo give another example, you could map that a restaurant exists and serves Tibetan food (that‚Äôs probably spelled out on their menu, along with their hours and address), but it would be impossible to indicate that the neighbourhood contains many people from Tibet, because‚Ä¶ I guess you‚Äôd have to ask them all? What would be meant by ‚Äúmany‚Äù or even ‚Äúneighbourhood‚Äù? People will come to very different conclusions on that one. That‚Äôs not independently verifiable.\nWhether there is or is not a bus stop in a given spot is much more verifiable. In a given context at least, we can probably all agree what counts as a bus stop. This verifiability requirement leads OSM to a sort of discreteness that not all mapping efforts share. Something does or does not exist, is this type of thing or that type of thing. OSM doesn‚Äôt allow things to kind of exist or exist to a degree. It doesn‚Äôt allow us to have competing versions of reality.\nOne important exception to the verifiability rule is political or administrative boundaries, which often have no tangible existence in the real world. These are included for completeness, and because they rarely change, but should generally not be considered as authoritative. OSM also doesn‚Äôt include private or identifiable information. You can map a house, but not say who lives there, and you also won‚Äôt find any property lines in OSM, unless they just so happen to be marked by some physical boundary like a fence."
  },
  {
    "objectID": "urban-data-analytics/openstreetmap/openstreetmap.html#how-is-the-data-structured",
    "href": "urban-data-analytics/openstreetmap/openstreetmap.html#how-is-the-data-structured",
    "title": "OpenStreetMap",
    "section": "How is the data structured?",
    "text": "How is the data structured?\nThere are two aspects to pretty much any spatial dataset: the geometries and the attributes.\n\nSpatial data\nOSM uses a vector data model, not a raster model. Rasters are pixels covering an area with a gradation of values. Vectors are discrete points and lines in space. In OSM, the fundamental types of spatial data are nodes, ways, and relations.\n\nNodes: Nodes or points have a single coordinate location. They may exist on their own or be members of the other types. You might map a smallish object like a post box as a node.\nWays: Ways consist of an ordered series of two or more nodes. A way that starts and ends at different nodes is a line while one that starts and ends at the same node is often (but not always) considered a closed polygon. You would probably map a street as an open way (line), and a cemetery as a closed way (polygon).\nRelations: Relations are grouped collections of any of these data types, including, reflexively, relations. These are the most complex type. A tram route for example would be a relation. It has a route (the tracks it follows) and also some stops or platforms which could be represented as points or polygons along the route. Relations can also used be used to break up very large features like the Great Lakes, which might otherwise cause your computer to overheat. Or they can be used to describe multipolygon or multiline geometry types.\n\n\n\nAttributes (tags)\nThe way OSM handles attributes, ‚Äútags‚Äù in OSM parlance, is different from many other datasets you may be familiar with, especially those from government sources. Tags are associated with nodes, ways, and/or relations, though many nodes and ways may also have no tags at all if they belong to one of the other types (ways or relations) which has its own tags to describe the whole feature.\nTags take the form of key=value pairs, such as building=residential or amenity=restaurant. Usually, like these examples, their meaning is intuitively obvious, at least to an anglophone. In other cases, a more complex pattern has become the norm. For example, an address could be indicated with a set of tags like:\n\naddr:housenumber=123\naddr:unit=B\naddr:street=Example Street\naddr:postcode=A1B2C3\naddr:city=London\n\nThis approach breaks the address down into its parts to make the result more machine readable.\nTags, or at least their keys are always in English, and specifically British English since that‚Äôs where OSM originated. For names however (the tag for this is name=*), the practice is to always use the local language, for example name=Ëá∫ÁÅ£, while the English name can be given like name:en=Taiwan.\nAt this point, you may be noticing that I‚Äôm describing tagging with words like ‚Äúcould‚Äù, ‚Äúmay‚Äù, ‚Äúthe practice is‚Äù‚Ä¶ This is because there‚Äôs no single authority on what tags are valid or how they need to be constructed except for the technical constraint that both the keys and values are text strings. Instead, the OSM community has established and documented a wide range of norms for tagging common, and not so common, features. The best way to find these is by searching the OSM wiki, or perhaps by looking at what‚Äôs on the map in an area you‚Äôre familiar with. Tags that stray too far outside the norms are allowed but in practice are of little value. Applications that ingest OSM data may not support them and other mappers likely won‚Äôt know what to do with them either. Because of this, it can be important to know that a restaurant is amenity=restaurant, while a laundromat is shop=laundry. Why is it a shop and not also an amenity? Well, for the same reason that you have a tailbone: because it started out that way and now it‚Äôs pretty baked in and it‚Äôs not causing any harm. There are a lot of little idiosyncrasies like that but they‚Äôre not too hard to remember after a while.\nAnother important difference from many spatial datasets is that there‚Äôs no real concept of completeness for tagging. What tags would exhaustively describe a restaurant for example? OSM is always a work in progress. Often the first person to add something to the map does something very simple: saying, ‚Äúthere‚Äôs a restaurant here‚Äù (amenity=restaurant) and then a month later someone comes by and adds the tag cuisine=tibetan, telling us that it serves Tibetan food. Later on, someone may add the name and a web address for the restaurant, a phone number, a list of payment methods accepted, whether they serve vegetarian dishes, how accessible the place is, the hours‚Ä¶ any combination of any of these tags may be found together. Just when you think you‚Äôve tagged every conceivable attribute, someone will come along and point out that the building has a grey, slate, Mansard style roof (roof:colour=gray, roof:material=slate, roof:shape=mansard). It‚Äôs truly never done.\n\n\nSpatial data + tags\nTags can be applied to nodes, ways, or relations, and there‚Äôs not always a prescribed spatial type needed to define a given feature. To take up the restaurant example again, in many dense areas, restaurants are mostly mapped as nodes, especially if they share a building with other uses. But if a restaurant has its own building, then the amenity=restaurant tag might go on the way (polygon) representing the building. If that building happened to have an interior courtyard, it would need to be mapped as a relation (because the polygon‚Äôs inner ring would make it a multi-geometry) and the restaurant tags would go on the relation.\nAs I indicated before, tagging is also somewhat optional. Most nodes in the database have no tags at all and simply serve to help define the shapes and positions of the tagged lines or polygons that reference them.\nAs you can see, OSM‚Äôs data structure is enormously flexible. This is a huge strength, given the enormous variety of real things out there in the world that we want to describe, but it can in equal measure be a liability for those who want to use OSM to answer ‚Äúsimple‚Äù questions like ‚ÄúHow many Tibetan restaurants are there in Toronto?‚Äù"
  },
  {
    "objectID": "urban-data-analytics/openstreetmap/openstreetmap.html#strengths-and-weaknesses-of-osm-as-a-dataset",
    "href": "urban-data-analytics/openstreetmap/openstreetmap.html#strengths-and-weaknesses-of-osm-as-a-dataset",
    "title": "OpenStreetMap",
    "section": "Strengths and weaknesses of OSM as a dataset",
    "text": "Strengths and weaknesses of OSM as a dataset\nOSM seems like the kind of thing you might either love or hate, but are unlikely to have ambivalent feelings about, at least if you‚Äôre a geospatial nerd like me. Its great for some applications and for others might just drive you insane.\n\nStrengths\n\nFlexibility in describing the world\nAs described in the previous section, OSM has great flexibility for describing things in the real world. The structure of tagging means we can add information as it becomes available, add detail to entities that are already in the database, and also describe things at various levels of detail, both spatially and in terms of attributes.\n\n\nOSM spans boundaries\nFor much of my life, most of the world‚Äôs geospatial data was collected and administered by government agencies. These agencies did great work, right up to the edge of their jurisdiction and then beyond that was a vast, blank terra incognita. OpenStreetMap may include boundaries, but they sit inside of a global dataset and aren‚Äôt fundamental to how the map gets made. This means you can use OSM to analyse X or Y in North America, or within 500km of some point, without worrying about administrative differences in data collection.\n\n\n\nA cycling route crossing international borders\n\n\n\n\nImplicit topology\nThe topological (and cross-border) nature of OSM data makes it particularly well suited to transportation applications like finding walking or cycling directions from A to B. Indeed, this is the aspect of OSM to date that‚Äôs been the most widely commercialized, but it‚Äôs also available to you for free thanks to a wide range of open-source routing applications.\n\n\nVersion control\nOSM is intended to represent the current state of the world, but it also maintains a history of past changes. While much of the history of OSM to-date has been a story of adding more and more detailed data, there‚Äôs also a vast store of real history slowly accumulating in the database. There‚Äôs the potential to rewind the clock and see that that Tibetan restaurant used to be a dispensary, and before that a church.\n\n\nIt accepts your edits\nOne of the biggest benefits of working with OSM data for a project may be a the simple fact that you can make edits. Inevitably, the data you need for your analysis will be incomplete. But you can use what‚Äôs in OSM already as a starting point and add the bits that are missing, allowing you to complete your analysis (without having to collect all the data) while at the same time leaving the map better than you found it. I‚Äôve done this myself many times. When working with typical spatial datasets, where you download everything from some organization, it can be very hard to make edits and then also synchronize those edits with any updates from the authoritative source.\n\n\n\nWeaknesses\n\nInconsistency\nOSM data can be wildly inconsistent in quality, detail, and scale. In many parts of central Europe, you‚Äôd be hard pressed to add any more to the map, where even individual street trees can appear with their genus, species, and trunk circumference. On the flip side, much of rural Africa and South America can look like it was hastily sketched on a napkin with even major features like roadways or rivers totally absent or misaligned by dozens of meters. Even within a city like Toronto the difference in quality and completeness between downtown and the suburbs is pretty noticeable if you‚Äôre paying attention.\n\n\n\nData in northern Ontario missing large, defining features of the landscape such as lakes\n\n\n\n\nStrong selection biases in the data coverage\nEven the inconsistency of OSM data is inconsistent. That is to say, the inconsistency is very much not random and subject to the interests and biases of the contributors. One example that stands out to me is the recent rise of paid map editors (yes, by the way, those exist!) working for big international navigation and logistics companies. These companies use OSM as their primary data source and are very interested in doing quality control on features that could lead to errors for in-vehicle navigation systems. This can mean that tagging errors in, say, major North American roadways are very quickly identified and fixed, while infrastructure for pedestrians just does not get the same scrutiny. A comparative analysis of those features would very much need to consider that one of those features has a lot of money behind it.\n\n\nOSM is always changing\nEven setting aside the issues around consistency and bias, the nature of OSM as a community driven project means that it‚Äôs always growing and evolving. This can be frustrating for data consumers. Let me give an example. I wanted to make a map of cycling infrastructure - bike lanes and the like - and had written code to look for three of the common tags for that:\n\ncycleway=lane\ncycleway:left=lane\ncycleway:right=lane\n\nThese indicate respectively that a bike lane is on both sides of a street or just on the left or right side, depending which way the line is drawn. All was going well, until(!) at some point it started to become common to see the first tag supplanted by cycleway:both=lane, a tag that I didn‚Äôt even know to look for until I was deep into trying to figure out why some major lanes were missing from my map. The software I was using still doesn‚Äôt actually support this tagging out of the box, and it‚Äôs far too common in practice now for me to try to edit things back to the older tagging style. To keep my project up to date I must find ways to adapt my code.\n\n\nIt accepts your edits\nJust as it‚Äôs a strength that the wise cartographer can add their wisdom to the map, so is it a detriment that any fool can add their foolishness too. Almost all edits to OSM are made in good faith but it‚Äôs the nature of a growing project that there will always be beginners making simple mistakes or failing to fully understand the norms of the community. If you‚Äôre reading this, it‚Äôs likely you‚Äôll make some mistakes too and perhaps you‚Äôll get a polite message from me or some other local mapper one day pointing out a better way of doing things. I‚Äôve found OSM to be a supportive community that happily welcomes and develops newcomers, but the truth is that the map at any given moment is likely full of small mistakes that haven‚Äôt been fixed yet.\n\n\n\nA user has misused the name=* tag, filling it with a description and some kind of identifier. No one walking past this building would recognize that as its ‚Äúname‚Äù.\n\n\nOne example that comes to mind is the border between India and China. It‚Äôs a very large and complex boundary that neither country actually agrees on. OSM has a nuanced way of tagging disputed political boundaries, but I found that in practice the boundary relations for each country would often break down (i.e.¬†fail to be a closed polygon) because novice editors didn‚Äôt understand this complexity. It was a pain to have to fix China and re-download the data every once in a while when I was regularly making maps of the region."
  },
  {
    "objectID": "urban-data-analytics/openstreetmap/openstreetmap.html#access-the-data-and-contribute-edits",
    "href": "urban-data-analytics/openstreetmap/openstreetmap.html#access-the-data-and-contribute-edits",
    "title": "OpenStreetMap",
    "section": "Access the data and contribute edits",
    "text": "Access the data and contribute edits\nOSM is just a big online database and you can connect to it to both read (download) data and edit (contribute) data. I‚Äôll start by describing the download process, but you‚Äôll want to make sure you give editing a try too. Even if you think you‚Äôll only ever download data, I‚Äôve found that, first of all, whatever you‚Äôre doing you‚Äôre going to find some data that just needs to be fixed and who better to do it than you? Second, editing puts you in the shoes of the people who make all the data in the first place. There‚Äôs just no better way to understand what you‚Äôre working with than to be part of the process of making it.\n\nDownloading data\nOpenStreetMap itself, that is, openstreetmap.org offers fairly limited ways of downloading data; you can download either all the data within a fairly small bounding box as an XML file, or you can download a compressed version (.pbf) of the entire planet. Unless you‚Äôre a professional database administrator with some time to kill (and disk space!), I wouldn‚Äôt recommend the later option. But let‚Äôs talk briefly about that XML file format. For a very simple example, it looks something like this:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;osm version=\"0.6\" generator=\"openstreetmap-cgimap 2.0.1 (3529599 spike-06.openstreetmap.org)\" copyright=\"OpenStreetMap and contributors\" attribution=\"http://www.openstreetmap.org/copyright\" license=\"http://opendatacommons.org/licenses/odbl/1-0/\"&gt;\n    &lt;bounds minlat=\"43.8697990\" minlon=\"-79.7034210\" maxlat=\"43.8708900\" maxlon=\"-79.7012510\"/&gt;\n    &lt;node id=\"11278892239\" visible=\"true\" version=\"1\" changeset=\"142909966\" timestamp=\"2023-10-21T06:56:10Z\" user=\"nitemoon\" uid=\"15815701\" lat=\"43.8707980\" lon=\"-79.7028380\"/&gt;\n    &lt;node id=\"11278892240\" visible=\"true\" version=\"1\" changeset=\"142909966\" timestamp=\"2023-10-21T06:56:10Z\" user=\"nitemoon\" uid=\"15815701\" lat=\"43.8707710\" lon=\"-79.7029400\"/&gt;\n    &lt;node id=\"11278892241\" visible=\"true\" version=\"1\" changeset=\"142909966\" timestamp=\"2023-10-21T06:56:10Z\" user=\"nitemoon\" uid=\"15815701\" lat=\"43.8706280\" lon=\"-79.7028670\"/&gt;\n    &lt;node id=\"11278892242\" visible=\"true\" version=\"1\" changeset=\"142909966\" timestamp=\"2023-10-21T06:56:10Z\" user=\"nitemoon\" uid=\"15815701\" lat=\"43.8706550\" lon=\"-79.7027640\"/&gt;\n    &lt;way id=\"1217260361\" visible=\"true\" version=\"1\" changeset=\"142909966\" timestamp=\"2023-10-21T06:56:10Z\" user=\"nitemoon\" uid=\"15815701\"&gt;\n        &lt;nd ref=\"11278892239\"/&gt;\n        &lt;nd ref=\"11278892240\"/&gt;\n        &lt;nd ref=\"11278892241\"/&gt;\n        &lt;nd ref=\"11278892242\"/&gt;\n        &lt;nd ref=\"11278892239\"/&gt;\n        &lt;tag k=\"building\" v=\"yes\"/&gt;\n        &lt;tag k=\"source\" v=\"microsoft/BuildingFootprints\"/&gt;\n    &lt;/way&gt;\n&lt;/osm&gt;\nLook closely and you can see that the file structure directly mirrors the structure of the OSM database described above. There are four nodes, each with a unique numeric ID and a set of coordinates. There‚Äôs also a single way which contains an ordered list of references to each of those four nodes, and some tags describing what the way is. Take a close look and you should be able to tell from the XML that we‚Äôre looking at a single rectangular building mapped as a closed polygon. Note that while there are four nodes, there are actually five references to them, with the first and last reference being to the same node ID. This is what tells us this is a closed polygon and not an open line. There‚Äôs also a lot of extra metadata in this file which tells us who created these elements and when. The nodes themselves in this case don‚Äôt have any tags.\nBut as I said, what OSM itself can give you is limited. OSM wants to conserve resources on their servers to ensure that contributors get the freshest data at the scale they need it to make edits.\nFortunately, OSM as a community has a variety of resources to fill the gaps for data consumers. These kind Internet strangers have downloaded that whole planet file and set up their copies to stay synchronized with the main database. They then offer additional ways of downloading larger chunks of data from their cloned databases, though often with some lag, ranging from minutes to days, from the main database.\n\nThe Overpass API\nPerhaps the most versatile of these community sources is the Overpass API. Overpass allows you to query the database in a great variety of ways using its own OSM-specific query language. To be honest this can quickly get really complicated outside of some relatively simple use-cases, but it‚Äôs a really powerful tool if you‚Äôre willing to learn it. Here‚Äôs a really simple example to give you a flavour of it.\n[out:json][timeout:5][bbox:{{bbox}}];\n(node[shop=wool];way[shop=wool];)-&gt;.elements;\n.elements out geom;\nPretty easy to read, right? Here‚Äôs the same example with some explanatory comments.\n[out:json][timeout:5][bbox:{{bbox}}];\n// This is a comment. The lines above tell overpass to\n// 1. output results as JSON\n// 2. give up after 5 seconds if the query hasn't finished (lets the server plan better)\n// 3. search only within the current map view\n\n// gather results and store in .elements variable\n(\n    // the parentheses union nodes and ways together\n    node[shop=wool]; // collects any ways with these tags\n    way[shop=wool]; // collects any nodes...\n) -&gt; .elements;\n// output resulting geometries (and tags)\n.elements out geom;\nGo to the Overpass API and pop that query text in the sidebar. Zoom out until you can see your whole country and hit ‚Äúrun‚Äù. You should see a few scattered nodes show up on the map. Congrats! You just efficiently searched the many gigabytes of data within your map view for all the shops that sell yarn. Brits, by the way, call all sorts of yarn ‚Äúwool‚Äù, which can be quite confusing for people trying to knit with alpaca or acrylic fibres. This is a good example of the rule I mentioned earlier of tags always being in British English.\nNow click the ‚Äúexport‚Äù tab to download the data in a variety of formats. If you wanted to quickly make a map with this in a standard desktop GIS, GeoJSON would be a good, straightforward choice. You could just drag this straight into QGIS and begin styling the layers.\nPersonally, I often use Overpass to download fresh (delayed by just a minute or two) city or metro level data. There‚Äôs actually a link for this directly on openstreetmap.org, which will download everything within the current (rectangular) map view as an XML file. Give it a try! Zoom to your city or neighborhood, go to the ‚Äúexport‚Äù option, and select ‚ÄúOverpass API‚Äù in the sidebar to download. If you go too big (e.g.¬†country level), the API will timeout and throw an error.\nDid you download a file? Good! Now you may be asking: &gt; What on Earth will I do with this 600MB XML file? I tried to open it with a text editor and my computer crashed!\nIndeed. It‚Äôs a big file, and you‚Äôll probably want to use some kind of program to parse the file in some way before working with it. When making a map, I typically use a program called osm2pgsql to import the XML data into a PostgreSQL database on my computer. Many other tools exist though. I‚Äôll just point to a few and let you explore their documentation on your own.\n\nosmium: extract and filter data, or convert between OSM formats like PBF, XML, and JSON\nogr2ogr: convert between OSM and many other spatal formats like GeoJSON or shapefile\nOpen Source Routing Machine (ORSM): parse the street network and set up an efficient and customizable routing engine\n\n\n\nGeoFabrik Regional downloads\nAnother handy resource offers downloads in a format that might be more familiar to the typical GISer. GeoFabrik is a German geospatial company which offers OSM downloads in both compressed XLM (.osm.pbf) and shapefile format, packaged into chunks split up by major regional and political boundaries. For example you could download data within North America, Canada or Ontario.\nShapefiles are a very common format for spatial data which, like Ginko trees and horseshoe crabs, are a surprisingly ancient thing somehow still living among us. They‚Äôre tabular file format (that is, data in the form of a table), where each tagged entity (node, way, relation) is a row and each tag is a column. Because there are many, many tags, and most entities have only a few of them not all tags can be accommodated. Thus, the Geofabrik shapefiles can only include the most common tags, and are split into just a few thematic groups like buildings, landuse, natural features, place names, points of interest, etc. Shapefiles also must keep different geometry types separate, meaning there will be separate files for point, line, and polygon geometries.\nThese downloads can be a good entry point into OSM data if you‚Äôre already comfortable working with shapefiles but the file format limits what you can do with the data. If you want to make a quick map of common features without worrying too much about the nuances of the data structure, this is probably what you want.\n\n\n\nI was able to quickly make this map of the 2.9 million buildings mapped in Ontario so far\n\n\n\n\n\nContributing edits\nTo make edits, you‚Äôll need to create an OSM account. You can sign up or log in using the buttons on the top right of the page at openstreetmap.org.\nOnce you have an account, there are a few different ways of making edits. The most accessible and beginner friendly is the iD editor, which is built right into openstreetmap.org. It has a lot of useful features that help you find good descriptive tags for whatever you‚Äôre adding, but it can be pretty limited in its ability to make larger or more complex edits. It‚Äôs definitely the best place to start.\nI‚Äôll also talk briefly about the JOSM editor, which is my usual go-to. It‚Äôs a much more powerful desktop GIS software that will let you do just about anything, but without a lot of the guardrails provided for newer uses by iD.\nThese aren‚Äôt the only editors though. Just as there are many ways to download data, contributed by the OSM community, there are many ways to edit it.\n\nThe ID editor\nThe iD editor is the beginner-friendly editor built right into the page at openstreetmap.org. Just click ‚Äúedit‚Äù in the top menu and you‚Äôll be able to edit the contents of your current map view.\nFor your first edit (you‚Äôre following along, right?) I recommend starting in a rural or suburban area that you‚Äôre at least somewhat familiar with. Unless the area you‚Äôre in is very densely mapped already, you should able to find some features which aren‚Äôt mapped yet. Once you‚Äôre in editing mode, you‚Äôll see OSM‚Äôs vector map data overlaid on top of some recent aerial imagery.\n\n\n\nThe iD Editor, panning through rural Ontario\n\n\nPan the map until you find some clear features in the imagery that aren‚Äôt yet mapped. In the example above, I found some large buildings around what looks like a rural homestead (with a private basketball court??). To add the large building with the white roof as a polygon, I‚Äôll click on the ‚ÄúArea‚Äù button at the top, then click on each of the four corners of the building, double-clicking on the last one to close out the polygon.\n\n\n\nThe iD Editor, with a closed way (polygon) drawn but not tagged\n\n\nI‚Äôve now drawn the way, but not yet added any tags. On the left-hand panel, you can see iD suggesting some things this feature might be. I can tell this is a building, but not what kind of building it is. Maybe it‚Äôs full of cows? Or it could be their private indoor hockey rink. Who knows. Scrolling through the options, I see ‚ÄúBuilding Features‚Äù, click that, then select just ‚ÄúBuilding‚Äù. I also could have used the search feature to get to the same result.\n\n\n\nThe iD Editor, with a closed way (polygon) drawn and tagged\n\n\nI can now see the tag I‚Äôve added to this way (building=yes), along with some blanks for suggested tags that often accompany the building=* tag. For more information on any of these, you can click the ‚Äúi‚Äù button to get a detailed description of each tag. It‚Äôs OK to leave these tags blank.\nFor now, I‚Äôm happy with this edit and want to save it. I hit the ‚ÄúSave‚Äù button in the top right. Changes to the map are made in ‚Äúchangesets‚Äù and each one needs some description. I‚Äôll add a brief description for this simple edit, and also add a source for the information I‚Äôve added, selected from the drop-down selector. Adding a description and a source helps future editors understand something about the context of my edit.\n\n\n\nThe iD Editor, saving a changeset\n\n\nOnce you click the ‚ÄúUpload‚Äù button, your edit will be saved for all time in the OSM database. Be sure you haven‚Äôt done anything too silly, or added your banking information to the changeset comment. Here‚Äôs the edit I just made: https://www.openstreetmap.org/changeset/165765315.\n\n\n\nA recently uploaded changeset\n\n\nYou can see there‚Äôs a bit of extra information added by the editor, such as the actual source of the aerial imagery I was looking at. Notice that right after my edit, the building I added doesn‚Äôt actually show up on the main page‚Äôs map yet. My edit has made it into the OSM database, but it takes a while for changes to be reflected across the OSM ecosystem, such as on the rendered map or in data downloaded from other sources like those discussed above. By the time you‚Äôre reading this, you‚Äôll see the building at the link above, assuming it hasn‚Äôt been converted into a go-kart track by then.\nAs for your edit, if you wanted to confirm that your changes were saved, you could edit the map again in the same spot and you should see your modified data there since the editors all pull data, hot and fresh, from the main OSM database.\n\n\nJOSM\nWhile iD is the most popular OSM editor, and the best for beginners, you should also be aware that editors like JOSM are out there too. JOSM is basically a fully-featured desktop GIS specific to OSM. It has numerous plugins and a range of powerful (and potentially dangerous) features, like allowing the editing of many features at once. It you want to do some specific thing, but iD doesn‚Äôt seem to have a feature to support that, JOSM probably has three different ways of doing it.\n\n\n\nThe JOSM editor\n\n\nFor confident and experienced editors JOSM is a great tool but it can also be overwhelming and it counts on the user to know what they‚Äôre doing to a pretty large degree. While most individual OSM editors use iD, most actual map edits are made in JOSM.\nTo give an example, one important way of using JOSM is for data imports. Now first of all, one does not simply import data into OSM from other sources. Imports are a long, arduous journey of a process involving much discussion with the community, whose consensus and approval is required, and a very thorough review of data licensing and permissions. But once an import process is underway, JOSM could be used to bring in hundreds or even thousands of features at a time, while checking for conflicts with preexisting data."
  },
  {
    "objectID": "urban-data-analytics/openstreetmap/openstreetmap.html#concluding-thoughts",
    "href": "urban-data-analytics/openstreetmap/openstreetmap.html#concluding-thoughts",
    "title": "OpenStreetMap",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\nOSM is a whole world unto itself, I guess quite literally. Over the last decade or so its millions of users around the world, spanning many languages and cultures have developed a shared representation of the entire world. Like that world, it‚Äôs big and complex and messy and still very much a work in progress. But it‚Äôs also filled to the brim with useful, and indeed fascinating information. OSM can take a while to familiarize yourself with, and indeed you‚Äôll never know everything about it. It‚Äôs just too big and it‚Äôs always changing. But I hope I‚Äôve enticed you here to look a little more into it and consider OSM data as a potentially valuable resource in your various endeavors.\nWelcome to the OSM community!"
  }
]