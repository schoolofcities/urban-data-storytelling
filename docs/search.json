[
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html",
    "title": "Canadian census data",
    "section": "",
    "text": "Statistics Canada conducts a national census of the population every five years, asking a range of demographic and socio-economic questions. The results paint a demographic portrait of the country at the time period the census was conducted.\nThe most recent census at the time of writing was in 2021.\nLots of census data are publicly available for download.\nMost data are pre-aggregated to a variety of geographic boundaries (e.g. provinces, cities, neighbourhoods, blocks, etc.), which allow for finding a variety of demographic and socio-economic statistics for specific places as well as for making a range of maps.\nFor example, here’s a map of population density in the Greater Toronto Area (GTA), clearly showing where people are clustered throughout the region.\n\n\n\nMap of population density in Toronto\n\n\nMaps like these are pretty easy to make! Let’s learn how.\nSpecifically, this tutorial covers:\n\nan overview of Canadian Census data\nwhere to find census data on the Statistics Canada website\nhow to explore maps of census data using CensusMapper\nhow to download census data and make a choropleth map in QGIS\n\n\n\nThere are two parts to the census, the short-form survey and the long-form survey. The short-form survey asks a set of basic household and demographic questions (e.g. address, age, marital status, etc.) and is sent to all households in Canada. The long-from survey is sent to 25% of households in Canada. It asks additional questions pertaining to a broader range of demographic, social, and economic topics (e.g. religion, education, journey to work, etc.). Statistics Canada also augments collected census survey data by joining in data from other administrative sources, including income data collected by the Canadian Revenue Agency (CRA).\nCensus data are collected primarily on a household-by-household basis (one adult member in each household usually fills out the census on behalf of everyone in the household). Data of individual responses from the census are often called census “micro-data”. Because of personal identification concerns, this data is only accessible by accredited researchers. (A public use microdata file called the PUMF is available though. It is a random sample of the overall population, with several of the identifying variables removed, such as home addresses and postal code).\n\n\n\nSummaries (i.e. aggregations) of census data to a range of geographic areas are publicly available to view online or download. These are super useful for understanding the demographics of a place. For example, the total population in a province, the number of people who speak Spanish in Toronto, or the average income in a specific neighbourhood.\nThe Census Profile tables on Statistics Canada’s website allow for searching for census data for specific variables and geographic areas. For example, here’s an output of “Knowledge of Official Languages” in Ontario.\n\n\n\nTable of knowledge of language in Ontario\n\n\nWhen working with census data, it’s often advisable to use the Census Dictionary, the main reference guidebook, to understand what different data variables and geographies in the census represent. For example, here’s the entry for Knowledge of official languages.\nCensus profile data is typically limited to single categories totals (e.g. number of people who speak French) by gender, as shown in the table above. However, if you are interested cross-tabulations, summaries across multiple categories (e.g. number of people who have knowledge of French who also speak French at work, e.g. total number low-income residents who live in different types of housing), then there are a variety of Data Tables available for this purpose.\nIf neither the Census Profile or Data Tables fit your purpose, there is also a Public Use Microdata File (PUMF). This is a non-aggregated (i.e. each row is a disaggregated individual-level response) dataset covering a sample of the Canadian population. This data can be queried and cross-tabulated across any number of categories included. For privacy reasons, the data only include larger geographic linkages (e.g. provinces, large metro areas), and are only a sample of the overall census.\n\n\n\nThe are a number of geographic boundaries available with associated census data, ranging in scale from city blocks to the entire country. Below is an example of commonly used boundaries for urban-scale maps and analysis.\n\n\n\nCommon census boundaries in Toronto\n\n\nEach polygon on this map has associated publicly available summary census data. Joining this tabular data to these spatial boundaries allows for making a wide range of maps showing the distribution of demographics and socio-economic variables\nYou can bulk download census data for a number of geographic levels and formats from the Statistics Canada website. These downloads are essentially copies of the Census Profile data, but for all regions noted in each row.\n\n\n\nCensusMapper is a website for exploring and downloading census data across Canada. When we first land on the website, it defaults to a map of Population Density in Vancouver and it shares a number of preset options for making maps.\nIf we want to search for a specific census variable, we can click Make a Map in the top right, and then select the year (e.g. 2021). Here we can search and explore all available data. Using the search icon at the top-left or by clicking inset Canada map can help us to navigate elsewhere in the country. Just through a few clicks, I was able to create this map of knowledge of Portuguese in Toronto. Try making your own map!\n\n\n\nPortuguese in Toronto on CensusMapper\n\n\nWe can also use CensusMapper to download census data for specified geographic boundaries. To do so, click on API on the top right. First select the census year. Variable Selection is used for searching for and selecting the variables (i.e. attribute data such as knowledge of a particular language) to download. Region Selection is the geographic area that we want download data for (e.g. for Toronto). In the Overview panel, we can view what we’ve selected as well as pick the geographic aggregation level (e.g. Census Tracts, Dissemination Areas, etc.). Once selected, we can then download the attribute table and/or geographic boundaries.\nCensusMapper is partly built on an R library for downloading census data called cancensus. If you work with R, it is definitely worth checking out!\n\n\n\nWhile CensusMapper (and other online tools like it) are great for exploring and downloading data, we often want to make more customized maps (e.g. for a report, a paper, a website, etc.) or analyze census data in conjunction with other data sources (e.g. comparing demographic data to the location of libraries, public transit, or grocery stores, etc.).\nTo do so, the general process is to first download census data directly from the Statistics Canada website above or from CensusMapper and then load it into whatever software or library that you are working with.\nLINK TO CHROPLETH TUTORIAL AND OTHERS?",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#overview-of-the-canadian-census",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#overview-of-the-canadian-census",
    "title": "Canadian census data",
    "section": "",
    "text": "There are two parts to the census, the short-form survey and the long-form survey. The short-form survey asks a set of basic household and demographic questions (e.g. address, age, marital status, etc.) and is sent to all households in Canada. The long-from survey is sent to 25% of households in Canada. It asks additional questions pertaining to a broader range of demographic, social, and economic topics (e.g. religion, education, journey to work, etc.). Statistics Canada also augments collected census survey data by joining in data from other administrative sources, including income data collected by the Canadian Revenue Agency (CRA).\nCensus data are collected primarily on a household-by-household basis (one adult member in each household usually fills out the census on behalf of everyone in the household). Data of individual responses from the census are often called census “micro-data”. Because of personal identification concerns, this data is only accessible by accredited researchers. (A public use microdata file called the PUMF is available though. It is a random sample of the overall population, with several of the identifying variables removed, such as home addresses and postal code).",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#finding-census-data",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#finding-census-data",
    "title": "Canadian census data",
    "section": "",
    "text": "Summaries (i.e. aggregations) of census data to a range of geographic areas are publicly available to view online or download. These are super useful for understanding the demographics of a place. For example, the total population in a province, the number of people who speak Spanish in Toronto, or the average income in a specific neighbourhood.\nThe Census Profile tables on Statistics Canada’s website allow for searching for census data for specific variables and geographic areas. For example, here’s an output of “Knowledge of Official Languages” in Ontario.\n\n\n\nTable of knowledge of language in Ontario\n\n\nWhen working with census data, it’s often advisable to use the Census Dictionary, the main reference guidebook, to understand what different data variables and geographies in the census represent. For example, here’s the entry for Knowledge of official languages.\nCensus profile data is typically limited to single categories totals (e.g. number of people who speak French) by gender, as shown in the table above. However, if you are interested cross-tabulations, summaries across multiple categories (e.g. number of people who have knowledge of French who also speak French at work, e.g. total number low-income residents who live in different types of housing), then there are a variety of Data Tables available for this purpose.\nIf neither the Census Profile or Data Tables fit your purpose, there is also a Public Use Microdata File (PUMF). This is a non-aggregated (i.e. each row is a disaggregated individual-level response) dataset covering a sample of the Canadian population. This data can be queried and cross-tabulated across any number of categories included. For privacy reasons, the data only include larger geographic linkages (e.g. provinces, large metro areas), and are only a sample of the overall census.",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#census-geography",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#census-geography",
    "title": "Canadian census data",
    "section": "",
    "text": "The are a number of geographic boundaries available with associated census data, ranging in scale from city blocks to the entire country. Below is an example of commonly used boundaries for urban-scale maps and analysis.\n\n\n\nCommon census boundaries in Toronto\n\n\nEach polygon on this map has associated publicly available summary census data. Joining this tabular data to these spatial boundaries allows for making a wide range of maps showing the distribution of demographics and socio-economic variables\nYou can bulk download census data for a number of geographic levels and formats from the Statistics Canada website. These downloads are essentially copies of the Census Profile data, but for all regions noted in each row.",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#making-maps-with-censusmapper",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#making-maps-with-censusmapper",
    "title": "Canadian census data",
    "section": "",
    "text": "CensusMapper is a website for exploring and downloading census data across Canada. When we first land on the website, it defaults to a map of Population Density in Vancouver and it shares a number of preset options for making maps.\nIf we want to search for a specific census variable, we can click Make a Map in the top right, and then select the year (e.g. 2021). Here we can search and explore all available data. Using the search icon at the top-left or by clicking inset Canada map can help us to navigate elsewhere in the country. Just through a few clicks, I was able to create this map of knowledge of Portuguese in Toronto. Try making your own map!\n\n\n\nPortuguese in Toronto on CensusMapper\n\n\nWe can also use CensusMapper to download census data for specified geographic boundaries. To do so, click on API on the top right. First select the census year. Variable Selection is used for searching for and selecting the variables (i.e. attribute data such as knowledge of a particular language) to download. Region Selection is the geographic area that we want download data for (e.g. for Toronto). In the Overview panel, we can view what we’ve selected as well as pick the geographic aggregation level (e.g. Census Tracts, Dissemination Areas, etc.). Once selected, we can then download the attribute table and/or geographic boundaries.\nCensusMapper is partly built on an R library for downloading census data called cancensus. If you work with R, it is definitely worth checking out!",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#further-analysis-and-visualization-of-census-data",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#further-analysis-and-visualization-of-census-data",
    "title": "Canadian census data",
    "section": "",
    "text": "While CensusMapper (and other online tools like it) are great for exploring and downloading data, we often want to make more customized maps (e.g. for a report, a paper, a website, etc.) or analyze census data in conjunction with other data sources (e.g. comparing demographic data to the location of libraries, public transit, or grocery stores, etc.).\nTo do so, the general process is to first download census data directly from the Statistics Canada website above or from CensusMapper and then load it into whatever software or library that you are working with.\nLINK TO CHROPLETH TUTORIAL AND OTHERS?",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "This notebook provides an introduction for analyzing urban data. It will cover …\n\nExploring, filtering, and sorting data\nCleaning data and removing missing data\nCreating new columns from existing data\nJoining data from multiple tables\nComputing descriptive statistics (sum, mean, etc.)\nAggregating data via cross-tabulations and pivot tables\n\nTo do this, we’ll primarily be using pandas, a Python library for analyzing and organizing tabular data.\n\nimport pandas as pd\n\npandas is probably the most common library for working with both big and small datasets in Python, and is the basis for working with more analytical packages (e.g. numpy, scipy, scikit-learn) and analyzing geographic data (e.g. geopandas). For each section, we’ll also link to relevant documentation for doing similar tasks in Microsoft Excel and Google Sheets.\nHere are download links to this notebook and example datasets.\n\nDownload Notebook\n\ncities.csv\n\ncapitals.csv\n\nnew_york_cities.csv\n\n\n\nA very common way of structuring and analyzing a dataset is in a 2-dimensional table format, where rows are individual records or observations, while columns are attributes (often called variables) about those observations. For example, rows could each be a city and columns could indicate the population for different time periods. Data stored in spreadsheets often take this format.\nIn pandas, a DataFrame is a tabular data structure similar to a spreadsheet, where data is organized in rows and columns. Columns can contain different kinds of data, such as numbers, strings, dates, and so on. When we load data in pandas, we typically load it into the structure of a DataFrame.\nLet’s first take a look at a small dataset, Canadian municipalities and their population in 2021 and 2016, based on Census data. In Statistics Canada lingo, these are called Census Subdivisions. This dataset only includes municipalities with a population greater than 25,000 in 2021.\nThe main method for loading csv data is to use the read_csv function, but pandas can also read and write many other data formats.\n\ndf = pd.read_csv(\"data/cities.csv\")\n\nGreat! Now our data is stored in the variable df in the structure of a DataFrame.\n\n\n\nIn spreadsheet software, when we open a data file, we will see the top rows of the data right away.\nIn pandas, we can simply type the name of the DataFrame, in this case df, in the cell to view it. By default, it will print the top and bottom rows in the DataFrame\n\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nWinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows × 4 columns\n\n\n\nWe can specify which rows we want to view.\nLet’s explore what this data frame looks like. Adding the function .head(N) or .tail(N) prints the top or bottom N rows of the DataFrame. The following prints the first 10 rows.\nTry to edit this to print the bottom 10 rows or a different number of rows.\n\ndf.head(10)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n7\nBlainville\nQue.\n59819.0\nNaN\n\n\n8\nBoisbriand\nQue.\n28308.0\n26884.0\n\n\n9\nBoucherville\nQue.\n41743.0\n41671.0\n\n\n\n\n\n\n\nNotice that each column has a unique name. We can view the data of this column alone by using that name, and see what unique values exist using .unique().\nTry viewing the data of another column. Beware of upper and lower case – exact names matter!\n\ndf['Prov/terr'].head(10)  # Top 10 only\n\n0     B.C.\n1    Alta.\n2     Ont.\n3     Que.\n4     Ont.\n5     Ont.\n6     Ont.\n7     Que.\n8     Que.\n9     Que.\nName: Prov/terr, dtype: object\n\n\n\ndf['Prov/terr'].unique()  # Unique values for the *full* dataset - what happens if you do df['Prov/terr'].head(10).unique()?\n\narray(['B.C.', 'Alta.', 'Ont.', 'Que.', 'Man.', nan, 'N.S.', 'P.E.I.',\n       'N.L.', 'N.B.', 'Sask.', 'Y.T.'], dtype=object)\n\n\n\n\n\nWe often want to look at only a portion of our data that fit some set of conditions (e.g. all cities in a province that have a population more than 100,000). This is often called filtering, querying, or subsetting a dataset.\nLet’s try to do some filtering in pandas with our data of Canadian cities. Check out these links for filtering and sorting in spreadsheet software:\n\nFiltering in Excel\nFiltering in Google Sheets\n\nWe can use the columns to identify data that we might want to filter by. The line below shows data only for Ontario, but see if you can filter for another province or territory.\n\ndf.loc[df['Prov/terr'] == 'Ont.']\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n10\nBradford West Gwillimbury\nOnt.\n42880.0\n35325.0\n\n\n...\n...\n...\n...\n...\n\n\n171\nWhitby\nOnt.\n138501.0\n128377.0\n\n\n172\nWhitchurch-Stouffville\nOnt.\n49864.0\n45837.0\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n67 rows × 4 columns\n\n\n\nPandas allows us to use other similar mathematical concepts filter for data. Previously, we asked for all data in Ontario.\nNow, filter for all cities which had a population of at least 100,000 in 2021.\nHINT: in Python, “greater than or equals to” (i.e., “at least”) is represented using the syntax &gt;=.\nPandas also allows us to combine filtering conditions.\nUse the template below to select for all cities in Ontario with a population of over 100,000 in 2021.\n\ndf.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n\nNow let’s count how many cities actually meet these conditions. Run the line below to see how many cities there are in this data set in Ontario.\n\ndf.loc[df['Prov/terr'] == 'Ont.'].count()\n\nName                66\nProv/terr           67\nPopulation, 2021    66\nPopulation, 2016    65\ndtype: int64\n\n\nThe function .count() tells us how much data there is for each column - but if we wanted to just see one column, we could also filter for that individual column using df[COL_NAME].\nTry a different condition and count the amount of data for it.\n\n\n\nYou might have noticed that these cities are in alphabetical order - what if we wanted to see them in the order of population? In pandas, we do this using the sort_values function. The default is to sort in ascending order, so we set this to be False (i.e. descending) so the most populous cities are at the top.\n\ndf.sort_values(by='Population, 2021', ascending=False)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n\n\n89\nMontréal\nQue.\n1762949.0\n1704694.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n\n\n...\n...\n...\n...\n...\n\n\n115\nPrince Edward County\nOnt.\n25704.0\n24735.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n40\nDrummondville\nQue.\nNaN\n75423.0\n\n\n109\nPeterborough\nOnt.\nNaN\nNaN\n\n\n169\nWest Kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n179 rows × 4 columns\n\n\n\nLet’s put some in this together now.\nFilter the data to show all cities which are in the province of Quebec with at least a population of 50,000 in 2016, and then try to sort the cities by their 2016 population.\nHINT: You can do this in two steps (which is more readable) by storing the data that you filter into a variable called df_filtered, then running the command to sort the values on df_filtered.\n\n\n\nOnce we have processed and analyzed our data, we may want to save it for future use or share it with others. pandas makes it easy to export data to various formats, such as CSV or Excel files.\nBelow, we demonstrate how to export a DataFrame to both CSV and Excel formats. This is particularly useful for sharing results or viewing the data in other tools like spreadsheet software.\n\n# Save to CSV\ndf_filtered.to_csv('df_filtered.csv', index=False)\n\n# Save to Excel\ndf_filtered.to_excel('df_filtered.xlsx', index=False)\n\n\n\n\nOften, the data we have might not be in the condition want it to be in. Some data might be missing, and other data might have odd naming conventions.\nA simple example is that we might want all city names to be lowercase - which is what the code below does.\n\ndf['Name'] = df['Name'].str.lower()\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nabbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nairdrie\nAlta.\n74100.0\n61581.0\n\n\n2\najax\nOnt.\n126666.0\n119677.0\n\n\n3\nalma\nQue.\n30331.0\n30771.0\n\n\n4\naurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nwindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nwinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nwood buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nwoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nwoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows × 4 columns\n\n\n\npandas has a number of methods like str.lower() to alter data (see the full API). But the important thing to note here is that we directly modified the existing values of a column. We might not always want to do this, but often it is a good way of saving memory and shows that data frames are not just static forms but modifiable.\nLikewise, we might want better names for the columns we have. Take a look at the API for .rename() and modify the data frame df such that we rename the column Name to City. Pandas has more methods than we could ever remember - so learning to navigate the API is a crucial part of using the library.\nHINT: Take a look at the first example\n\n\n\nUnfortunately, it is pretty common that dataset we work with will be missing data values for some rows and columns. This can create complications when we want to produce summary statistics or visualizations. There are different strategies for dealing with this (i.e., imputing data), but the easiest is just to remove them. But first come out let’s check how much data is missing.\n\ndf.isnull().sum()\n\nName                3\nProv/terr           4\nPopulation, 2021    3\nPopulation, 2016    4\ndtype: int64\n\n\nIt seems that each column has a couple of data points missing. Let’s take a look at which rows these occur in. Similar to how we created a condition to filter for certain data, the code below creates a condition to filter for rows with missing data.\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n7\nblainville\nQue.\n59819.0\nNaN\n\n\n18\ncaledon\nNaN\n76581.0\n66502.0\n\n\n30\nNaN\nOnt.\n101427.0\n92013.0\n\n\n40\ndrummondville\nQue.\nNaN\n75423.0\n\n\n51\ngrimsby\nOnt.\n28883.0\nNaN\n\n\n64\nla prairie\nNaN\n26406.0\n24110.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n86\nmission\nNaN\n41519.0\n38554.0\n\n\n109\npeterborough\nOnt.\nNaN\nNaN\n\n\n131\nNaN\nQue.\n29954.0\n27359.0\n\n\n157\ntimmins\nNaN\n41145.0\n41788.0\n\n\n169\nwest kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n\n\n\nYou can see that some rows are missing multiple values, While others are just missing one. We can remove rows which have missing data using the function dropna and assign it to df so we’re working with complete data only going forward. Try to modify the code below to drop rows whose empty values are in one of the two population columns - that is, if the name or province is missing, we want to keep that row still. Look at the API to figure this out, specifically the argument subset for .dropna()\n\ndf = df.dropna()\n\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n\n\n\n\n\nGreat. Now let’s reset to our original data frame and exclude any rows with missing values.\n\ndf = pd.read_csv(\"data/cities.csv\")\ndf = df.dropna()\n\nInstead of dropping (i.e. removing) rows with missing values, we can instead replace them with specific values with fillna. For example, df.fillna(0) would replace all missing data values with a 0. This wouldn’t make sense in our data of Canadian cities, but can be useful in other contexts.\nSimilarly we can promgramatically find and replace data in any other column or across our dataset. For example, if we wanted to rename 'Y.T.' to 'Yukon' we would run the replace function as so df = df.replace('Y.T.', 'Yukon')\n\n\nWe can add or delete columns as needed. Let’s first add a column which shows the change in population between 2021 and 2016 and then sort by the cities that lost the most people. We can calculate that via a simple subtraction as follows.\n\ndf[\"pop_change\"] = df[\"Population, 2021\"] - df[\"Population, 2016\"]\ndf.sort_values(\"pop_change\", ascending = False).head(5)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\npop_change\n\n\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n83206.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n77811.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n67564.0\n\n\n11\nBrampton\nOnt.\n656480.0\n593638.0\n62842.0\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n62785.0\n\n\n\n\n\n\n\nPandas supports mathematical equations between columns, just like the subtraction we did above. Create a new column called pct_pop_change that computes the percentage change in population between 2016 and 2021, and sort by the cities with the greatest increase.\nHINT: the way to compute percent change is 100 * (Y - X) / X.\nNow let’s clear these new columns out using drop to return to what we had originally.\n\ndf = df.drop(columns=['pop_change', 'pct_pop_change'])\n\n\n\n\nMuch of the time, we are working with multiple sets of data which may overlap in key ways. Perhaps we want to include measures of income in cities, or look at voting patterns - this may require us to combine multiple data frames so we can analyze them.\nPandas methods to combine data frames are quite similar to that of database operations - if you’ve worked with SQL before, this will come very easily to you. There’s an extensive tutorial on this topic, but we’ll focus on simple cases of pd.concat() and pd.merge(). If you are curious about how to do this in spreadsheet software like Excel, check out this tutorial\nFirst, we can use pd.concat() to stack DataFrames vertically when new data has the same columns but additional rows (e.g., adding cities from another region). This is like adding new entries to a database table.\n\ndf_ny_cities = pd.read_csv(\"./data/new_york_cities.csv\")\ncombined = pd.concat([df, df_ny_cities], ignore_index=True)\nprint(\"Original rows:\", len(df), \"| After append:\", len(combined))\ndisplay(combined.tail(5))  # Show new rows\n\nOriginal rows: 167 | After append: 169\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n164\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n165\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n166\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n167\nNew York City\nN.Y.\n8804190.0\n8537673.0\n\n\n168\nBuffalo\nN.Y.\n278349.0\n258071.0\n\n\n\n\n\n\n\nSecond, we can use pd_merge() (or pd.concat(axis=1)) to combine DataFrames side-by-side when they share a key column (e.g., city names). Here, we’ll a column denoting whether the city is a provincial capital by matching city names.\nLet’s first load this data:\n\ndf_capitals = pd.read_csv('./data/capitals.csv')\ndf_capitals\n\n\n\n\n\n\n\n\nName\nIs_Capital\n\n\n\n\n0\nToronto\nTrue\n\n\n1\nQuébec\nTrue\n\n\n2\nVictoria\nTrue\n\n\n3\nEdmonton\nTrue\n\n\n4\nWinnipeg\nTrue\n\n\n5\nFredericton\nTrue\n\n\n6\nHalifax\nTrue\n\n\n7\nCharlottetown\nTrue\n\n\n8\nSt. John's\nTrue\n\n\n9\nRegina\nTrue\n\n\n10\nWhitehorse\nTrue\n\n\n\n\n\n\n\n\ndf_with_capitals = pd.merge(\n    df,  # Original data\n    df_capitals,  # New data\n    on=\"Name\",  # The same column \n    how=\"left\"  # Keep all data from the \"left\", ie. original\n)\n\n# Set non-capitals to False\ndf_with_capitals[\"Is_Capital\"] = df_with_capitals[\"Is_Capital\"].astype('boolean').fillna(False)\n\n# Verify: Show capitals and counts\nprint(f\"Found {df_with_capitals['Is_Capital'].sum()} capitals:\")\ndf_with_capitals[df_with_capitals[\"Is_Capital\"]]\n\nFound 11 capitals:\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\nIs_Capital\n\n\n\n\n23\nCharlottetown\nP.E.I.\n38809.0\n36094.0\nTrue\n\n\n38\nEdmonton\nAlta.\n1010899.0\n933088.0\nTrue\n\n\n41\nFredericton\nN.B.\n63116.0\n58721.0\nTrue\n\n\n49\nHalifax\nN.S.\n439819.0\n403131.0\nTrue\n\n\n108\nQuébec\nQue.\n549459.0\n531902.0\nTrue\n\n\n111\nRegina\nSask.\n226404.0\n215106.0\nTrue\n\n\n139\nSt. John's\nN.L.\n110525.0\n108860.0\nTrue\n\n\n147\nToronto\nOnt.\n2794356.0\n2731571.0\nTrue\n\n\n154\nVictoria\nB.C.\n91867.0\n85792.0\nTrue\n\n\n161\nWhitehorse\nY.T.\n28201.0\n25085.0\nTrue\n\n\n163\nWinnipeg\nMan.\n749607.0\n705244.0\nTrue\n\n\n\n\n\n\n\n\n\n\n\nThe data we have is only as good as we understand what’s going on. There’s some basic methods in pandas we can use to get an idea of what the data says.\nThe most basic function you can use to get an idea of what’s going on is .describe(). It shows you how much data there is, and a number of summary statistics.\nModify the code below to report summary statistics for cities in Quebec only.\n\ndf.describe()\n\n\n\n\n\n\n\n\nPopulation, 2021\nPopulation, 2016\n\n\n\n\ncount\n1.670000e+02\n1.670000e+02\n\n\nmean\n1.573169e+05\n1.487358e+05\n\n\nstd\n3.074452e+05\n2.959960e+05\n\n\nmin\n2.570400e+04\n2.378700e+04\n\n\n25%\n3.770050e+04\n3.449850e+04\n\n\n50%\n6.414100e+04\n6.316600e+04\n\n\n75%\n1.348910e+05\n1.255940e+05\n\n\nmax\n2.794356e+06\n2.731571e+06\n\n\n\n\n\n\n\nInstead of picking out an examining a subset of the data one by one, we can use the function .groupby(). Given a column name, it will group rows which have the same value. In the example below, that means grouping every row which has the same province name. We can then apply a function to this (or multiple functions using .agg()) to examine different aspects of the data.\n\n# Group by province and calculate total population\nprovince_pop = df.groupby('Prov/terr')['Population, 2021'].sum()\nprint(\"Total population by province:\")\nprovince_pop.sort_values(ascending=False)\n\nTotal population by province:\n\n\nProv/terr\nOnt.      11598308.0\nQue.       5474109.0\nB.C.       3662922.0\nAlta.      3192892.0\nMan.        800920.0\nSask.       563966.0\nN.S.        533513.0\nN.B.        240595.0\nN.L.        137693.0\nP.E.I.       38809.0\nY.T.         28201.0\nName: Population, 2021, dtype: float64\n\n\n\n# Multiple aggregation statistics\nstats = df.groupby('Prov/terr')['Population, 2021'].agg(['count', 'mean', 'max', 'sum'])\nstats\n\n\n\n\n\n\n\n\ncount\nmean\nmax\nsum\n\n\nProv/terr\n\n\n\n\n\n\n\n\nAlta.\n17\n187817.176471\n1306784.0\n3192892.0\n\n\nB.C.\n29\n126307.655172\n662248.0\n3662922.0\n\n\nMan.\n2\n400460.000000\n749607.0\n800920.0\n\n\nN.B.\n4\n60148.750000\n79470.0\n240595.0\n\n\nN.L.\n2\n68846.500000\n110525.0\n137693.0\n\n\nN.S.\n2\n266756.500000\n439819.0\n533513.0\n\n\nOnt.\n64\n181223.562500\n2794356.0\n11598308.0\n\n\nP.E.I.\n1\n38809.000000\n38809.0\n38809.0\n\n\nQue.\n41\n133514.853659\n1762949.0\n5474109.0\n\n\nSask.\n4\n140991.500000\n266141.0\n563966.0\n\n\nY.T.\n1\n28201.000000\n28201.0\n28201.0\n\n\n\n\n\n\n\nBelow, we’ve added a column which shows the percent growth for each city. Use .groupby('Prov/terr') and use the function .median() (just as we used .sum() above) to observe the median growth rate per province or territory. Make sure to use sort_values() and set ascending to False.\n\ndf['Percent_Growth'] = 100 * (df['Population, 2021'] - df['Population, 2016']) / df['Population, 2016'] \n\n\n\n\nA cross tabulation, also called a frequency table or a contingency table, is a table that shows the summarizes two categorical variables by displaying the number of occurrences in each pair of categories.\nLet’s show an example by counting the number of cities in each province by a categorization of city size.\nWe will need two tools to do this: - cut, which bins continuous numbers (like population) into categories (e.g., “Small”/“Medium”/“Large”), turning numbers into meaningful groups. - crosstab, which counts how often these groups appear together—like how many “Medium” cities exist per province—revealing patterns that raw numbers hide.\n\n# Create a size category column\ndf['Size'] = pd.cut(df['Population, 2021'],\n                    bins=[0, 50000, 200000, float('inf')],\n                    labels=['Small', 'Medium', 'Large'])\n\n# Cross-tab: Province vs. Size\nsize_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\nsize_table\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Create a size category column\n----&gt; 2 df['Size'] = pd.cut(df['Population, 2021'],\n      3                     bins=[0, 50000, 200000, float('inf')],\n      4                     labels=['Small', 'Medium', 'Large'])\n      6 # Cross-tab: Province vs. Size\n      7 size_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\n\nNameError: name 'pd' is not defined\n\n\n\nRecall that we just created the column 'Percent_Growth' as well. Use these two functions to create different bins for different levels of growth and cross tabulate them.\nIf you’ve worked with Excel or Google Sheets, this is very similar to doing a Pivot Table.\nPivot tables are able to summarize data across several categories, and for different types of summaries (e.g. sum, mean, median, etc.)\nThe pivot_table function in pandas to aggregate data, and has more options than the crosstab function. Both are quite similar though. Here’s an example where we are using pivot_table to sum the population in each province by the 3 size groups.\nTry to edit this to compute the mean or median city Percent_Growth\n\npd.pivot_table(data = df, values = ['Population, 2021'], index = ['Prov/terr'], columns = ['Size'], aggfunc = 'sum', observed=True)\n\n\n\n\n\n\n\n\nPopulation, 2021\n\n\nSize\nSmall\nMedium\nLarge\n\n\nProv/terr\n\n\n\n\n\n\n\nAlta.\n234664.0\n640545.0\n2317683.0\n\n\nB.C.\n330537.0\n1642753.0\n1689632.0\n\n\nMan.\nNaN\n51313.0\n749607.0\n\n\nN.B.\n28114.0\n212481.0\nNaN\n\n\nN.L.\n27168.0\n110525.0\nNaN\n\n\nN.S.\nNaN\n93694.0\n439819.0\n\n\nOnt.\n930812.0\n2925641.0\n7741855.0\n\n\nP.E.I.\n38809.0\nNaN\nNaN\n\n\nQue.\n806267.0\n1371544.0\n3296298.0\n\n\nSask.\n71421.0\nNaN\n492545.0\n\n\nY.T.\n28201.0\nNaN\nNaN\n\n\n\n\n\n\n\nThere are often multiple ways to achieve similar results. In the previous section we looked at the groupby function to summarize data by one column, while above we used crosstab or pivot_table. However, we could also use the groupby function for this purpose. Check out the example below.\nYou should notice that it has created a long-table formate rather than a wide-table format. Both formats can be useful, wide-table formats are easier for viewing data for only 2 categories, while long-table formats are often used for inputting data into modelling or visualization libraries.\n\ndf.groupby(['Prov/terr', 'Size'], observed=False)['Population, 2021'].agg(['sum'])\n\n\n\n\n\n\n\n\n\nsum\n\n\nProv/terr\nSize\n\n\n\n\n\nAlta.\nSmall\n234664.0\n\n\nMedium\n640545.0\n\n\nLarge\n2317683.0\n\n\nB.C.\nSmall\n330537.0\n\n\nMedium\n1642753.0\n\n\nLarge\n1689632.0\n\n\nMan.\nSmall\n0.0\n\n\nMedium\n51313.0\n\n\nLarge\n749607.0\n\n\nN.B.\nSmall\n28114.0\n\n\nMedium\n212481.0\n\n\nLarge\n0.0\n\n\nN.L.\nSmall\n27168.0\n\n\nMedium\n110525.0\n\n\nLarge\n0.0\n\n\nN.S.\nSmall\n0.0\n\n\nMedium\n93694.0\n\n\nLarge\n439819.0\n\n\nOnt.\nSmall\n930812.0\n\n\nMedium\n2925641.0\n\n\nLarge\n7741855.0\n\n\nP.E.I.\nSmall\n38809.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0\n\n\nQue.\nSmall\n806267.0\n\n\nMedium\n1371544.0\n\n\nLarge\n3296298.0\n\n\nSask.\nSmall\n71421.0\n\n\nMedium\n0.0\n\n\nLarge\n492545.0\n\n\nY.T.\nSmall\n28201.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#tables-dataframes",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#tables-dataframes",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "A very common way of structuring and analyzing a dataset is in a 2-dimensional table format, where rows are individual records or observations, while columns are attributes (often called variables) about those observations. For example, rows could each be a city and columns could indicate the population for different time periods. Data stored in spreadsheets often take this format.\nIn pandas, a DataFrame is a tabular data structure similar to a spreadsheet, where data is organized in rows and columns. Columns can contain different kinds of data, such as numbers, strings, dates, and so on. When we load data in pandas, we typically load it into the structure of a DataFrame.\nLet’s first take a look at a small dataset, Canadian municipalities and their population in 2021 and 2016, based on Census data. In Statistics Canada lingo, these are called Census Subdivisions. This dataset only includes municipalities with a population greater than 25,000 in 2021.\nThe main method for loading csv data is to use the read_csv function, but pandas can also read and write many other data formats.\n\ndf = pd.read_csv(\"data/cities.csv\")\n\nGreat! Now our data is stored in the variable df in the structure of a DataFrame.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#viewing-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#viewing-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "In spreadsheet software, when we open a data file, we will see the top rows of the data right away.\nIn pandas, we can simply type the name of the DataFrame, in this case df, in the cell to view it. By default, it will print the top and bottom rows in the DataFrame\n\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nWinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows × 4 columns\n\n\n\nWe can specify which rows we want to view.\nLet’s explore what this data frame looks like. Adding the function .head(N) or .tail(N) prints the top or bottom N rows of the DataFrame. The following prints the first 10 rows.\nTry to edit this to print the bottom 10 rows or a different number of rows.\n\ndf.head(10)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n7\nBlainville\nQue.\n59819.0\nNaN\n\n\n8\nBoisbriand\nQue.\n28308.0\n26884.0\n\n\n9\nBoucherville\nQue.\n41743.0\n41671.0\n\n\n\n\n\n\n\nNotice that each column has a unique name. We can view the data of this column alone by using that name, and see what unique values exist using .unique().\nTry viewing the data of another column. Beware of upper and lower case – exact names matter!\n\ndf['Prov/terr'].head(10)  # Top 10 only\n\n0     B.C.\n1    Alta.\n2     Ont.\n3     Que.\n4     Ont.\n5     Ont.\n6     Ont.\n7     Que.\n8     Que.\n9     Que.\nName: Prov/terr, dtype: object\n\n\n\ndf['Prov/terr'].unique()  # Unique values for the *full* dataset - what happens if you do df['Prov/terr'].head(10).unique()?\n\narray(['B.C.', 'Alta.', 'Ont.', 'Que.', 'Man.', nan, 'N.S.', 'P.E.I.',\n       'N.L.', 'N.B.', 'Sask.', 'Y.T.'], dtype=object)",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#filtering-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#filtering-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "We often want to look at only a portion of our data that fit some set of conditions (e.g. all cities in a province that have a population more than 100,000). This is often called filtering, querying, or subsetting a dataset.\nLet’s try to do some filtering in pandas with our data of Canadian cities. Check out these links for filtering and sorting in spreadsheet software:\n\nFiltering in Excel\nFiltering in Google Sheets\n\nWe can use the columns to identify data that we might want to filter by. The line below shows data only for Ontario, but see if you can filter for another province or territory.\n\ndf.loc[df['Prov/terr'] == 'Ont.']\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n10\nBradford West Gwillimbury\nOnt.\n42880.0\n35325.0\n\n\n...\n...\n...\n...\n...\n\n\n171\nWhitby\nOnt.\n138501.0\n128377.0\n\n\n172\nWhitchurch-Stouffville\nOnt.\n49864.0\n45837.0\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n67 rows × 4 columns\n\n\n\nPandas allows us to use other similar mathematical concepts filter for data. Previously, we asked for all data in Ontario.\nNow, filter for all cities which had a population of at least 100,000 in 2021.\nHINT: in Python, “greater than or equals to” (i.e., “at least”) is represented using the syntax &gt;=.\nPandas also allows us to combine filtering conditions.\nUse the template below to select for all cities in Ontario with a population of over 100,000 in 2021.\n\ndf.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n\nNow let’s count how many cities actually meet these conditions. Run the line below to see how many cities there are in this data set in Ontario.\n\ndf.loc[df['Prov/terr'] == 'Ont.'].count()\n\nName                66\nProv/terr           67\nPopulation, 2021    66\nPopulation, 2016    65\ndtype: int64\n\n\nThe function .count() tells us how much data there is for each column - but if we wanted to just see one column, we could also filter for that individual column using df[COL_NAME].\nTry a different condition and count the amount of data for it.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#sorting-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#sorting-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "You might have noticed that these cities are in alphabetical order - what if we wanted to see them in the order of population? In pandas, we do this using the sort_values function. The default is to sort in ascending order, so we set this to be False (i.e. descending) so the most populous cities are at the top.\n\ndf.sort_values(by='Population, 2021', ascending=False)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n\n\n89\nMontréal\nQue.\n1762949.0\n1704694.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n\n\n...\n...\n...\n...\n...\n\n\n115\nPrince Edward County\nOnt.\n25704.0\n24735.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n40\nDrummondville\nQue.\nNaN\n75423.0\n\n\n109\nPeterborough\nOnt.\nNaN\nNaN\n\n\n169\nWest Kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n179 rows × 4 columns\n\n\n\nLet’s put some in this together now.\nFilter the data to show all cities which are in the province of Quebec with at least a population of 50,000 in 2016, and then try to sort the cities by their 2016 population.\nHINT: You can do this in two steps (which is more readable) by storing the data that you filter into a variable called df_filtered, then running the command to sort the values on df_filtered.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#exporting-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#exporting-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Once we have processed and analyzed our data, we may want to save it for future use or share it with others. pandas makes it easy to export data to various formats, such as CSV or Excel files.\nBelow, we demonstrate how to export a DataFrame to both CSV and Excel formats. This is particularly useful for sharing results or viewing the data in other tools like spreadsheet software.\n\n# Save to CSV\ndf_filtered.to_csv('df_filtered.csv', index=False)\n\n# Save to Excel\ndf_filtered.to_excel('df_filtered.xlsx', index=False)",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#updating-and-renaming-columns",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#updating-and-renaming-columns",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Often, the data we have might not be in the condition want it to be in. Some data might be missing, and other data might have odd naming conventions.\nA simple example is that we might want all city names to be lowercase - which is what the code below does.\n\ndf['Name'] = df['Name'].str.lower()\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nabbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nairdrie\nAlta.\n74100.0\n61581.0\n\n\n2\najax\nOnt.\n126666.0\n119677.0\n\n\n3\nalma\nQue.\n30331.0\n30771.0\n\n\n4\naurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nwindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nwinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nwood buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nwoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nwoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows × 4 columns\n\n\n\npandas has a number of methods like str.lower() to alter data (see the full API). But the important thing to note here is that we directly modified the existing values of a column. We might not always want to do this, but often it is a good way of saving memory and shows that data frames are not just static forms but modifiable.\nLikewise, we might want better names for the columns we have. Take a look at the API for .rename() and modify the data frame df such that we rename the column Name to City. Pandas has more methods than we could ever remember - so learning to navigate the API is a crucial part of using the library.\nHINT: Take a look at the first example",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#handling-missing-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#handling-missing-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Unfortunately, it is pretty common that dataset we work with will be missing data values for some rows and columns. This can create complications when we want to produce summary statistics or visualizations. There are different strategies for dealing with this (i.e., imputing data), but the easiest is just to remove them. But first come out let’s check how much data is missing.\n\ndf.isnull().sum()\n\nName                3\nProv/terr           4\nPopulation, 2021    3\nPopulation, 2016    4\ndtype: int64\n\n\nIt seems that each column has a couple of data points missing. Let’s take a look at which rows these occur in. Similar to how we created a condition to filter for certain data, the code below creates a condition to filter for rows with missing data.\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n7\nblainville\nQue.\n59819.0\nNaN\n\n\n18\ncaledon\nNaN\n76581.0\n66502.0\n\n\n30\nNaN\nOnt.\n101427.0\n92013.0\n\n\n40\ndrummondville\nQue.\nNaN\n75423.0\n\n\n51\ngrimsby\nOnt.\n28883.0\nNaN\n\n\n64\nla prairie\nNaN\n26406.0\n24110.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n86\nmission\nNaN\n41519.0\n38554.0\n\n\n109\npeterborough\nOnt.\nNaN\nNaN\n\n\n131\nNaN\nQue.\n29954.0\n27359.0\n\n\n157\ntimmins\nNaN\n41145.0\n41788.0\n\n\n169\nwest kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n\n\n\nYou can see that some rows are missing multiple values, While others are just missing one. We can remove rows which have missing data using the function dropna and assign it to df so we’re working with complete data only going forward. Try to modify the code below to drop rows whose empty values are in one of the two population columns - that is, if the name or province is missing, we want to keep that row still. Look at the API to figure this out, specifically the argument subset for .dropna()\n\ndf = df.dropna()\n\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n\n\n\n\n\nGreat. Now let’s reset to our original data frame and exclude any rows with missing values.\n\ndf = pd.read_csv(\"data/cities.csv\")\ndf = df.dropna()\n\nInstead of dropping (i.e. removing) rows with missing values, we can instead replace them with specific values with fillna. For example, df.fillna(0) would replace all missing data values with a 0. This wouldn’t make sense in our data of Canadian cities, but can be useful in other contexts.\nSimilarly we can promgramatically find and replace data in any other column or across our dataset. For example, if we wanted to rename 'Y.T.' to 'Yukon' we would run the replace function as so df = df.replace('Y.T.', 'Yukon')\n\n\nWe can add or delete columns as needed. Let’s first add a column which shows the change in population between 2021 and 2016 and then sort by the cities that lost the most people. We can calculate that via a simple subtraction as follows.\n\ndf[\"pop_change\"] = df[\"Population, 2021\"] - df[\"Population, 2016\"]\ndf.sort_values(\"pop_change\", ascending = False).head(5)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\npop_change\n\n\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n83206.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n77811.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n67564.0\n\n\n11\nBrampton\nOnt.\n656480.0\n593638.0\n62842.0\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n62785.0\n\n\n\n\n\n\n\nPandas supports mathematical equations between columns, just like the subtraction we did above. Create a new column called pct_pop_change that computes the percentage change in population between 2016 and 2021, and sort by the cities with the greatest increase.\nHINT: the way to compute percent change is 100 * (Y - X) / X.\nNow let’s clear these new columns out using drop to return to what we had originally.\n\ndf = df.drop(columns=['pop_change', 'pct_pop_change'])\n\n\n\n\nMuch of the time, we are working with multiple sets of data which may overlap in key ways. Perhaps we want to include measures of income in cities, or look at voting patterns - this may require us to combine multiple data frames so we can analyze them.\nPandas methods to combine data frames are quite similar to that of database operations - if you’ve worked with SQL before, this will come very easily to you. There’s an extensive tutorial on this topic, but we’ll focus on simple cases of pd.concat() and pd.merge(). If you are curious about how to do this in spreadsheet software like Excel, check out this tutorial\nFirst, we can use pd.concat() to stack DataFrames vertically when new data has the same columns but additional rows (e.g., adding cities from another region). This is like adding new entries to a database table.\n\ndf_ny_cities = pd.read_csv(\"./data/new_york_cities.csv\")\ncombined = pd.concat([df, df_ny_cities], ignore_index=True)\nprint(\"Original rows:\", len(df), \"| After append:\", len(combined))\ndisplay(combined.tail(5))  # Show new rows\n\nOriginal rows: 167 | After append: 169\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n164\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n165\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n166\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n167\nNew York City\nN.Y.\n8804190.0\n8537673.0\n\n\n168\nBuffalo\nN.Y.\n278349.0\n258071.0\n\n\n\n\n\n\n\nSecond, we can use pd_merge() (or pd.concat(axis=1)) to combine DataFrames side-by-side when they share a key column (e.g., city names). Here, we’ll a column denoting whether the city is a provincial capital by matching city names.\nLet’s first load this data:\n\ndf_capitals = pd.read_csv('./data/capitals.csv')\ndf_capitals\n\n\n\n\n\n\n\n\nName\nIs_Capital\n\n\n\n\n0\nToronto\nTrue\n\n\n1\nQuébec\nTrue\n\n\n2\nVictoria\nTrue\n\n\n3\nEdmonton\nTrue\n\n\n4\nWinnipeg\nTrue\n\n\n5\nFredericton\nTrue\n\n\n6\nHalifax\nTrue\n\n\n7\nCharlottetown\nTrue\n\n\n8\nSt. John's\nTrue\n\n\n9\nRegina\nTrue\n\n\n10\nWhitehorse\nTrue\n\n\n\n\n\n\n\n\ndf_with_capitals = pd.merge(\n    df,  # Original data\n    df_capitals,  # New data\n    on=\"Name\",  # The same column \n    how=\"left\"  # Keep all data from the \"left\", ie. original\n)\n\n# Set non-capitals to False\ndf_with_capitals[\"Is_Capital\"] = df_with_capitals[\"Is_Capital\"].astype('boolean').fillna(False)\n\n# Verify: Show capitals and counts\nprint(f\"Found {df_with_capitals['Is_Capital'].sum()} capitals:\")\ndf_with_capitals[df_with_capitals[\"Is_Capital\"]]\n\nFound 11 capitals:\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\nIs_Capital\n\n\n\n\n23\nCharlottetown\nP.E.I.\n38809.0\n36094.0\nTrue\n\n\n38\nEdmonton\nAlta.\n1010899.0\n933088.0\nTrue\n\n\n41\nFredericton\nN.B.\n63116.0\n58721.0\nTrue\n\n\n49\nHalifax\nN.S.\n439819.0\n403131.0\nTrue\n\n\n108\nQuébec\nQue.\n549459.0\n531902.0\nTrue\n\n\n111\nRegina\nSask.\n226404.0\n215106.0\nTrue\n\n\n139\nSt. John's\nN.L.\n110525.0\n108860.0\nTrue\n\n\n147\nToronto\nOnt.\n2794356.0\n2731571.0\nTrue\n\n\n154\nVictoria\nB.C.\n91867.0\n85792.0\nTrue\n\n\n161\nWhitehorse\nY.T.\n28201.0\n25085.0\nTrue\n\n\n163\nWinnipeg\nMan.\n749607.0\n705244.0\nTrue",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#summary-statistics",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#summary-statistics",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "The data we have is only as good as we understand what’s going on. There’s some basic methods in pandas we can use to get an idea of what the data says.\nThe most basic function you can use to get an idea of what’s going on is .describe(). It shows you how much data there is, and a number of summary statistics.\nModify the code below to report summary statistics for cities in Quebec only.\n\ndf.describe()\n\n\n\n\n\n\n\n\nPopulation, 2021\nPopulation, 2016\n\n\n\n\ncount\n1.670000e+02\n1.670000e+02\n\n\nmean\n1.573169e+05\n1.487358e+05\n\n\nstd\n3.074452e+05\n2.959960e+05\n\n\nmin\n2.570400e+04\n2.378700e+04\n\n\n25%\n3.770050e+04\n3.449850e+04\n\n\n50%\n6.414100e+04\n6.316600e+04\n\n\n75%\n1.348910e+05\n1.255940e+05\n\n\nmax\n2.794356e+06\n2.731571e+06\n\n\n\n\n\n\n\nInstead of picking out an examining a subset of the data one by one, we can use the function .groupby(). Given a column name, it will group rows which have the same value. In the example below, that means grouping every row which has the same province name. We can then apply a function to this (or multiple functions using .agg()) to examine different aspects of the data.\n\n# Group by province and calculate total population\nprovince_pop = df.groupby('Prov/terr')['Population, 2021'].sum()\nprint(\"Total population by province:\")\nprovince_pop.sort_values(ascending=False)\n\nTotal population by province:\n\n\nProv/terr\nOnt.      11598308.0\nQue.       5474109.0\nB.C.       3662922.0\nAlta.      3192892.0\nMan.        800920.0\nSask.       563966.0\nN.S.        533513.0\nN.B.        240595.0\nN.L.        137693.0\nP.E.I.       38809.0\nY.T.         28201.0\nName: Population, 2021, dtype: float64\n\n\n\n# Multiple aggregation statistics\nstats = df.groupby('Prov/terr')['Population, 2021'].agg(['count', 'mean', 'max', 'sum'])\nstats\n\n\n\n\n\n\n\n\ncount\nmean\nmax\nsum\n\n\nProv/terr\n\n\n\n\n\n\n\n\nAlta.\n17\n187817.176471\n1306784.0\n3192892.0\n\n\nB.C.\n29\n126307.655172\n662248.0\n3662922.0\n\n\nMan.\n2\n400460.000000\n749607.0\n800920.0\n\n\nN.B.\n4\n60148.750000\n79470.0\n240595.0\n\n\nN.L.\n2\n68846.500000\n110525.0\n137693.0\n\n\nN.S.\n2\n266756.500000\n439819.0\n533513.0\n\n\nOnt.\n64\n181223.562500\n2794356.0\n11598308.0\n\n\nP.E.I.\n1\n38809.000000\n38809.0\n38809.0\n\n\nQue.\n41\n133514.853659\n1762949.0\n5474109.0\n\n\nSask.\n4\n140991.500000\n266141.0\n563966.0\n\n\nY.T.\n1\n28201.000000\n28201.0\n28201.0\n\n\n\n\n\n\n\nBelow, we’ve added a column which shows the percent growth for each city. Use .groupby('Prov/terr') and use the function .median() (just as we used .sum() above) to observe the median growth rate per province or territory. Make sure to use sort_values() and set ascending to False.\n\ndf['Percent_Growth'] = 100 * (df['Population, 2021'] - df['Population, 2016']) / df['Population, 2016']",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "<img src='https://raw.githubusercontent.com/schoolofcities/gta-immigration/refs/heads/main/src/assets/top-logo-full.svg'  style='height:auto;width:220px'></img><br>Urban Data Analytics & Visualization 📊📈🏙️",
    "section": "",
    "text": "Hello I am the homepage yes yes"
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html",
    "title": "Spatial data & GIS",
    "section": "",
    "text": "A lot of urban datasets are directly linked to specific places, e.g. addresses, streets, neighbourhoods, political or administrative boundaries, etc.\nData that include place-based information are often called spatial, geographic, or geospatial data.\nGeographic Information Systems (GIS) are tools and software for analyzing, processing, and visualizing spatial data.\nThis page will cover the basics of spatial data and how we can view and interact with this data in the software QGIS (you will need to download QGIS to work on the hands-on part of this tutorial).\n\n\nA spatial dataset is a combination of…\n\nattribute data (the what)\nlocation data and spatial dimensions (the where)\n\nSpatial data, this combination of attribute and location data, can be organized and represented in a number of different formats.\nFor example, a city can be represented on a map via a single point with a label (e.g. based on latitude and longitude coordinates). Or a city can be represented as a polygon, based on on it’s administrative boundary\nImportantly, there are always uncertainty about the level of accuracy, precision, and resolution of spatial data. Spatial data are abstractions of reality, and thus have some loss of information when used for visualization and analysis. Any analysis can only be as good as the available data.\nThe two most common forms of spatial data are vector data and raster data.\n\n\n\nExamples of spatial data\n\n\n\n\nVector data uses geographic coordinates, or a series of coordinates, to create points, lines, and polygons representing real-world features.\ne.g. in the map below (a screenshot of OpenStreetMap) lines are used to represent roads and rail, points for retail, polygons for parks and buildings, etc.\n\n\n\nScreenshot of OpenStreetMap\n\n\nSpatial data can be stored as columns in traditional table-based formats (e.g. .csv), but there are also a wide range of data formats specific to spatial data. These are some of the most common:\n\nGeoJSON .geojson\nGeoPacakge .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nHere is an example of a single point location of a city stored in a .geojson file. .geojson is a standardized data schema .json format for spatial data.\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [-80.992, 46.490]\n            },\n            \"properties\": {\n                \"city_name\": \"Sudbury\"\n            }\n        }\n    ]\n}\n\n\n\nRaster data represents space as a continuous grid with equal cell sizes. Each cell contains a value pertaining to the type of feature it represents. These values can be quantitative (e.g. elevation) or categorical (e.g. type of land use). Common examples of raster data include Digital Elevation Models (DEMs), satellite imagery, and scanned images like historical maps.\ne.g. the map below shows a DEM for Toronto at two different scales.\n\n\n\nDEM of Toronto\n\n\n\n\n\n\nWe use Geographic Information Systems (GIS) to analyze, manipulate, and visualize spatial data.\nWhy is GIS useful?\n\nexplore spatial patterns and relationships in data\ncreate maps for publications\ngenerate new data, either “by hand” or via spatial relationships from other data (e.g. through spatial queries)\nperform spatial analysis (i.e. statistical methods applied to spatial data)\n\nGIS is often thought of as more than just a tool or piece of software. It can refer to all aspects of managing and analyzing digital spatially referenced data.\nThe power of GIS software and tools is the ability to work with data stored in different layers (e.g. a layer for roads, another for buildings, and so on) in conjunction with each other. These layers can be visualized and analyzed relative to each other based on their spatial relationships.\n\n\n\nSimple AI generated schematic of layers\n\n\nGIS software usually links to data stored elsewhere on a computer, rather than in a project file. If the source location of the data (i.e. which folder it’s in) changes, then this will have to be updated in the GIS project. If data are edited in GIS, it will update the data in its source location.\nThe open-source QGIS and the proprietary ESRI ArcGIS are the two most used desktop GIS software. ESRI’s suite of tools are often used by larger corporate and government organizations while QGIS is more used by small consultants and freelancers, non-profits, and academia. Many spatial data processing, visualization, and analyses steps also have equivalents in Python, R, SQL and other programming languages via specific libraries\nWe’ll be working with QGIS and Python, because they free and work across multiple operating systems, and are commonly used across different areas of work and research. But pretty much everything we’ll show can be accomplished across different software options, the buttons and steps will just be a bit different.\nLearning the core analytical and visualization concepts and ideas are much more important than exactly where to click or what specific functions to run.\n\n\n\nA CRS is a framework for describing the geographic position of location-based data. It tells your GIS software where your data are located on the Earth. Without a CRS, your data would just be a bunch of numbers without an ability to place them on a map and relate them to other data.\nThere are thousands of CRSs. Each CRS has specific attributes about its position and one main set of spatial units (e.g. a CRS will measure location in degrees longitude/latitude, metres, miles, etc.).\nProbably the most common CRS is WGS84, which uses longitude/latitude to link data to the Earth’s surface.\nWhen doing spatial analyses and processing where we are comparing two or more datasets to each other, it is important that they are in the same CRS.\n\n\nCRS are often paired with map projections. This is sometimes called a Projected CRS.\nThe earth is a 3D globe, but most maps are represented on 2D surfaces (e.g. on a screen, piece of paper). Map projections are mathematical model to flatten the earth to create a 2D view.\nEvery projection distorts shape, area, distance, or direction in some way or another. Different map projections distort the Earth in different ways. Here are some examples! For more check out this projection transition tool\n\n\n\nQGIS screenshot without data\n\n\nFor doing analysis and an urban scale, it is important that we choose a map projection that conserves area and distances in both X and Y directions (i.e. space is not being distorted in one particular direction more than others)\nFor example, these are two aerial images of Toronto, the left uses a local Mercator projection which does not distort data at a local scale, while the right is a rectangular projection where degrees latitude are equal shown at the same scale as degrees longitude.\nA general recommendation for urban-scale analyses is to use a Universal Transverse Mercator (UTM) projected CRS, which conserves area, distance, and direction along bands of the earth. This is a good tool for finding which UTM zone your region is in.\n\n\n\n\nLet’s use QGIS to quickly view a few spatial datasets. When you open up QGIS, it should look something like this:\n\n\n\nQGIS screenshot without data\n\n\nThe Browser panel is used for finding and loading data on your computer or from external sources, each line and symbol is for a different data source. The Layer panel will list all the datasets that are loaded into your project. The big white space will populate with data once it is loaded. The buttons at the top include a variety of options for loading data, saving your project, exploring your map, and various common processing and analytical tools.\n\n\nLet’s begin by adding in a basemap. Basemaps are often used to provide geographic reference for other data that we load into QGIS.\nWe’ll add a basemap of OpenStreetMap, a free crowd-sourced map of the world, available as XYZ raster tiles. To do this, go to Layer - Data Source Manager or simply click on the Data Source Manager button highlighted below. When you open the Data Source Manager you have options for a wide variety of data types to add to your map.\n\n\n\nData source manager location in QGIS\n\n\n\n\n\nAdding an XYZ tile\n\n\nHere is the URL to paste into QGIS for adding an OpenStreetMap basemap: https://tile.openstreetmap.org/{z}/{x}/{y}.png.\nWhen you click ‘Add’ it should be added to the map. You can use the navigation buttons or just scroll on your mouse to zoom to your city.\n\n\n\nQGIS with OpenStreetMap\n\n\nLet’s now add some vector data to the map. We’ve pre-downloaded two datasets from the City of Toronto’s Open Data Portal - City Wards (polygons) - Library Locations (points)\nThese can be added via the Data Source Manager - Vector, or simply via drag-and-drop from the folder of where they are located on your computer.\n\n\n\nQGIS with Toronto data\n\n\n\n\n\nThe data added to the project will be listed in the Layers panel. The order of the layers determine the order of which they stack on top of each other on the map. In the example above, the libraries are listed at the top of the other two layers, and it thus appears on top of the other two layers on the map.\nTo change the layer order, you can drag individual layers to be above or below any other layer in the Layers panel.\nMost vector data has associated attribute data, e.g. the number of a ward, the name of a library, etc. To view this data, right-click on a layer and then click on Open Attribute Table. In GIS, column names are often called “Fields”\n\n\n\nWhen you load vector data like this into QGIS, the layers have default styling (e.g. colours, sizes, line-widths, etc.).\nThere are tonnes of options in QGIS to change these initial styles. To do so, right-click on a layer, go to Properties, and then Symbology.\nThere are lots of options in here to change the colours, size, and symbols.\nFor example, in the screenshot below, the sizes of the libraries and the line-widths of the wards are increased a bit, the wards are given a transparent fill, and the library symbols are converted to squares.\n\n\n\nQGIS with Toronto data\n\n\nTip! you get extra options if you click on the sub-component of the symbol. e.g. in this example I clicked on Simple Fill to find additional styling options like pattern-based fill styles.\n\n\n\nQGIS symbology example\n\n\nWe’ll explore data-driven styling, where colours or other style options are based on data values, in a future tutorial.\n\n\n\nYou can change the project CRS in QGIS by going to Project - Properties - CRS.\n\n\n\nYou can export and create copies of any of your data layers. This may be useful if you want to change the file format, the CRS, subset the attribute table, or simply just make a copy of the data.\nTo do this, go to Export - Save Features As, and you can adjust the Format, CRS, and other options as needed.\n\n\n\nYou can save a project in QGIS by going to Project and then Save or Save As.\nIt will get saved to a .qgis file. These files save styling, filters, layer orders, map scale and location, and layout settings.\nHowever, .qgis files do not save datasets directly in the file, only paths and links to where the data is stored, either on your computer or from a URL like the OpenStreetMap basemap. Saving only links to data is common in other tools and software.\nTherefore, if you share your project, you’ll need to also share any local datasets it is linking to.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data & GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#spatial-data",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#spatial-data",
    "title": "Spatial data & GIS",
    "section": "",
    "text": "A spatial dataset is a combination of…\n\nattribute data (the what)\nlocation data and spatial dimensions (the where)\n\nSpatial data, this combination of attribute and location data, can be organized and represented in a number of different formats.\nFor example, a city can be represented on a map via a single point with a label (e.g. based on latitude and longitude coordinates). Or a city can be represented as a polygon, based on on it’s administrative boundary\nImportantly, there are always uncertainty about the level of accuracy, precision, and resolution of spatial data. Spatial data are abstractions of reality, and thus have some loss of information when used for visualization and analysis. Any analysis can only be as good as the available data.\nThe two most common forms of spatial data are vector data and raster data.\n\n\n\nExamples of spatial data\n\n\n\n\nVector data uses geographic coordinates, or a series of coordinates, to create points, lines, and polygons representing real-world features.\ne.g. in the map below (a screenshot of OpenStreetMap) lines are used to represent roads and rail, points for retail, polygons for parks and buildings, etc.\n\n\n\nScreenshot of OpenStreetMap\n\n\nSpatial data can be stored as columns in traditional table-based formats (e.g. .csv), but there are also a wide range of data formats specific to spatial data. These are some of the most common:\n\nGeoJSON .geojson\nGeoPacakge .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nHere is an example of a single point location of a city stored in a .geojson file. .geojson is a standardized data schema .json format for spatial data.\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [-80.992, 46.490]\n            },\n            \"properties\": {\n                \"city_name\": \"Sudbury\"\n            }\n        }\n    ]\n}\n\n\n\nRaster data represents space as a continuous grid with equal cell sizes. Each cell contains a value pertaining to the type of feature it represents. These values can be quantitative (e.g. elevation) or categorical (e.g. type of land use). Common examples of raster data include Digital Elevation Models (DEMs), satellite imagery, and scanned images like historical maps.\ne.g. the map below shows a DEM for Toronto at two different scales.\n\n\n\nDEM of Toronto",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data & GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#geographic-information-systems-gis",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#geographic-information-systems-gis",
    "title": "Spatial data & GIS",
    "section": "",
    "text": "We use Geographic Information Systems (GIS) to analyze, manipulate, and visualize spatial data.\nWhy is GIS useful?\n\nexplore spatial patterns and relationships in data\ncreate maps for publications\ngenerate new data, either “by hand” or via spatial relationships from other data (e.g. through spatial queries)\nperform spatial analysis (i.e. statistical methods applied to spatial data)\n\nGIS is often thought of as more than just a tool or piece of software. It can refer to all aspects of managing and analyzing digital spatially referenced data.\nThe power of GIS software and tools is the ability to work with data stored in different layers (e.g. a layer for roads, another for buildings, and so on) in conjunction with each other. These layers can be visualized and analyzed relative to each other based on their spatial relationships.\n\n\n\nSimple AI generated schematic of layers\n\n\nGIS software usually links to data stored elsewhere on a computer, rather than in a project file. If the source location of the data (i.e. which folder it’s in) changes, then this will have to be updated in the GIS project. If data are edited in GIS, it will update the data in its source location.\nThe open-source QGIS and the proprietary ESRI ArcGIS are the two most used desktop GIS software. ESRI’s suite of tools are often used by larger corporate and government organizations while QGIS is more used by small consultants and freelancers, non-profits, and academia. Many spatial data processing, visualization, and analyses steps also have equivalents in Python, R, SQL and other programming languages via specific libraries\nWe’ll be working with QGIS and Python, because they free and work across multiple operating systems, and are commonly used across different areas of work and research. But pretty much everything we’ll show can be accomplished across different software options, the buttons and steps will just be a bit different.\nLearning the core analytical and visualization concepts and ideas are much more important than exactly where to click or what specific functions to run.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data & GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#coordinate-reference-systems-crs-and-map-projections",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#coordinate-reference-systems-crs-and-map-projections",
    "title": "Spatial data & GIS",
    "section": "",
    "text": "A CRS is a framework for describing the geographic position of location-based data. It tells your GIS software where your data are located on the Earth. Without a CRS, your data would just be a bunch of numbers without an ability to place them on a map and relate them to other data.\nThere are thousands of CRSs. Each CRS has specific attributes about its position and one main set of spatial units (e.g. a CRS will measure location in degrees longitude/latitude, metres, miles, etc.).\nProbably the most common CRS is WGS84, which uses longitude/latitude to link data to the Earth’s surface.\nWhen doing spatial analyses and processing where we are comparing two or more datasets to each other, it is important that they are in the same CRS.\n\n\nCRS are often paired with map projections. This is sometimes called a Projected CRS.\nThe earth is a 3D globe, but most maps are represented on 2D surfaces (e.g. on a screen, piece of paper). Map projections are mathematical model to flatten the earth to create a 2D view.\nEvery projection distorts shape, area, distance, or direction in some way or another. Different map projections distort the Earth in different ways. Here are some examples! For more check out this projection transition tool\n\n\n\nQGIS screenshot without data\n\n\nFor doing analysis and an urban scale, it is important that we choose a map projection that conserves area and distances in both X and Y directions (i.e. space is not being distorted in one particular direction more than others)\nFor example, these are two aerial images of Toronto, the left uses a local Mercator projection which does not distort data at a local scale, while the right is a rectangular projection where degrees latitude are equal shown at the same scale as degrees longitude.\nA general recommendation for urban-scale analyses is to use a Universal Transverse Mercator (UTM) projected CRS, which conserves area, distance, and direction along bands of the earth. This is a good tool for finding which UTM zone your region is in.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data & GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#working-with-spatial-data-in-qgis",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#working-with-spatial-data-in-qgis",
    "title": "Spatial data & GIS",
    "section": "",
    "text": "Let’s use QGIS to quickly view a few spatial datasets. When you open up QGIS, it should look something like this:\n\n\n\nQGIS screenshot without data\n\n\nThe Browser panel is used for finding and loading data on your computer or from external sources, each line and symbol is for a different data source. The Layer panel will list all the datasets that are loaded into your project. The big white space will populate with data once it is loaded. The buttons at the top include a variety of options for loading data, saving your project, exploring your map, and various common processing and analytical tools.\n\n\nLet’s begin by adding in a basemap. Basemaps are often used to provide geographic reference for other data that we load into QGIS.\nWe’ll add a basemap of OpenStreetMap, a free crowd-sourced map of the world, available as XYZ raster tiles. To do this, go to Layer - Data Source Manager or simply click on the Data Source Manager button highlighted below. When you open the Data Source Manager you have options for a wide variety of data types to add to your map.\n\n\n\nData source manager location in QGIS\n\n\n\n\n\nAdding an XYZ tile\n\n\nHere is the URL to paste into QGIS for adding an OpenStreetMap basemap: https://tile.openstreetmap.org/{z}/{x}/{y}.png.\nWhen you click ‘Add’ it should be added to the map. You can use the navigation buttons or just scroll on your mouse to zoom to your city.\n\n\n\nQGIS with OpenStreetMap\n\n\nLet’s now add some vector data to the map. We’ve pre-downloaded two datasets from the City of Toronto’s Open Data Portal - City Wards (polygons) - Library Locations (points)\nThese can be added via the Data Source Manager - Vector, or simply via drag-and-drop from the folder of where they are located on your computer.\n\n\n\nQGIS with Toronto data\n\n\n\n\n\nThe data added to the project will be listed in the Layers panel. The order of the layers determine the order of which they stack on top of each other on the map. In the example above, the libraries are listed at the top of the other two layers, and it thus appears on top of the other two layers on the map.\nTo change the layer order, you can drag individual layers to be above or below any other layer in the Layers panel.\nMost vector data has associated attribute data, e.g. the number of a ward, the name of a library, etc. To view this data, right-click on a layer and then click on Open Attribute Table. In GIS, column names are often called “Fields”\n\n\n\nWhen you load vector data like this into QGIS, the layers have default styling (e.g. colours, sizes, line-widths, etc.).\nThere are tonnes of options in QGIS to change these initial styles. To do so, right-click on a layer, go to Properties, and then Symbology.\nThere are lots of options in here to change the colours, size, and symbols.\nFor example, in the screenshot below, the sizes of the libraries and the line-widths of the wards are increased a bit, the wards are given a transparent fill, and the library symbols are converted to squares.\n\n\n\nQGIS with Toronto data\n\n\nTip! you get extra options if you click on the sub-component of the symbol. e.g. in this example I clicked on Simple Fill to find additional styling options like pattern-based fill styles.\n\n\n\nQGIS symbology example\n\n\nWe’ll explore data-driven styling, where colours or other style options are based on data values, in a future tutorial.\n\n\n\nYou can change the project CRS in QGIS by going to Project - Properties - CRS.\n\n\n\nYou can export and create copies of any of your data layers. This may be useful if you want to change the file format, the CRS, subset the attribute table, or simply just make a copy of the data.\nTo do this, go to Export - Save Features As, and you can adjust the Format, CRS, and other options as needed.\n\n\n\nYou can save a project in QGIS by going to Project and then Save or Save As.\nIt will get saved to a .qgis file. These files save styling, filters, layer orders, map scale and location, and layout settings.\nHowever, .qgis files do not save datasets directly in the file, only paths and links to where the data is stored, either on your computer or from a URL like the OpenStreetMap basemap. Saving only links to data is common in other tools and software.\nTherefore, if you share your project, you’ll need to also share any local datasets it is linking to.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data & GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/urban-data-analysis.html",
    "href": "notebooks/urban-data-analytics/urban-data-analysis.html",
    "title": "Hello yes",
    "section": "",
    "text": "Hello yes"
  },
  {
    "objectID": "notebooks/urban-data-analytics/urban-data-analytics.html",
    "href": "notebooks/urban-data-analytics/urban-data-analytics.html",
    "title": "Hello yes",
    "section": "",
    "text": "Hello yes"
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#cross-tabulations-and-pivot-tables",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#cross-tabulations-and-pivot-tables",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "A cross tabulation, also called a frequency table or a contingency table, is a table that shows the summarizes two categorical variables by displaying the number of occurrences in each pair of categories.\nLet’s show an example by counting the number of cities in each province by a categorization of city size.\nWe will need two tools to do this: - cut, which bins continuous numbers (like population) into categories (e.g., “Small”/“Medium”/“Large”), turning numbers into meaningful groups. - crosstab, which counts how often these groups appear together—like how many “Medium” cities exist per province—revealing patterns that raw numbers hide.\n\n# Create a size category column\ndf['Size'] = pd.cut(df['Population, 2021'],\n                    bins=[0, 50000, 200000, float('inf')],\n                    labels=['Small', 'Medium', 'Large'])\n\n# Cross-tab: Province vs. Size\nsize_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\nsize_table\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Create a size category column\n----&gt; 2 df['Size'] = pd.cut(df['Population, 2021'],\n      3                     bins=[0, 50000, 200000, float('inf')],\n      4                     labels=['Small', 'Medium', 'Large'])\n      6 # Cross-tab: Province vs. Size\n      7 size_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\n\nNameError: name 'pd' is not defined\n\n\n\nRecall that we just created the column 'Percent_Growth' as well. Use these two functions to create different bins for different levels of growth and cross tabulate them.\nIf you’ve worked with Excel or Google Sheets, this is very similar to doing a Pivot Table.\nPivot tables are able to summarize data across several categories, and for different types of summaries (e.g. sum, mean, median, etc.)\nThe pivot_table function in pandas to aggregate data, and has more options than the crosstab function. Both are quite similar though. Here’s an example where we are using pivot_table to sum the population in each province by the 3 size groups.\nTry to edit this to compute the mean or median city Percent_Growth\n\npd.pivot_table(data = df, values = ['Population, 2021'], index = ['Prov/terr'], columns = ['Size'], aggfunc = 'sum', observed=True)\n\n\n\n\n\n\n\n\nPopulation, 2021\n\n\nSize\nSmall\nMedium\nLarge\n\n\nProv/terr\n\n\n\n\n\n\n\nAlta.\n234664.0\n640545.0\n2317683.0\n\n\nB.C.\n330537.0\n1642753.0\n1689632.0\n\n\nMan.\nNaN\n51313.0\n749607.0\n\n\nN.B.\n28114.0\n212481.0\nNaN\n\n\nN.L.\n27168.0\n110525.0\nNaN\n\n\nN.S.\nNaN\n93694.0\n439819.0\n\n\nOnt.\n930812.0\n2925641.0\n7741855.0\n\n\nP.E.I.\n38809.0\nNaN\nNaN\n\n\nQue.\n806267.0\n1371544.0\n3296298.0\n\n\nSask.\n71421.0\nNaN\n492545.0\n\n\nY.T.\n28201.0\nNaN\nNaN\n\n\n\n\n\n\n\nThere are often multiple ways to achieve similar results. In the previous section we looked at the groupby function to summarize data by one column, while above we used crosstab or pivot_table. However, we could also use the groupby function for this purpose. Check out the example below.\nYou should notice that it has created a long-table formate rather than a wide-table format. Both formats can be useful, wide-table formats are easier for viewing data for only 2 categories, while long-table formats are often used for inputting data into modelling or visualization libraries.\n\ndf.groupby(['Prov/terr', 'Size'], observed=False)['Population, 2021'].agg(['sum'])\n\n\n\n\n\n\n\n\n\nsum\n\n\nProv/terr\nSize\n\n\n\n\n\nAlta.\nSmall\n234664.0\n\n\nMedium\n640545.0\n\n\nLarge\n2317683.0\n\n\nB.C.\nSmall\n330537.0\n\n\nMedium\n1642753.0\n\n\nLarge\n1689632.0\n\n\nMan.\nSmall\n0.0\n\n\nMedium\n51313.0\n\n\nLarge\n749607.0\n\n\nN.B.\nSmall\n28114.0\n\n\nMedium\n212481.0\n\n\nLarge\n0.0\n\n\nN.L.\nSmall\n27168.0\n\n\nMedium\n110525.0\n\n\nLarge\n0.0\n\n\nN.S.\nSmall\n0.0\n\n\nMedium\n93694.0\n\n\nLarge\n439819.0\n\n\nOnt.\nSmall\n930812.0\n\n\nMedium\n2925641.0\n\n\nLarge\n7741855.0\n\n\nP.E.I.\nSmall\n38809.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0\n\n\nQue.\nSmall\n806267.0\n\n\nMedium\n1371544.0\n\n\nLarge\n3296298.0\n\n\nSask.\nSmall\n71421.0\n\n\nMedium\n0.0\n\n\nLarge\n492545.0\n\n\nY.T.\nSmall\n28201.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/spatial-data-visualization/spatial-data-visualization.html",
    "href": "notebooks/urban-data-visualization/spatial-data-visualization/spatial-data-visualization.html",
    "title": "Maps & Spatial Data Visualization",
    "section": "",
    "text": "What is a map?\nReference maps vs thematic maps\nMaps always abstractions - l - we can’t show everything - we have to choose\n\n\n\n\n\n\n\n\n\n\n\nVisual variables often used for map data\nSingle rule\nStyle via data values - categorical example - e.g.  - numeric example - e.g.  — link to choropleth and prop symbol?",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Spatial Data Visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/spatial-data-visualization/spatial-data-visualization.html#simplification",
    "href": "notebooks/urban-data-visualization/spatial-data-visualization/spatial-data-visualization.html#simplification",
    "title": "Maps & Spatial Data Visualization",
    "section": "",
    "text": "Symbology\nVisual variables often used for map data\nReference maps vs thematic maps",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Spatial Data Visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/spatial-data-visualization/spatial-data-visualization.html#symbology",
    "href": "notebooks/urban-data-visualization/spatial-data-visualization/spatial-data-visualization.html#symbology",
    "title": "Maps & Spatial Data Visualization",
    "section": "",
    "text": "Visual variables often used for map data\nSingle rule\nStyle via data values - categorical example - e.g.  - numeric example - e.g.  — link to choropleth and prop symbol?",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Spatial Data Visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/spatial-data-visualization/maps-and-spatial-data-visualization.html",
    "href": "notebooks/urban-data-visualization/spatial-data-visualization/maps-and-spatial-data-visualization.html",
    "title": "Maps & Spatial Data Visualization",
    "section": "",
    "text": "What is a map?\nReference maps vs thematic maps\nMaps always abstractions - l - we can’t show everything - we have to choose\n\n\n\n\n\n\n\n\n\n\n\nVisual variables often used for map data\nSingle rule\nStyle via data values - categorical example - e.g.  - numeric example - e.g.  — link to choropleth and prop symbol?"
  },
  {
    "objectID": "notebooks/urban-data-visualization/spatial-data-visualization/maps-and-spatial-data-visualization.html#symbology",
    "href": "notebooks/urban-data-visualization/spatial-data-visualization/maps-and-spatial-data-visualization.html#symbology",
    "title": "Maps & Spatial Data Visualization",
    "section": "",
    "text": "Visual variables often used for map data\nSingle rule\nStyle via data values - categorical example - e.g.  - numeric example - e.g.  — link to choropleth and prop symbol?"
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "What is a map?\nCartography\nFor research, for communication\nDecisions, heirarchy\nReference (what is where, navigation) or thematic (show specific data, visualization for a story, etc.)\nStatic and interactive.\nThis page will cover the basics of making maps and spatial data visualizations. The last part of this notebook shows an example in QGIS. If you are interested in creating specific types of maps, check out the following notebooks: - Choropleth maps - Proportional symbol maps - Mapping point density\n\n\nExamples of reference maps - old and new\nMaps - Google Maps, for example, is probably the most viewed map in human history now.\nReference maps are often used as base maps for thematic maps and visualizing spatial data.\n\n\n\nExamples of thematic maps - geo-visualization - spatial data visualization\nThese types of maps can be super useful for exploratory data analysis during the research process.\n\n\n\nStatic (varying sizes)\nDynamic/interactive (dashboard or story)\nGenerally - start with static\nDynamic and interactive maps typically take more work to create as they include all the design thinking in a static map, plus additional thinking about motion and how a reader would interact with it.\n\n\n\nMaps always abstractions of reality. It is impossible to show everything in the world on a piece of paper or a screen - nor would we want to, we usually want to focus our reader’s attention on specific data, trend, or story. Making maps and spatial data visualizations therefore often involve various decisions on …\n\nselection of data to include on the map\ngeneralization of data to reduce the complexity of the data to make it easier to view\nhow we want to style and symbolize our data\nhow to order and layer data\nwhat other map layout elements to include (e.g. title, legend, north arrow), as well as how to design these to make a map easy to understand\n\nPOLITICS?\n\n\n\nCartographic selection is about picking the most important things to draw on a map so it’s not too crowded. Imagine drawing a treasure map — you’d likely want to show big landmarks like mountains or forests, but leave out tiny rocks or every individual tree.\nIn practice, this can include deciding about whether or not to include a dataset at all when creating a map (e.g. in GIS or Python or other map-making software), or if we do include the dataset, if we want to filter it to only show certain features.\nFor example, you may want to include a dataset of public transit lines as a reference layer for a map of your city. However, you may not want to include every single transit route. An example of cartographic selection would be to only show major transit lines by pre-filtering the data by mode (e.g. only show metro/subway) or frequency (e.g. only show routes where a bus or train comes every 10 minutes or less). Now of course if the goal of your map and research is specifically about public transit, you may want to include all the routes.\nThe process of selecting some, but not all of the data that you have available, reduces clutter on your maps and can make them easier to read. What data to include depends on your objectives, audience, and story.\n\n\n\nSimilar to selection is cartographic generalization, this is when mapmakers simplify real-world details to make maps clearer and less cluttered, especially at smaller scales.\nFor example, a coastline with tons of tiny twists and inlets might get smoothed out—keeping the overall shape but removing unnecessary complexity. This helps the map stay readable without losing its key features. It’s like sketching a quick but accurate version of a photo instead of drawing every single pixel.\nAnother example would be that if you had a dataset of sports and recreation facilities in a city with data on the type of activity each is predominately used by (e.g. baseball, football, tennis, etc.). We can choose to have a different colour or symbol for each of these types, or generalize these to have them all look the same (e.g. the same shade of green).\n\n\n\nOnce we’ve decided what we want to include on our map, we have to decide how we want to style each layer. In some mapping tools, like QGIS, style options are often called Symbology\nThese are the common graphic styling options available for vector data layers. If you’ve worked with other design software, especially vector graphic software, many of these will be familiar.\nPoints:\n\nSymbol type (e.g. circle, square, etc.)\nSize\nFill colour\nStroke colour\nStroke width\nOpacity\n\nLines:\n\nWidth\nColour\nStroke style (e.g. solid, dashed, etc.)\nOpacity\n\nPolygons:\n\nFill colour\nFill pattern (solid, hatching, etc.)\nStroke colour\nStroke width\nStroke style (e.g. solid, dashed, etc.)\nOpacity\n\nNote that there are other options as well, the above are just the most commonly used.\n\n\nSingle-rule styling is when we want all features in a dataset to look the same, regardless of how they may different in terms of their attributes\nIn the example in our notebook on introducing spatial data and GIS, we created a simple map of Toronto where we had two vector datasets 1) ward administrative boundaries (polygons) and 2) public libraries (points).\nEach of these layers is styled via a single rule. For example, all libraries are blue squares, even though some libraries may be larger than others or have longer opening hours.\n\n\n\nScreenshot of QGIS with single-rule based styling for two layers\n\n\n\n\n\nVisual variables often used for map data?\nStyle via data values - categorical example - e.g.  - numeric example - e.g.  — link to choropleth and prop symbol?",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbology",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbology",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Visual variables often used for map data\nSingle rule\nIn the example in our notebook on introducing spatial data and GIS, we created a simple map of Toronto where we had two vector datasets 1) ward administrative boundaries (polygons) and 2) public libraries (points).\n\n\nStyle via data values - categorical example - e.g.  - numeric example - e.g.  — link to choropleth and prop symbol?",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#examples-of-thematic-maps",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#examples-of-thematic-maps",
    "title": "Maps & Spatial Data Visualization",
    "section": "",
    "text": "Static or interactive.\nMaps always abstractions - l - we can’t show everything - we have to choose",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Spatial Data Visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#examples-of-thematic-maps---geo-vi",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#examples-of-thematic-maps---geo-vi",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Static or interactive.\nMaps always abstractions - l - we can’t show everything - we have to choose",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#examples-of-thematic-maps---geo-visualization---spatial-data-visualization",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#examples-of-thematic-maps---geo-visualization---spatial-data-visualization",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Maps always abstractions - l - we can’t show everything - we have to choose",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#selection",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#selection",
    "title": "Maps & Visualizing Spatial Data",
    "section": "Selection",
    "text": "Selection",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#generalization",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#generalization",
    "title": "Maps & Visualizing Spatial Data",
    "section": "Generalization",
    "text": "Generalization",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#simplification",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#simplification",
    "title": "Maps & Visualizing Spatial Data",
    "section": "Simplification",
    "text": "Simplification",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#map-layouts",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#map-layouts",
    "title": "Maps & Visualizing Spatial Data",
    "section": "Map layouts",
    "text": "Map layouts",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#reference-maps",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#reference-maps",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Examples of reference maps - old and new\nMaps - Google Maps, for example, is probably the most viewed map in human history now.\nReference maps are often used as base maps for thematic maps and visualizing spatial data.",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#decisions",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#decisions",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Maps always abstractions - l - we can’t show everything - we have to choose",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbology-1",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbology-1",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Visual variables often used for map data\nSingle rule\nStyle via data values - categorical example - e.g.  - numeric example - e.g.  — link to choropleth and prop symbol?",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#static-interactive-maps",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#static-interactive-maps",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Static (varying sizes) or interactive (dashboard or story)",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#static-dynamic-maps",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#static-dynamic-maps",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Static (varying sizes)\nDynamic/interactive (dashboard or story)\nGenerally - start with static\nDynamic and interactive maps typically take more work to create as they include all the design thinking in a static map, plus additional thinking about motion and how a reader would interact with it.",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#whats-on-the-map",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#whats-on-the-map",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Maps always abstractions of reality. It is impossible to show everything in the world on a piece of paper or a screen - nor would we want to, we usually want to focus our reader’s attention on specific data, trend, or story. Making maps and spatial data visualizations therefore often involve various decisions on …\n\nselection of data to include on the map\ngeneralization of data to reduce the complexity of the data to make it easier to view\nhow we want to style and symbolize our data\nhow to order and layer data\nwhat other map layout elements to include (e.g. title, legend, north arrow), as well as how to design these to make a map easy to understand\n\nPOLITICS?",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#cartographic-selection",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#cartographic-selection",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Cartographic selection is about picking the most important things to draw on a map so it’s not too crowded. Imagine drawing a treasure map — you’d likely want to show big landmarks like mountains or forests, but leave out tiny rocks or every individual tree.\nIn practice, this can include deciding about whether or not to include a dataset at all when creating a map (e.g. in GIS or Python or other map-making software), or if we do include the dataset, if we want to filter it to only show certain features.\nFor example, you may want to include a dataset of public transit lines as a reference layer for a map of your city. However, you may not want to include every single transit route. An example of cartographic selection would be to only show major transit lines by pre-filtering the data by mode (e.g. only show metro/subway) or frequency (e.g. only show routes where a bus or train comes every 10 minutes or less). Now of course if the goal of your map and research is specifically about public transit, you may want to include all the routes.\nThe process of selecting some, but not all of the data that you have available, reduces clutter on your maps and can make them easier to read. What data to include depends on your objectives, audience, and story.",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#cartographic-generalization",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#cartographic-generalization",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Similar to selection is cartographic generalization, this is when mapmakers simplify real-world details to make maps clearer and less cluttered, especially at smaller scales.\nFor example, a coastline with tons of tiny twists and inlets might get smoothed out—keeping the overall shape but removing unnecessary complexity. This helps the map stay readable without losing its key features. It’s like sketching a quick but accurate version of a photo instead of drawing every single pixel.\nAnother example would be that if you had a dataset of sports and recreation facilities in a city with data on the type of activity each is predominately used by (e.g. baseball, football, tennis, etc.). We can choose to have a different colour or symbol for each of these types, or generalize these to have them all look the same (e.g. the same shade of green).",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#thematic-maps",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#thematic-maps",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Examples of thematic maps - geo-visualization - spatial data visualization\nThese types of maps can be super useful for exploratory data analysis during the research process.",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbols-layer-styling",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbols-layer-styling",
    "title": "Maps & Visualizing Spatial Data",
    "section": "",
    "text": "Once we’ve decided what we want to include on our map, we have to decide how we want to style each layer. In some mapping tools, like QGIS, style options are often called Symbology\nThese are the common graphic styling options available for vector data layers. If you’ve worked with other design software, especially vector graphic software, many of these will be familiar.\nPoints:\n\nSymbol type (e.g. circle, square, etc.)\nSize\nFill colour\nStroke colour\nStroke width\nOpacity\n\nLines:\n\nWidth\nColour\nStroke style (e.g. solid, dashed, etc.)\nOpacity\n\nPolygons:\n\nFill colour\nFill pattern (solid, hatching, etc.)\nStroke colour\nStroke width\nStroke style (e.g. solid, dashed, etc.)\nOpacity\n\nNote that there are other options as well, the above are just the most commonly used.\n\n\nSingle-rule styling is when we want all features in a dataset to look the same, regardless of how they may different in terms of their attributes\nIn the example in our notebook on introducing spatial data and GIS, we created a simple map of Toronto where we had two vector datasets 1) ward administrative boundaries (polygons) and 2) public libraries (points).\nEach of these layers is styled via a single rule. For example, all libraries are blue squares, even though some libraries may be larger than others or have longer opening hours.\n\n\n\nScreenshot of QGIS with single-rule based styling for two layers\n\n\n\n\n\nVisual variables often used for map data?\nStyle via data values - categorical example - e.g.  - numeric example - e.g.  — link to choropleth and prop symbol?",
    "crumbs": [
      "Urban Data Visualization",
      "Maps & Visualizing Spatial Data"
    ]
  }
]