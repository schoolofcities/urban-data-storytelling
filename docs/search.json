[
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html",
    "title": "Canadian census data",
    "section": "",
    "text": "Statistics Canada conducts a national census of the population every five years, asking a range of demographic and socio-economic questions. The results paint a demographic portrait of the country at the time period the census was conducted.\nThe most recent census at the time of writing was in 2021.\nLots of census data are publicly available for download.\nMost data are pre-aggregated to a variety of geographic boundaries (e.g. provinces, cities, neighbourhoods, blocks, etc.), which allow for finding a variety of demographic and socio-economic statistics for specific places as well as for making a range of maps.\nFor example, here’s a map of population density in the Greater Toronto Area (GTA), clearly showing where people are clustered throughout the region.\n\n\n\nMap of population density in Toronto\n\n\nMaps like these are pretty easy to make! Let’s learn how.\nSpecifically, this tutorial covers:\n\nan overview of Canadian Census data\nwhere to find census data on the Statistics Canada website\nhow to explore maps of census data using CensusMapper\nhow to download census data and make a choropleth map in QGIS\n\n\n\nThere are two parts to the census, the short-form survey and the long-form survey. The short-form survey asks a set of basic household and demographic questions (e.g. address, age, marital status, etc.) and is sent to all households in Canada. The long-from survey is sent to 25% of households in Canada. It asks additional questions pertaining to a broader range of demographic, social, and economic topics (e.g. religion, education, journey to work, etc.). Statistics Canada also augments collected census survey data by joining in data from other administrative sources, including income data collected by the Canadian Revenue Agency (CRA).\nCensus data are collected primarily on a household-by-household basis (one adult member in each household usually fills out the census on behalf of everyone in the household). Data of individual responses from the census are often called census “micro-data”. Because of personal identification concerns, this data is only accessible by accredited researchers. (A public use microdata file called the PUMF is available though. It is a random sample of the overall population, with several of the identifying variables removed, such as home addresses and postal code).\n\n\n\nSummaries (i.e. aggregations) of census data to a range of geographic areas are publicly available to view online or download. These are super useful for understanding the demographics of a place. For example, the total population in a province, the number of people who speak Spanish in Toronto, or the average income in a specific neighbourhood.\nThe Census Profile tables on Statistics Canada’s website allow for searching for census data for specific variables and geographic areas. For example, here’s an output of “Knowledge of Official Languages” in Ontario.\n\n\n\nTable of knowledge of language in Ontario\n\n\nWhen working with census data, it’s often advisable to use the Census Dictionary, the main reference guidebook, to understand what different data variables and geographies in the census represent. For example, here’s the entry for Knowledge of official languages.\nCensus profile data is typically limited to single categories totals (e.g. number of people who speak French) by gender, as shown in the table above. However, if you are interested cross-tabulations, summaries across multiple categories (e.g. number of people who have knowledge of French who also speak French at work, e.g. total number low-income residents who live in different types of housing), then there are a variety of Data Tables available for this purpose.\nIf neither the Census Profile or Data Tables fit your purpose, there is also a Public Use Microdata File (PUMF). This is a non-aggregated (i.e. each row is a disaggregated individual-level response) dataset covering a sample of the Canadian population. This data can be queried and cross-tabulated across any number of categories included. For privacy reasons, the data only include larger geographic linkages (e.g. provinces, large metro areas), and are only a sample of the overall census.\n\n\n\nThe are a number of geographic boundaries available with associated census data, ranging in scale from city blocks to the entire country. Below is an example of commonly used boundaries for urban-scale maps and analysis.\n\n\n\nCommon census boundaries in Toronto\n\n\nEach polygon on this map has associated publicly available summary census data. Joining this tabular data to these spatial boundaries allows for making a wide range of maps showing the distribution of demographics and socio-economic variables\nYou can bulk download census data for a number of geographic levels and formats from the Statistics Canada website. These downloads are essentially copies of the Census Profile data, but for all regions noted in each row.\n\n\n\nCensusMapper is a website for exploring and downloading census data across Canada. When we first land on the website, it defaults to a map of Population Density in Vancouver and it shares a number of preset options for making maps.\nIf we want to search for a specific census variable, we can click Make a Map in the top right, and then select the year (e.g. 2021). Here we can search and explore all available data. Using the search icon at the top-left or by clicking inset Canada map can help us to navigate elsewhere in the country. Just through a few clicks, I was able to create this map of knowledge of Portuguese in Toronto. Try making your own map!\n\n\n\nPortuguese in Toronto on CensusMapper\n\n\nWe can also use CensusMapper to download census data for specified geographic boundaries. To do so, click on API on the top right. First select the census year. Variable Selection is used for searching for and selecting the variables (i.e. attribute data such as knowledge of a particular language) to download. Region Selection is the geographic area that we want download data for (e.g. for Toronto). In the Overview panel, we can view what we’ve selected as well as pick the geographic aggregation level (e.g. Census Tracts, Dissemination Areas, etc.). Once selected, we can then download the attribute table and/or geographic boundaries.\nCensusMapper is partly built on an R library for downloading census data called cancensus. If you work with R, it is definitely worth checking out!\n\n\n\nWhile CensusMapper (and other online tools like it) are great for exploring and downloading data, we often want to make more customized maps (e.g. for a report, a paper, a website, etc.) or analyze census data in conjunction with other data sources (e.g. comparing demographic data to the location of libraries, public transit, or grocery stores, etc.).\nTo do so, the general process is to first download census data directly from the Statistics Canada website above or from CensusMapper and then load it into whatever software or library that you are working with.\nLINK TO CHROPLETH TUTORIAL AND OTHERS?",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#overview-of-the-canadian-census",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#overview-of-the-canadian-census",
    "title": "Canadian census data",
    "section": "",
    "text": "There are two parts to the census, the short-form survey and the long-form survey. The short-form survey asks a set of basic household and demographic questions (e.g. address, age, marital status, etc.) and is sent to all households in Canada. The long-from survey is sent to 25% of households in Canada. It asks additional questions pertaining to a broader range of demographic, social, and economic topics (e.g. religion, education, journey to work, etc.). Statistics Canada also augments collected census survey data by joining in data from other administrative sources, including income data collected by the Canadian Revenue Agency (CRA).\nCensus data are collected primarily on a household-by-household basis (one adult member in each household usually fills out the census on behalf of everyone in the household). Data of individual responses from the census are often called census “micro-data”. Because of personal identification concerns, this data is only accessible by accredited researchers. (A public use microdata file called the PUMF is available though. It is a random sample of the overall population, with several of the identifying variables removed, such as home addresses and postal code).",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#finding-census-data",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#finding-census-data",
    "title": "Canadian census data",
    "section": "",
    "text": "Summaries (i.e. aggregations) of census data to a range of geographic areas are publicly available to view online or download. These are super useful for understanding the demographics of a place. For example, the total population in a province, the number of people who speak Spanish in Toronto, or the average income in a specific neighbourhood.\nThe Census Profile tables on Statistics Canada’s website allow for searching for census data for specific variables and geographic areas. For example, here’s an output of “Knowledge of Official Languages” in Ontario.\n\n\n\nTable of knowledge of language in Ontario\n\n\nWhen working with census data, it’s often advisable to use the Census Dictionary, the main reference guidebook, to understand what different data variables and geographies in the census represent. For example, here’s the entry for Knowledge of official languages.\nCensus profile data is typically limited to single categories totals (e.g. number of people who speak French) by gender, as shown in the table above. However, if you are interested cross-tabulations, summaries across multiple categories (e.g. number of people who have knowledge of French who also speak French at work, e.g. total number low-income residents who live in different types of housing), then there are a variety of Data Tables available for this purpose.\nIf neither the Census Profile or Data Tables fit your purpose, there is also a Public Use Microdata File (PUMF). This is a non-aggregated (i.e. each row is a disaggregated individual-level response) dataset covering a sample of the Canadian population. This data can be queried and cross-tabulated across any number of categories included. For privacy reasons, the data only include larger geographic linkages (e.g. provinces, large metro areas), and are only a sample of the overall census.",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#census-geography",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#census-geography",
    "title": "Canadian census data",
    "section": "",
    "text": "The are a number of geographic boundaries available with associated census data, ranging in scale from city blocks to the entire country. Below is an example of commonly used boundaries for urban-scale maps and analysis.\n\n\n\nCommon census boundaries in Toronto\n\n\nEach polygon on this map has associated publicly available summary census data. Joining this tabular data to these spatial boundaries allows for making a wide range of maps showing the distribution of demographics and socio-economic variables\nYou can bulk download census data for a number of geographic levels and formats from the Statistics Canada website. These downloads are essentially copies of the Census Profile data, but for all regions noted in each row.",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#making-maps-with-censusmapper",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#making-maps-with-censusmapper",
    "title": "Canadian census data",
    "section": "",
    "text": "CensusMapper is a website for exploring and downloading census data across Canada. When we first land on the website, it defaults to a map of Population Density in Vancouver and it shares a number of preset options for making maps.\nIf we want to search for a specific census variable, we can click Make a Map in the top right, and then select the year (e.g. 2021). Here we can search and explore all available data. Using the search icon at the top-left or by clicking inset Canada map can help us to navigate elsewhere in the country. Just through a few clicks, I was able to create this map of knowledge of Portuguese in Toronto. Try making your own map!\n\n\n\nPortuguese in Toronto on CensusMapper\n\n\nWe can also use CensusMapper to download census data for specified geographic boundaries. To do so, click on API on the top right. First select the census year. Variable Selection is used for searching for and selecting the variables (i.e. attribute data such as knowledge of a particular language) to download. Region Selection is the geographic area that we want download data for (e.g. for Toronto). In the Overview panel, we can view what we’ve selected as well as pick the geographic aggregation level (e.g. Census Tracts, Dissemination Areas, etc.). Once selected, we can then download the attribute table and/or geographic boundaries.\nCensusMapper is partly built on an R library for downloading census data called cancensus. If you work with R, it is definitely worth checking out!",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#further-analysis-and-visualization-of-census-data",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#further-analysis-and-visualization-of-census-data",
    "title": "Canadian census data",
    "section": "",
    "text": "While CensusMapper (and other online tools like it) are great for exploring and downloading data, we often want to make more customized maps (e.g. for a report, a paper, a website, etc.) or analyze census data in conjunction with other data sources (e.g. comparing demographic data to the location of libraries, public transit, or grocery stores, etc.).\nTo do so, the general process is to first download census data directly from the Statistics Canada website above or from CensusMapper and then load it into whatever software or library that you are working with.\nLINK TO CHROPLETH TUTORIAL AND OTHERS?",
    "crumbs": [
      "Urban Data Analytics",
      "Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "This notebook provides an introduction for analyzing urban data. It will cover …\n\nExploring, filtering, and sorting data\nCleaning data and removing missing data\nCreating new columns from existing data\nJoining data from multiple tables\nComputing descriptive statistics (sum, mean, etc.)\nAggregating data via cross-tabulations and pivot tables\n\nTo do this, we’ll primarily be using pandas, a Python library for analyzing and organizing tabular data.\nWhen using external libraries like pandas, we need to install the library before we can import and then use it. In Jupyter Notebook, we can do this by running !pip install [name of library].\nIf you don’t already have pip installed on your computer, follow these instructions. If you’re still having trouble, try Googling your specific questions or asking a chatbot for step-by-step instructions!\n\n!pip install pandas\n\nNext, let’s import the pandas library using the pd alias. An alias in Python is an alternate name for a library that’s typically shorter and easier to reference in the code later on.\n\nimport pandas as pd\n\npandas is probably the most common library for working with both big and small datasets in Python, and is the basis for working with more analytical packages (e.g. numpy, scipy, scikit-learn) and analyzing geographic data (e.g. geopandas). For each section, we’ll also link to relevant documentation for doing similar tasks in Microsoft Excel and Google Sheets.\nHere are download links to this notebook and example datasets.\n\nDownload Notebook\n\ncities.csv\n\ncapitals.csv\n\nnew_york_cities.csv\n\n\n\nA very common way of structuring and analyzing a dataset is in a 2-dimensional table format, where rows are individual records or observations, while columns are attributes (often called variables) about those observations. For example, rows could each be a city and columns could indicate the population for different time periods. Data stored in spreadsheets often take this format.\nIn pandas, a DataFrame is a tabular data structure similar to a spreadsheet, where data is organized in rows and columns. Columns can contain different kinds of data, such as numbers, strings, dates, and so on. When we load data in pandas, we typically load it into the structure of a DataFrame.\nLet’s first take a look at a small dataset, Canadian municipalities and their population in 2021 and 2016, based on Census data. In Statistics Canada lingo, these are called Census Subdivisions. This dataset only includes municipalities with a population greater than 25,000 in 2021.\nThe main method for loading csv data is to use the read_csv function, but pandas can also read and write many other data formats.\n\ndf = pd.read_csv(\"data/cities.csv\")\n\nGreat! Now our data is stored in the variable df in the structure of a DataFrame.\n\n\n\nIn spreadsheet software, when we open a data file, we will see the top rows of the data right away.\nIn pandas, we can simply type the name of the DataFrame, in this case df, in the cell to view it. By default, it will print the top and bottom rows in the DataFrame\n\ndf\n\nWe can specify which rows we want to view.\nLet’s explore what this data frame looks like. Adding the function .head(N) or .tail(N) prints the top or bottom N rows of the DataFrame. The following prints the first 10 rows.\nTry to edit this to print the bottom 10 rows or a different number of rows.\n\ndf.head(10)\n\nNotice that each column has a unique name. We can view the data of this column alone by using that name, and see what unique values exist using .unique().\nTry viewing the data of another column. Beware of upper and lower case – exact names matter!\n\ndf['Prov/terr'].head(10)  # Top 10 only\n\n\ndf['Prov/terr'].unique()  # Unique values for the *full* dataset - what happens if you do df['Prov/terr'].head(10).unique()?\n\n\n\n\nWe often want to look at only a portion of our data that fit some set of conditions (e.g. all cities in a province that have a population more than 100,000). This is often called filtering, querying, or subsetting a dataset.\nLet’s try to do some filtering in pandas with our data of Canadian cities. Check out these links for filtering and sorting in spreadsheet software:\n\nFiltering in Excel\nFiltering in Google Sheets\n\nWe can use the columns to identify data that we might want to filter by. The line below shows data only for Ontario, but see if you can filter for another province or territory.\n\ndf.loc[df['Prov/terr'] == 'Ont.']\n\nPandas allows us to use other similar mathematical concepts filter for data. Previously, we asked for all data in Ontario.\nNow, filter for all cities which had a population of at least 100,000 in 2021.\nHINT: in Python, “greater than or equals to” (i.e., “at least”) is represented using the syntax &gt;=.\nPandas also allows us to combine filtering conditions.\nUse the template below to select for all cities in Ontario with a population of over 100,000 in 2021.\n\ndf.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n\nNow let’s count how many cities actually meet these conditions. Run the line below to see how many cities there are in this data set in Ontario.\n\ndf.loc[df['Prov/terr'] == 'Ont.'].count()\n\nThe function .count() tells us how much data there is for each column - but if we wanted to just see one column, we could also filter for that individual column using df[COL_NAME].\nTry a different condition and count the amount of data for it.\n\n\n\nYou might have noticed that these cities are in alphabetical order - what if we wanted to see them in the order of population? In pandas, we do this using the sort_values function. The default is to sort in ascending order, so we set this to be False (i.e. descending) so the most populous cities are at the top.\n\ndf.sort_values(by='Population, 2021', ascending=False)\n\nLet’s put some in this together now.\nFilter the data to show all cities which are in the province of Quebec with at least a population of 50,000 in 2016, and then try to sort the cities by their 2016 population.\nHINT: You can do this in two steps (which is more readable) by storing the data that you filter into a variable called df_filtered, then running the command to sort the values on df_filtered.\n\n\n\nOnce we have processed and analyzed our data, we may want to save it for future use or share it with others. pandas makes it easy to export data to various formats, such as CSV or Excel files.\nBelow, we demonstrate how to export a DataFrame to both CSV and Excel formats. This is particularly useful for sharing results or viewing the data in other tools like spreadsheet software.\n\n# Save to CSV\ndf_filtered.to_csv('df_filtered.csv', index=False)\n\n# Save to Excel\ndf_filtered.to_excel('df_filtered.xlsx', index=False)\n\n\n\n\nOften, the data we have might not be in the condition want it to be in. Some data might be missing, and other data might have odd naming conventions.\nA simple example is that we might want all city names to be lowercase - which is what the code below does.\n\ndf['Name'] = df['Name'].str.lower()\ndf\n\npandas has a number of methods like str.lower() to alter data (see the full API). But the important thing to note here is that we directly modified the existing values of a column. We might not always want to do this, but often it is a good way of saving memory and shows that data frames are not just static forms but modifiable.\nLikewise, we might want better names for the columns we have. Take a look at the API for .rename() and modify the data frame df such that we rename the column Name to City. Pandas has more methods than we could ever remember - so learning to navigate the API is a crucial part of using the library.\nHINT: Take a look at the first example\n\n\n\nUnfortunately, it is pretty common that dataset we work with will be missing data values for some rows and columns. This can create complications when we want to produce summary statistics or visualizations. There are different strategies for dealing with this (i.e., imputing data), but the easiest is just to remove them. But first come out let’s check how much data is missing.\n\ndf.isnull().sum()\n\nIt seems that each column has a couple of data points missing. Let’s take a look at which rows these occur in. Similar to how we created a condition to filter for certain data, the code below creates a condition to filter for rows with missing data.\n\ndf.loc[df.isnull().any(axis=1)]\n\nYou can see that some rows are missing multiple values, While others are just missing one. We can remove rows which have missing data using the function dropna and assign it to df so we’re working with complete data only going forward. Try to modify the code below to drop rows whose empty values are in one of the two population columns - that is, if the name or province is missing, we want to keep that row still. Look at the API to figure this out, specifically the argument subset for .dropna()\n\ndf = df.dropna()\n\n\ndf.loc[df.isnull().any(axis=1)]\n\nGreat. Now let’s reset to our original data frame and exclude any rows with missing values.\n\ndf = pd.read_csv(\"data/cities.csv\")\ndf = df.dropna()\n\nInstead of dropping (i.e. removing) rows with missing values, we can instead replace them with specific values with fillna. For example, df.fillna(0) would replace all missing data values with a 0. This wouldn’t make sense in our data of Canadian cities, but can be useful in other contexts.\nSimilarly we can promgramatically find and replace data in any other column or across our dataset. For example, if we wanted to rename 'Y.T.' to 'Yukon' we would run the replace function as so df = df.replace('Y.T.', 'Yukon')\n\n\nWe can add or delete columns as needed. Let’s first add a column which shows the change in population between 2021 and 2016 and then sort by the cities that lost the most people. We can calculate that via a simple subtraction as follows.\n\ndf[\"pop_change\"] = df[\"Population, 2021\"] - df[\"Population, 2016\"]\ndf.sort_values(\"pop_change\", ascending = False).head(5)\n\nPandas supports mathematical equations between columns, just like the subtraction we did above. Create a new column called pct_pop_change that computes the percentage change in population between 2016 and 2021, and sort by the cities with the greatest increase.\nHINT: the way to compute percent change is 100 * (Y - X) / X.\nNow let’s clear these new columns out using drop to return to what we had originally.\n\ndf = df.drop(columns=['pop_change', 'pct_pop_change'])\n\n\n\n\nMuch of the time, we are working with multiple sets of data which may overlap in key ways. Perhaps we want to include measures of income in cities, or look at voting patterns - this may require us to combine multiple data frames so we can analyze them.\nPandas methods to combine data frames are quite similar to that of database operations - if you’ve worked with SQL before, this will come very easily to you. There’s an extensive tutorial on this topic, but we’ll focus on simple cases of pd.concat() and pd.merge(). If you are curious about how to do this in spreadsheet software like Excel, check out this tutorial\nFirst, we can use pd.concat() to stack DataFrames vertically when new data has the same columns but additional rows (e.g., adding cities from another region). This is like adding new entries to a database table.\n\ndf_ny_cities = pd.read_csv(\"./data/new_york_cities.csv\")\ncombined = pd.concat([df, df_ny_cities], ignore_index=True)\nprint(\"Original rows:\", len(df), \"| After append:\", len(combined))\ndisplay(combined.tail(5))  # Show new rows\n\nSecond, we can use pd_merge() (or pd.concat(axis=1)) to combine DataFrames side-by-side when they share a key column (e.g., city names). Here, we’ll a column denoting whether the city is a provincial capital by matching city names.\nLet’s first load this data:\n\ndf_capitals = pd.read_csv('./data/capitals.csv')\ndf_capitals\n\n\ndf_with_capitals = pd.merge(\n    df,  # Original data\n    df_capitals,  # New data\n    on=\"Name\",  # The same column \n    how=\"left\"  # Keep all data from the \"left\", ie. original\n)\n\n# Set non-capitals to False\ndf_with_capitals[\"Is_Capital\"] = df_with_capitals[\"Is_Capital\"].astype('boolean').fillna(False)\n\n# Verify: Show capitals and counts\nprint(f\"Found {df_with_capitals['Is_Capital'].sum()} capitals:\")\ndf_with_capitals[df_with_capitals[\"Is_Capital\"]]\n\n\n\n\n\nThe data we have is only as good as we understand what’s going on. There’s some basic methods in pandas we can use to get an idea of what the data says.\nThe most basic function you can use to get an idea of what’s going on is .describe(). It shows you how much data there is, and a number of summary statistics.\nModify the code below to report summary statistics for cities in Quebec only.\n\ndf.describe()\n\nInstead of picking out an examining a subset of the data one by one, we can use the function .groupby(). Given a column name, it will group rows which have the same value. In the example below, that means grouping every row which has the same province name. We can then apply a function to this (or multiple functions using .agg()) to examine different aspects of the data.\n\n# Group by province and calculate total population\nprovince_pop = df.groupby('Prov/terr')['Population, 2021'].sum()\nprint(\"Total population by province:\")\nprovince_pop.sort_values(ascending=False)\n\n\n# Multiple aggregation statistics\nstats = df.groupby('Prov/terr')['Population, 2021'].agg(['count', 'mean', 'max', 'sum'])\nstats\n\nBelow, we’ve added a column which shows the percent growth for each city. Use .groupby('Prov/terr') and use the function .median() (just as we used .sum() above) to observe the median growth rate per province or territory. Make sure to use sort_values() and set ascending to False.\n\ndf['Percent_Growth'] = 100 * (df['Population, 2021'] - df['Population, 2016']) / df['Population, 2016'] \n\n\n\n\nA cross tabulation, also called a frequency table or a contingency table, is a table that shows the summarizes two categorical variables by displaying the number of occurrences in each pair of categories.\nLet’s show an example by counting the number of cities in each province by a categorization of city size.\nWe will need two tools to do this: - cut, which bins continuous numbers (like population) into categories (e.g., “Small”/“Medium”/“Large”), turning numbers into meaningful groups. - crosstab, which counts how often these groups appear together—like how many “Medium” cities exist per province—revealing patterns that raw numbers hide.\n\n# Create a size category column\ndf['Size'] = pd.cut(df['Population, 2021'],\n                    bins=[0, 50000, 200000, float('inf')],\n                    labels=['Small', 'Medium', 'Large'])\n\n# Cross-tab: Province vs. Size\nsize_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\nsize_table\n\nRecall that we just created the column 'Percent_Growth' as well. Use these two functions to create different bins for different levels of growth and cross tabulate them.\nIf you’ve worked with Excel or Google Sheets, this is very similar to doing a Pivot Table.\nPivot tables are able to summarize data across several categories, and for different types of summaries (e.g. sum, mean, median, etc.)\nThe pivot_table function in pandas to aggregate data, and has more options than the crosstab function. Both are quite similar though. Here’s an example where we are using pivot_table to sum the population in each province by the 3 size groups.\nTry to edit this to compute the mean or median city Percent_Growth\n\npd.pivot_table(data = df, values = ['Population, 2021'], index = ['Prov/terr'], columns = ['Size'], aggfunc = 'sum', observed=True)\n\nThere are often multiple ways to achieve similar results. In the previous section we looked at the groupby function to summarize data by one column, while above we used crosstab or pivot_table. However, we could also use the groupby function for this purpose. Check out the example below.\nYou should notice that it has created a long-table formate rather than a wide-table format. Both formats can be useful, wide-table formats are easier for viewing data for only 2 categories, while long-table formats are often used for inputting data into modelling or visualization libraries.\n\ndf.groupby(['Prov/terr', 'Size'], observed=False)['Population, 2021'].agg(['sum'])",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#tables-dataframes",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#tables-dataframes",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "A very common way of structuring and analyzing a dataset is in a 2-dimensional table format, where rows are individual records or observations, while columns are attributes (often called variables) about those observations. For example, rows could each be a city and columns could indicate the population for different time periods. Data stored in spreadsheets often take this format.\nIn pandas, a DataFrame is a tabular data structure similar to a spreadsheet, where data is organized in rows and columns. Columns can contain different kinds of data, such as numbers, strings, dates, and so on. When we load data in pandas, we typically load it into the structure of a DataFrame.\nLet’s first take a look at a small dataset, Canadian municipalities and their population in 2021 and 2016, based on Census data. In Statistics Canada lingo, these are called Census Subdivisions. This dataset only includes municipalities with a population greater than 25,000 in 2021.\nThe main method for loading csv data is to use the read_csv function, but pandas can also read and write many other data formats.\n\ndf = pd.read_csv(\"data/cities.csv\")\n\nGreat! Now our data is stored in the variable df in the structure of a DataFrame.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#viewing-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#viewing-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "In spreadsheet software, when we open a data file, we will see the top rows of the data right away.\nIn pandas, we can simply type the name of the DataFrame, in this case df, in the cell to view it. By default, it will print the top and bottom rows in the DataFrame\n\ndf\n\nWe can specify which rows we want to view.\nLet’s explore what this data frame looks like. Adding the function .head(N) or .tail(N) prints the top or bottom N rows of the DataFrame. The following prints the first 10 rows.\nTry to edit this to print the bottom 10 rows or a different number of rows.\n\ndf.head(10)\n\nNotice that each column has a unique name. We can view the data of this column alone by using that name, and see what unique values exist using .unique().\nTry viewing the data of another column. Beware of upper and lower case – exact names matter!\n\ndf['Prov/terr'].head(10)  # Top 10 only\n\n\ndf['Prov/terr'].unique()  # Unique values for the *full* dataset - what happens if you do df['Prov/terr'].head(10).unique()?",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#filtering-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#filtering-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "We often want to look at only a portion of our data that fit some set of conditions (e.g. all cities in a province that have a population more than 100,000). This is often called filtering, querying, or subsetting a dataset.\nLet’s try to do some filtering in pandas with our data of Canadian cities. Check out these links for filtering and sorting in spreadsheet software:\n\nFiltering in Excel\nFiltering in Google Sheets\n\nWe can use the columns to identify data that we might want to filter by. The line below shows data only for Ontario, but see if you can filter for another province or territory.\n\ndf.loc[df['Prov/terr'] == 'Ont.']\n\nPandas allows us to use other similar mathematical concepts filter for data. Previously, we asked for all data in Ontario.\nNow, filter for all cities which had a population of at least 100,000 in 2021.\nHINT: in Python, “greater than or equals to” (i.e., “at least”) is represented using the syntax &gt;=.\nPandas also allows us to combine filtering conditions.\nUse the template below to select for all cities in Ontario with a population of over 100,000 in 2021.\n\ndf.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n\nNow let’s count how many cities actually meet these conditions. Run the line below to see how many cities there are in this data set in Ontario.\n\ndf.loc[df['Prov/terr'] == 'Ont.'].count()\n\nThe function .count() tells us how much data there is for each column - but if we wanted to just see one column, we could also filter for that individual column using df[COL_NAME].\nTry a different condition and count the amount of data for it.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#sorting-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#sorting-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "You might have noticed that these cities are in alphabetical order - what if we wanted to see them in the order of population? In pandas, we do this using the sort_values function. The default is to sort in ascending order, so we set this to be False (i.e. descending) so the most populous cities are at the top.\n\ndf.sort_values(by='Population, 2021', ascending=False)\n\nLet’s put some in this together now.\nFilter the data to show all cities which are in the province of Quebec with at least a population of 50,000 in 2016, and then try to sort the cities by their 2016 population.\nHINT: You can do this in two steps (which is more readable) by storing the data that you filter into a variable called df_filtered, then running the command to sort the values on df_filtered.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#exporting-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#exporting-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Once we have processed and analyzed our data, we may want to save it for future use or share it with others. pandas makes it easy to export data to various formats, such as CSV or Excel files.\nBelow, we demonstrate how to export a DataFrame to both CSV and Excel formats. This is particularly useful for sharing results or viewing the data in other tools like spreadsheet software.\n\n# Save to CSV\ndf_filtered.to_csv('df_filtered.csv', index=False)\n\n# Save to Excel\ndf_filtered.to_excel('df_filtered.xlsx', index=False)",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#updating-and-renaming-columns",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#updating-and-renaming-columns",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Often, the data we have might not be in the condition want it to be in. Some data might be missing, and other data might have odd naming conventions.\nA simple example is that we might want all city names to be lowercase - which is what the code below does.\n\ndf['Name'] = df['Name'].str.lower()\ndf\n\npandas has a number of methods like str.lower() to alter data (see the full API). But the important thing to note here is that we directly modified the existing values of a column. We might not always want to do this, but often it is a good way of saving memory and shows that data frames are not just static forms but modifiable.\nLikewise, we might want better names for the columns we have. Take a look at the API for .rename() and modify the data frame df such that we rename the column Name to City. Pandas has more methods than we could ever remember - so learning to navigate the API is a crucial part of using the library.\nHINT: Take a look at the first example",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#handling-missing-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#handling-missing-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Unfortunately, it is pretty common that dataset we work with will be missing data values for some rows and columns. This can create complications when we want to produce summary statistics or visualizations. There are different strategies for dealing with this (i.e., imputing data), but the easiest is just to remove them. But first come out let’s check how much data is missing.\n\ndf.isnull().sum()\n\nIt seems that each column has a couple of data points missing. Let’s take a look at which rows these occur in. Similar to how we created a condition to filter for certain data, the code below creates a condition to filter for rows with missing data.\n\ndf.loc[df.isnull().any(axis=1)]\n\nYou can see that some rows are missing multiple values, While others are just missing one. We can remove rows which have missing data using the function dropna and assign it to df so we’re working with complete data only going forward. Try to modify the code below to drop rows whose empty values are in one of the two population columns - that is, if the name or province is missing, we want to keep that row still. Look at the API to figure this out, specifically the argument subset for .dropna()\n\ndf = df.dropna()\n\n\ndf.loc[df.isnull().any(axis=1)]\n\nGreat. Now let’s reset to our original data frame and exclude any rows with missing values.\n\ndf = pd.read_csv(\"data/cities.csv\")\ndf = df.dropna()\n\nInstead of dropping (i.e. removing) rows with missing values, we can instead replace them with specific values with fillna. For example, df.fillna(0) would replace all missing data values with a 0. This wouldn’t make sense in our data of Canadian cities, but can be useful in other contexts.\nSimilarly we can promgramatically find and replace data in any other column or across our dataset. For example, if we wanted to rename 'Y.T.' to 'Yukon' we would run the replace function as so df = df.replace('Y.T.', 'Yukon')\n\n\nWe can add or delete columns as needed. Let’s first add a column which shows the change in population between 2021 and 2016 and then sort by the cities that lost the most people. We can calculate that via a simple subtraction as follows.\n\ndf[\"pop_change\"] = df[\"Population, 2021\"] - df[\"Population, 2016\"]\ndf.sort_values(\"pop_change\", ascending = False).head(5)\n\nPandas supports mathematical equations between columns, just like the subtraction we did above. Create a new column called pct_pop_change that computes the percentage change in population between 2016 and 2021, and sort by the cities with the greatest increase.\nHINT: the way to compute percent change is 100 * (Y - X) / X.\nNow let’s clear these new columns out using drop to return to what we had originally.\n\ndf = df.drop(columns=['pop_change', 'pct_pop_change'])\n\n\n\n\nMuch of the time, we are working with multiple sets of data which may overlap in key ways. Perhaps we want to include measures of income in cities, or look at voting patterns - this may require us to combine multiple data frames so we can analyze them.\nPandas methods to combine data frames are quite similar to that of database operations - if you’ve worked with SQL before, this will come very easily to you. There’s an extensive tutorial on this topic, but we’ll focus on simple cases of pd.concat() and pd.merge(). If you are curious about how to do this in spreadsheet software like Excel, check out this tutorial\nFirst, we can use pd.concat() to stack DataFrames vertically when new data has the same columns but additional rows (e.g., adding cities from another region). This is like adding new entries to a database table.\n\ndf_ny_cities = pd.read_csv(\"./data/new_york_cities.csv\")\ncombined = pd.concat([df, df_ny_cities], ignore_index=True)\nprint(\"Original rows:\", len(df), \"| After append:\", len(combined))\ndisplay(combined.tail(5))  # Show new rows\n\nSecond, we can use pd_merge() (or pd.concat(axis=1)) to combine DataFrames side-by-side when they share a key column (e.g., city names). Here, we’ll a column denoting whether the city is a provincial capital by matching city names.\nLet’s first load this data:\n\ndf_capitals = pd.read_csv('./data/capitals.csv')\ndf_capitals\n\n\ndf_with_capitals = pd.merge(\n    df,  # Original data\n    df_capitals,  # New data\n    on=\"Name\",  # The same column \n    how=\"left\"  # Keep all data from the \"left\", ie. original\n)\n\n# Set non-capitals to False\ndf_with_capitals[\"Is_Capital\"] = df_with_capitals[\"Is_Capital\"].astype('boolean').fillna(False)\n\n# Verify: Show capitals and counts\nprint(f\"Found {df_with_capitals['Is_Capital'].sum()} capitals:\")\ndf_with_capitals[df_with_capitals[\"Is_Capital\"]]",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#summary-statistics",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#summary-statistics",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "The data we have is only as good as we understand what’s going on. There’s some basic methods in pandas we can use to get an idea of what the data says.\nThe most basic function you can use to get an idea of what’s going on is .describe(). It shows you how much data there is, and a number of summary statistics.\nModify the code below to report summary statistics for cities in Quebec only.\n\ndf.describe()\n\nInstead of picking out an examining a subset of the data one by one, we can use the function .groupby(). Given a column name, it will group rows which have the same value. In the example below, that means grouping every row which has the same province name. We can then apply a function to this (or multiple functions using .agg()) to examine different aspects of the data.\n\n# Group by province and calculate total population\nprovince_pop = df.groupby('Prov/terr')['Population, 2021'].sum()\nprint(\"Total population by province:\")\nprovince_pop.sort_values(ascending=False)\n\n\n# Multiple aggregation statistics\nstats = df.groupby('Prov/terr')['Population, 2021'].agg(['count', 'mean', 'max', 'sum'])\nstats\n\nBelow, we’ve added a column which shows the percent growth for each city. Use .groupby('Prov/terr') and use the function .median() (just as we used .sum() above) to observe the median growth rate per province or territory. Make sure to use sort_values() and set ascending to False.\n\ndf['Percent_Growth'] = 100 * (df['Population, 2021'] - df['Population, 2016']) / df['Population, 2016']",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#cross-tabulations-and-pivot-tables",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#cross-tabulations-and-pivot-tables",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "A cross tabulation, also called a frequency table or a contingency table, is a table that shows the summarizes two categorical variables by displaying the number of occurrences in each pair of categories.\nLet’s show an example by counting the number of cities in each province by a categorization of city size.\nWe will need two tools to do this: - cut, which bins continuous numbers (like population) into categories (e.g., “Small”/“Medium”/“Large”), turning numbers into meaningful groups. - crosstab, which counts how often these groups appear together—like how many “Medium” cities exist per province—revealing patterns that raw numbers hide.\n\n# Create a size category column\ndf['Size'] = pd.cut(df['Population, 2021'],\n                    bins=[0, 50000, 200000, float('inf')],\n                    labels=['Small', 'Medium', 'Large'])\n\n# Cross-tab: Province vs. Size\nsize_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\nsize_table\n\nRecall that we just created the column 'Percent_Growth' as well. Use these two functions to create different bins for different levels of growth and cross tabulate them.\nIf you’ve worked with Excel or Google Sheets, this is very similar to doing a Pivot Table.\nPivot tables are able to summarize data across several categories, and for different types of summaries (e.g. sum, mean, median, etc.)\nThe pivot_table function in pandas to aggregate data, and has more options than the crosstab function. Both are quite similar though. Here’s an example where we are using pivot_table to sum the population in each province by the 3 size groups.\nTry to edit this to compute the mean or median city Percent_Growth\n\npd.pivot_table(data = df, values = ['Population, 2021'], index = ['Prov/terr'], columns = ['Size'], aggfunc = 'sum', observed=True)\n\nThere are often multiple ways to achieve similar results. In the previous section we looked at the groupby function to summarize data by one column, while above we used crosstab or pivot_table. However, we could also use the groupby function for this purpose. Check out the example below.\nYou should notice that it has created a long-table formate rather than a wide-table format. Both formats can be useful, wide-table formats are easier for viewing data for only 2 categories, while long-table formats are often used for inputting data into modelling or visualization libraries.\n\ndf.groupby(['Prov/terr', 'Size'], observed=False)['Population, 2021'].agg(['sum'])",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html",
    "href": "urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html",
    "title": "<img src='https://raw.githubusercontent.com/schoolofcities/gta-immigration/refs/heads/main/src/assets/top-logo-full.svg'  style='height:auto;width:220px'></img><br>Urban Data Analytics & Visualization 📊📈🏙️",
    "section": "",
    "text": "???"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "<img src='https://raw.githubusercontent.com/schoolofcities/gta-immigration/refs/heads/main/src/assets/top-logo-full.svg'  style='height:auto;width:220px'></img><br>Urban Data Analytics & Visualization 📊📈🏙️",
    "section": "",
    "text": "Hello I am the homepage yes yes"
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html",
    "title": "Spatial data & GIS",
    "section": "",
    "text": "A lot of urban datasets are directly linked to specific places, e.g. addresses, streets, neighbourhoods, political or administrative boundaries, etc.\nData that include place-based information are often called spatial, geographic, or geospatial data.\nGeographic Information Systems (GIS) are tools and software for analyzing, processing, and visualizing spatial data.\nThis page will cover the basics of spatial data and how we can view and interact with this data in the software QGIS (you will need to download QGIS to work on the hands-on part of this tutorial).\n\n\nA spatial dataset is a combination of…\n\nattribute data (the what)\nlocation data and spatial dimensions (the where)\n\nSpatial data, this combination of attribute and location data, can be organized and represented in a number of different formats.\nFor example, a city can be represented on a map via a single point with a label (e.g. based on latitude and longitude coordinates). Or a city can be represented as a polygon, based on on it’s administrative boundary\nImportantly, there are always uncertainty about the level of accuracy, precision, and resolution of spatial data. Spatial data are abstractions of reality, and thus have some loss of information when used for visualization and analysis. Any analysis can only be as good as the available data.\nThe two most common forms of spatial data are vector data and raster data.\n\n\n\nExamples of spatial data\n\n\n\n\nVector data uses geographic coordinates, or a series of coordinates, to create points, lines, and polygons representing real-world features.\ne.g. in the map below (a screenshot of OpenStreetMap) lines are used to represent roads and rail, points for retail, polygons for parks and buildings, etc.\n\n\n\nScreenshot of OpenStreetMap\n\n\nSpatial data can be stored as columns in traditional table-based formats (e.g. .csv), but there are also a wide range of data formats specific to spatial data. These are some of the most common:\n\nGeoJSON .geojson\nGeoPacakge .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nHere is an example of a single point location of a city stored in a .geojson file. .geojson is a standardized data schema .json format for spatial data.\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [-80.992, 46.490]\n            },\n            \"properties\": {\n                \"city_name\": \"Sudbury\"\n            }\n        }\n    ]\n}\n\n\n\nRaster data represents space as a continuous grid with equal cell sizes. Each cell contains a value pertaining to the type of feature it represents. These values can be quantitative (e.g. elevation) or categorical (e.g. type of land use). Common examples of raster data include Digital Elevation Models (DEMs), satellite imagery, and scanned images like historical maps.\ne.g. the map below shows a DEM for Toronto at two different scales.\n\n\n\nDEM of Toronto\n\n\n\n\n\n\nWe use Geographic Information Systems (GIS) to analyze, manipulate, and visualize spatial data.\nWhy is GIS useful?\n\nexplore spatial patterns and relationships in data\ncreate maps for publications\ngenerate new data, either “by hand” or via spatial relationships from other data (e.g. through spatial queries)\nperform spatial analysis (i.e. statistical methods applied to spatial data)\n\nGIS is often thought of as more than just a tool or piece of software. It can refer to all aspects of managing and analyzing digital spatially referenced data.\nThe power of GIS software and tools is the ability to work with data stored in different layers (e.g. a layer for roads, another for buildings, and so on) in conjunction with each other. These layers can be visualized and analyzed relative to each other based on their spatial relationships.\n\n\n\nSimple AI generated schematic of layers\n\n\nGIS software usually links to data stored elsewhere on a computer, rather than in a project file. If the source location of the data (i.e. which folder it’s in) changes, then this will have to be updated in the GIS project. If data are edited in GIS, it will update the data in its source location.\nThe open-source QGIS and the proprietary ESRI ArcGIS are the two most used desktop GIS software. ESRI’s suite of tools are often used by larger corporate and government organizations while QGIS is more used by small consultants and freelancers, non-profits, and academia. Many spatial data processing, visualization, and analyses steps also have equivalents in Python, R, SQL and other programming languages via specific libraries\nWe’ll be working with QGIS and Python, because they free and work across multiple operating systems, and are commonly used across different areas of work and research. But pretty much everything we’ll show can be accomplished across different software options, the buttons and steps will just be a bit different.\nLearning the core analytical and visualization concepts and ideas are much more important than exactly where to click or what specific functions to run.\n\n\n\nA CRS is a framework for describing the geographic position of location-based data. It tells your GIS software where your data are located on the Earth. Without a CRS, your data would just be a bunch of numbers without an ability to place them on a map and relate them to other data.\nThere are thousands of CRSs. Each CRS has specific attributes about its position and one main set of spatial units (e.g. a CRS will measure location in degrees longitude/latitude, metres, miles, etc.).\nProbably the most common CRS is WGS84, which uses longitude/latitude to link data to the Earth’s surface.\nWhen doing spatial analyses and processing where we are comparing two or more datasets to each other, it is important that they are in the same CRS.\n\n\nCRS are often paired with map projections. This is sometimes called a Projected CRS.\nThe earth is a 3D globe, but most maps are represented on 2D surfaces (e.g. on a screen, piece of paper). Map projections are mathematical model to flatten the earth to create a 2D view.\nEvery projection distorts shape, area, distance, or direction in some way or another. Different map projections distort the Earth in different ways. Here are some examples! For more check out this projection transition tool\n\n\n\nQGIS screenshot without data\n\n\nFor doing analysis and an urban scale, it is important that we choose a map projection that conserves area and distances in both X and Y directions (i.e. space is not being distorted in one particular direction more than others)\nFor example, these are two aerial images of Toronto, the left uses a local Mercator projection which does not distort data at a local scale, while the right is a rectangular projection where degrees latitude are equal shown at the same scale as degrees longitude.\nA general recommendation for urban-scale analyses is to use a Universal Transverse Mercator (UTM) projected CRS, which conserves area, distance, and direction along bands of the earth. This is a good tool for finding which UTM zone your region is in.\n\n\n\n\nLet’s use QGIS to quickly view a few spatial datasets. When you open up QGIS, it should look something like this:\n\n\n\nQGIS screenshot without data\n\n\nThe Browser panel is used for finding and loading data on your computer or from external sources, each line and symbol is for a different data source. The Layer panel will list all the datasets that are loaded into your project. The big white space will populate with data once it is loaded. The buttons at the top include a variety of options for loading data, saving your project, exploring your map, and various common processing and analytical tools.\n\n\nLet’s begin by adding in a basemap. Basemaps are often used to provide geographic reference for other data that we load into QGIS.\nWe’ll add a basemap of OpenStreetMap, a free crowd-sourced map of the world, available as XYZ raster tiles. To do this, go to Layer - Data Source Manager or simply click on the Data Source Manager button highlighted below. When you open the Data Source Manager you have options for a wide variety of data types to add to your map.\n\n\n\nData source manager location in QGIS\n\n\n\n\n\nAdding an XYZ tile\n\n\nHere is the URL to paste into QGIS for adding an OpenStreetMap basemap: https://tile.openstreetmap.org/{z}/{x}/{y}.png.\nWhen you click ‘Add’ it should be added to the map. You can use the navigation buttons or just scroll on your mouse to zoom to your city.\n\n\n\nQGIS with OpenStreetMap\n\n\nLet’s now add some vector data to the map. We’ve pre-downloaded two datasets from the City of Toronto’s Open Data Portal - City Wards (polygons) - Library Locations (points)\nThese can be added via the Data Source Manager - Vector, or simply via drag-and-drop from the folder of where they are located on your computer.\n\n\n\nQGIS with Toronto data\n\n\n\n\n\nThe data added to the project will be listed in the Layers panel. The order of the layers determine the order of which they stack on top of each other on the map. In the example above, the libraries are listed at the top of the other two layers, and it thus appears on top of the other two layers on the map.\nTo change the layer order, you can drag individual layers to be above or below any other layer in the Layers panel.\nMost vector data has associated attribute data, e.g. the number of a ward, the name of a library, etc. To view this data, right-click on a layer and then click on Open Attribute Table. In GIS, column names are often called “Fields”\n\n\n\nWhen you load vector data like this into QGIS, the layers have default styling (e.g. colours, sizes, line-widths, etc.).\nThere are tonnes of options in QGIS to change these initial styles. To do so, right-click on a layer, go to Properties, and then Symbology.\nThere are lots of options in here to change the colours, size, and symbols.\nFor example, in the screenshot below, the sizes of the libraries and the line-widths of the wards are increased a bit, the wards are given a transparent fill, and the library symbols are converted to squares.\n\n\n\nQGIS with Toronto data\n\n\nTip! you get extra options if you click on the sub-component of the symbol. e.g. in this example I clicked on Simple Fill to find additional styling options like pattern-based fill styles.\n\n\n\nQGIS symbology example\n\n\nWe’ll explore data-driven styling, where colours or other style options are based on data values, in a future tutorial.\n\n\n\nYou can change the project CRS in QGIS by going to Project - Properties - CRS.\n\n\n\nYou can export and create copies of any of your data layers. This may be useful if you want to change the file format, the CRS, subset the attribute table, or simply just make a copy of the data.\nTo do this, go to Export - Save Features As, and you can adjust the Format, CRS, and other options as needed.\n\n\n\nYou can save a project in QGIS by going to Project and then Save or Save As.\nIt will get saved to a .qgis file. These files save styling, filters, layer orders, map scale and location, and layout settings.\nHowever, .qgis files do not save datasets directly in the file, only paths and links to where the data is stored, either on your computer or from a URL like the OpenStreetMap basemap. Saving only links to data is common in other tools and software.\nTherefore, if you share your project, you’ll need to also share any local datasets it is linking to.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data & GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#spatial-data",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#spatial-data",
    "title": "Spatial data & GIS",
    "section": "",
    "text": "A spatial dataset is a combination of…\n\nattribute data (the what)\nlocation data and spatial dimensions (the where)\n\nSpatial data, this combination of attribute and location data, can be organized and represented in a number of different formats.\nFor example, a city can be represented on a map via a single point with a label (e.g. based on latitude and longitude coordinates). Or a city can be represented as a polygon, based on on it’s administrative boundary\nImportantly, there are always uncertainty about the level of accuracy, precision, and resolution of spatial data. Spatial data are abstractions of reality, and thus have some loss of information when used for visualization and analysis. Any analysis can only be as good as the available data.\nThe two most common forms of spatial data are vector data and raster data.\n\n\n\nExamples of spatial data\n\n\n\n\nVector data uses geographic coordinates, or a series of coordinates, to create points, lines, and polygons representing real-world features.\ne.g. in the map below (a screenshot of OpenStreetMap) lines are used to represent roads and rail, points for retail, polygons for parks and buildings, etc.\n\n\n\nScreenshot of OpenStreetMap\n\n\nSpatial data can be stored as columns in traditional table-based formats (e.g. .csv), but there are also a wide range of data formats specific to spatial data. These are some of the most common:\n\nGeoJSON .geojson\nGeoPacakge .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nHere is an example of a single point location of a city stored in a .geojson file. .geojson is a standardized data schema .json format for spatial data.\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [-80.992, 46.490]\n            },\n            \"properties\": {\n                \"city_name\": \"Sudbury\"\n            }\n        }\n    ]\n}\n\n\n\nRaster data represents space as a continuous grid with equal cell sizes. Each cell contains a value pertaining to the type of feature it represents. These values can be quantitative (e.g. elevation) or categorical (e.g. type of land use). Common examples of raster data include Digital Elevation Models (DEMs), satellite imagery, and scanned images like historical maps.\ne.g. the map below shows a DEM for Toronto at two different scales.\n\n\n\nDEM of Toronto",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data & GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#geographic-information-systems-gis",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#geographic-information-systems-gis",
    "title": "Spatial data & GIS",
    "section": "",
    "text": "We use Geographic Information Systems (GIS) to analyze, manipulate, and visualize spatial data.\nWhy is GIS useful?\n\nexplore spatial patterns and relationships in data\ncreate maps for publications\ngenerate new data, either “by hand” or via spatial relationships from other data (e.g. through spatial queries)\nperform spatial analysis (i.e. statistical methods applied to spatial data)\n\nGIS is often thought of as more than just a tool or piece of software. It can refer to all aspects of managing and analyzing digital spatially referenced data.\nThe power of GIS software and tools is the ability to work with data stored in different layers (e.g. a layer for roads, another for buildings, and so on) in conjunction with each other. These layers can be visualized and analyzed relative to each other based on their spatial relationships.\n\n\n\nSimple AI generated schematic of layers\n\n\nGIS software usually links to data stored elsewhere on a computer, rather than in a project file. If the source location of the data (i.e. which folder it’s in) changes, then this will have to be updated in the GIS project. If data are edited in GIS, it will update the data in its source location.\nThe open-source QGIS and the proprietary ESRI ArcGIS are the two most used desktop GIS software. ESRI’s suite of tools are often used by larger corporate and government organizations while QGIS is more used by small consultants and freelancers, non-profits, and academia. Many spatial data processing, visualization, and analyses steps also have equivalents in Python, R, SQL and other programming languages via specific libraries\nWe’ll be working with QGIS and Python, because they free and work across multiple operating systems, and are commonly used across different areas of work and research. But pretty much everything we’ll show can be accomplished across different software options, the buttons and steps will just be a bit different.\nLearning the core analytical and visualization concepts and ideas are much more important than exactly where to click or what specific functions to run.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data & GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#coordinate-reference-systems-crs-and-map-projections",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#coordinate-reference-systems-crs-and-map-projections",
    "title": "Spatial data & GIS",
    "section": "",
    "text": "A CRS is a framework for describing the geographic position of location-based data. It tells your GIS software where your data are located on the Earth. Without a CRS, your data would just be a bunch of numbers without an ability to place them on a map and relate them to other data.\nThere are thousands of CRSs. Each CRS has specific attributes about its position and one main set of spatial units (e.g. a CRS will measure location in degrees longitude/latitude, metres, miles, etc.).\nProbably the most common CRS is WGS84, which uses longitude/latitude to link data to the Earth’s surface.\nWhen doing spatial analyses and processing where we are comparing two or more datasets to each other, it is important that they are in the same CRS.\n\n\nCRS are often paired with map projections. This is sometimes called a Projected CRS.\nThe earth is a 3D globe, but most maps are represented on 2D surfaces (e.g. on a screen, piece of paper). Map projections are mathematical model to flatten the earth to create a 2D view.\nEvery projection distorts shape, area, distance, or direction in some way or another. Different map projections distort the Earth in different ways. Here are some examples! For more check out this projection transition tool\n\n\n\nQGIS screenshot without data\n\n\nFor doing analysis and an urban scale, it is important that we choose a map projection that conserves area and distances in both X and Y directions (i.e. space is not being distorted in one particular direction more than others)\nFor example, these are two aerial images of Toronto, the left uses a local Mercator projection which does not distort data at a local scale, while the right is a rectangular projection where degrees latitude are equal shown at the same scale as degrees longitude.\nA general recommendation for urban-scale analyses is to use a Universal Transverse Mercator (UTM) projected CRS, which conserves area, distance, and direction along bands of the earth. This is a good tool for finding which UTM zone your region is in.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data & GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#working-with-spatial-data-in-qgis",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#working-with-spatial-data-in-qgis",
    "title": "Spatial data & GIS",
    "section": "",
    "text": "Let’s use QGIS to quickly view a few spatial datasets. When you open up QGIS, it should look something like this:\n\n\n\nQGIS screenshot without data\n\n\nThe Browser panel is used for finding and loading data on your computer or from external sources, each line and symbol is for a different data source. The Layer panel will list all the datasets that are loaded into your project. The big white space will populate with data once it is loaded. The buttons at the top include a variety of options for loading data, saving your project, exploring your map, and various common processing and analytical tools.\n\n\nLet’s begin by adding in a basemap. Basemaps are often used to provide geographic reference for other data that we load into QGIS.\nWe’ll add a basemap of OpenStreetMap, a free crowd-sourced map of the world, available as XYZ raster tiles. To do this, go to Layer - Data Source Manager or simply click on the Data Source Manager button highlighted below. When you open the Data Source Manager you have options for a wide variety of data types to add to your map.\n\n\n\nData source manager location in QGIS\n\n\n\n\n\nAdding an XYZ tile\n\n\nHere is the URL to paste into QGIS for adding an OpenStreetMap basemap: https://tile.openstreetmap.org/{z}/{x}/{y}.png.\nWhen you click ‘Add’ it should be added to the map. You can use the navigation buttons or just scroll on your mouse to zoom to your city.\n\n\n\nQGIS with OpenStreetMap\n\n\nLet’s now add some vector data to the map. We’ve pre-downloaded two datasets from the City of Toronto’s Open Data Portal - City Wards (polygons) - Library Locations (points)\nThese can be added via the Data Source Manager - Vector, or simply via drag-and-drop from the folder of where they are located on your computer.\n\n\n\nQGIS with Toronto data\n\n\n\n\n\nThe data added to the project will be listed in the Layers panel. The order of the layers determine the order of which they stack on top of each other on the map. In the example above, the libraries are listed at the top of the other two layers, and it thus appears on top of the other two layers on the map.\nTo change the layer order, you can drag individual layers to be above or below any other layer in the Layers panel.\nMost vector data has associated attribute data, e.g. the number of a ward, the name of a library, etc. To view this data, right-click on a layer and then click on Open Attribute Table. In GIS, column names are often called “Fields”\n\n\n\nWhen you load vector data like this into QGIS, the layers have default styling (e.g. colours, sizes, line-widths, etc.).\nThere are tonnes of options in QGIS to change these initial styles. To do so, right-click on a layer, go to Properties, and then Symbology.\nThere are lots of options in here to change the colours, size, and symbols.\nFor example, in the screenshot below, the sizes of the libraries and the line-widths of the wards are increased a bit, the wards are given a transparent fill, and the library symbols are converted to squares.\n\n\n\nQGIS with Toronto data\n\n\nTip! you get extra options if you click on the sub-component of the symbol. e.g. in this example I clicked on Simple Fill to find additional styling options like pattern-based fill styles.\n\n\n\nQGIS symbology example\n\n\nWe’ll explore data-driven styling, where colours or other style options are based on data values, in a future tutorial.\n\n\n\nYou can change the project CRS in QGIS by going to Project - Properties - CRS.\n\n\n\nYou can export and create copies of any of your data layers. This may be useful if you want to change the file format, the CRS, subset the attribute table, or simply just make a copy of the data.\nTo do this, go to Export - Save Features As, and you can adjust the Format, CRS, and other options as needed.\n\n\n\nYou can save a project in QGIS by going to Project and then Save or Save As.\nIt will get saved to a .qgis file. These files save styling, filters, layer orders, map scale and location, and layout settings.\nHowever, .qgis files do not save datasets directly in the file, only paths and links to where the data is stored, either on your computer or from a URL like the OpenStreetMap basemap. Saving only links to data is common in other tools and software.\nTherefore, if you share your project, you’ll need to also share any local datasets it is linking to.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data & GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data.html",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data.html",
    "title": "<img src='https://raw.githubusercontent.com/schoolofcities/gta-immigration/refs/heads/main/src/assets/top-logo-full.svg'  style='height:auto;width:220px'></img><br>Urban Data Analytics & Visualization 📊📈🏙️",
    "section": "",
    "text": "???"
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html",
    "title": "What and where of data",
    "section": "",
    "text": "Both the United States Census Bureau and Statistics Canada collect a combination of survey data and administrative data. Survey data, like the decennial census or American Community Survey in the United States, involves asking people for information about themselves or another topic. Administrative data, like tax records, are collected by governments or other entities for the purpose of running programs or providing services.\nAnother type of data that is often used in urban analyses comes from remote sensing, which involves measuring the physical characteristics of an area from a distance (USGS). Spatial data collected via remote sensing is often used in analyses of the built or natural environment (e.g., land cover classification or urban growth). One example of this type of data is the satellite imagery layer in Google Maps:\n\n\n\nSatellite imagery of Ottawa\n\n\nCrowdsourced data, which is collected from the public - usually online or via social media or apps - often helps to fill information gaps left by official data sources. For example, FixMyStreet allows residents in the UK to report problems like graffiti or potholes in their neighborhood and sends the information directly to local authorities. OpenStreetMap is a popular crowdsourced mapping dataset with billions of data points representing buildings, streets, transit lines, and much more.\n\n\n\nOpenStreetMap in Vancouver\n\n\nWeb scraping, or extracting information from the internet, is another method for creating datasets. Since the data do not already exist and must be created, this can be more time-intensive than using existing datasets. However, packages like beautifulsoup or selenium in Python make this process easier.\n\n\n\nThe table below lists a handful of websites where you can find freely available data about cities, the environment, land use, transportation, Indigenous communities, housing and homelessness. This is a non-exhaustive list; there are many other great data sources available. Note that municipalities’ open data portals typically contain information on all or most of these topics. For more information about census data specifically, see Canadian census data.\n\n\n\nTopic\nData sources\n\n\n\n\nCities\n- Open data portals (e.g., Toronto, Montreal, & Vancouver)\n\n\nEnvironment\n- NASA’s Earth Science Data Systems (ESDS) Program- The Canadian Urban Environmental Health Research Consortium- Natural Resources Canada- Environment and Climate Change Canada\n\n\nLand use\n- OpenStreetMap- Land cover of Canada- Municipal-level zoning maps (e.g., in Toronto)\n\n\nTransportation\n- Metrolinx Open Data for the Greater Toronto Area- Canadian Urban Transit Association- Mobilizing Justice Hub- Transitland\n\n\nIndigenous communities\n- First Nations Data Centre- Native Land Digital\n\n\nHousing and homelessness\n- Housing - Statistics Canada- Housing data from Canada Mortgage and Housing Corporation\n\n\n\nIn addition to free, open data like the datasets in the above table, there are also proprietary datasets that are useful for analyzing urban phenomena require payment to access. For example, cell phone mobility data from Spectus can be used to measure post-pandemic downtown recovery trends, real estate data from Costar can be used to assess vacancy rates or rent prices, and consumer data from Data Axle can be used to study the impact of new housing on migration patterns.\n\n\n\nData format refers to how data is stored and structured. In practice, this is most relevant when loading and saving data. The data format you choose to use depends on the data’s size, structure, use, how it is being stored, and whether it is spatial (has a geometry column) or not.\nSome of the most common data formats for non-spatial data are:\n\nCSV (comma separated values) .csv\nJSON (JavaScript Object Notation) .json\nXML (Extensible Markup Language) .xml\n\nSome of the most common data formats for spatial data are (see Spatial data & GIS for more information):\n\nGeoJSON .geojson\nGeoPackage .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nWhile the file formats above suffice for relatively small or simple datasets, very large or complex datasets require more efficient storage via formats like Parquet (see instructions for Python). Relational databases are another commonly used data storage format for “big data” because they are more efficient, faster to query, more secure, and can be accessed by multiple users.\n\n\n\nIt’s important to make sure that each variable in your dataset is in the right format so the computer interpets it correctly. For example, if you load a CSV file with a column representing the population of a census tract, you want to make sure this variable is interpreted as a number and not a string of characters. See the table below for a list of common data types, and see this link for a more detailed explanation of each one.\n\n\n\nTable of common data types (source)\n\n\nWhen coding in Python, you’ll also need to get acquainted with Python’s built-in data types, some of which are explained here - but don’t worry about this yet; we’ll cover this in more detail later.\n\n\n\nWhen choosing which software to use to analyze or visualize data, one of the main considerations is whether the software is open source or proprietary. Open source software has its source code publicly available and can be modified by anyone on the internet. Proprietary software’s source code is not publicly available, is typically developed and updated by a closed group, and is licensed to users in exchange for payment. Read this link if you’re interested in learning more about the difference between the two.\nIn this course, we will focus on open source software because it is free and available to everyone. Below is a list of some of the main open source programming languages, software, and tools that we recommend using for data analysis and mapping.\n\n\n\nPurpose\nSoftware/Tools\n\n\n\n\nMapping\nQGIS, Python, R\n\n\nAnalyzing / visualizing data\nPython, R, SQL\n\n\nGraphic design / layouts\nInkscape, GIMP\n\n\nWeb development\nHTML, CSS, Javascript, Svelte\n\n\nWeb-based maps/visualization\nD3, MapLibre, PMtiles\n\n\nHosting / project management\nGitHub\n\n\n\n\n\n\nWhile is no set of specific step-by-step instructions for data analysis – each project requires unique data sources, variables, methodologies, and outputs – there is a general framework you should follow:\n\nDefine the problem or research question. What question are you trying to answer with data? Is data analysis the best way to answer that question? Who is the audience for your data analysis, and what do they want to know?\nCollect data. What kind of data do you need to answer your research question, and where can you find it? Does it exist? In what format?\nClean data. Make sure the data has appropriate variable names, does not have misspellings or other errors, and the variables are the correct data types. Get rid of any redundant or irrelevant data that you don’t need, and determine a method for dealing with any missing values.\nAnalyze data. Start by exploring the data to understand its structure and any statistical patterns. Then perform your analysis – for example, are you trying to uncover trends, or measure the relationship among variables?\nVisualize data. Create plots, maps, or other visual representations that illustrate the structure, trends, or relationships present in your data.\nPresent data. Clearly communicate your results to your intended audience. This could involve writing a report, or creating a presentation or interactive dashboard. Whatever gets your message across!\n\n\n\n\nLearning new software for data analysis or mapping can be confusing and frustrating. Luckily, there are a lot of great resources that can help!\nThe first place you should look when you’re confused about how to do something is the official documentation. For example, if you’re having trouble loading a CSV file in Python using the pandas package, take a look at this page on the pandas website. If you’re not sure how to create a spatial buffer in QGIS, check out the QGIS Buffer operations page.\nIf you’re still stuck on a question, Google it! Chances are, someone else has dealt with a similar issue, and there is likely a community of people helping them out. For example, one of the most popular resources for coding is Stack Overflow, a website where programmers ask and answer questions. Responses with the most votes are shown at the top of the page, making it easy to find helpful code snippets and explanations that you can adapt for your own needs. The website is so widely used that Stack Overflow posts will usually show up towards the top when you Google search coding questions.\nThere are also websites like W3Schools and GeeksforGeeks that offer online courses and tutorials covering everything from sorting a list in Python to building complicated statistical models. These websites show up often as results for relevant Google searches.\nFinally, a word of caution about using AI chatbots like ChatGPT - while these chatbots can be extremely helpful for debugging code or providing instructions about how to do a spatial join in QGIS, be careful. They are often wrong, and sometimes make up packages or functions that don’t exist. Also, if you use chatbots for coding help, make sure you understand what they are telling you. Asking for guidance or hints about specific, discrete questions is much better than asking the chatbot to write an entire Python script for you. The more you rely on the chatbots, the less you will learn, and the less you will be able to do on your own. Learning coding in particular can feel like an uphill battle, but if you start with a solid foundation and thorough understanding of how it works, you will feel much more confident in your ability to tackle more complicated problems later on.\nAlong the same lines, there are tools like Jupyter AI that offer AI assistance integrated into your coding environment. While these can be very useful and help you code more efficiently, remember that the more you rely on tools like this early on in your coding journey, the more dependent you will be on them and the less you will learn.",
    "crumbs": [
      "Urban Data Analytics",
      "What and where of data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#how-is-urban-data-created",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#how-is-urban-data-created",
    "title": "What and where of data",
    "section": "",
    "text": "Data can be collected via surveys, like the census, that people fill out with information about themselves. Some spatial data, like satellite imagery, comes from remote sensing"
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#where-does-data-come-from",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#where-does-data-come-from",
    "title": "What and where of data",
    "section": "",
    "text": "Both the United States Census Bureau and Statistics Canada collect a combination of survey data and administrative data. Survey data, like the decennial census or American Community Survey in the United States, involves asking people for information about themselves or another topic. Administrative data, like tax records, are collected by governments or other entities for the purpose of running programs or providing services.\nAnother type of data that is often used in urban analyses comes from remote sensing, which involves measuring the physical characteristics of an area from a distance (USGS). Spatial data collected via remote sensing is often used in analyses of the built or natural environment (e.g., land cover classification or urban growth). One example of this type of data is the satellite imagery layer in Google Maps:\n\n\n\nSatellite imagery of Ottawa\n\n\nCrowdsourced data, which is collected from the public - usually online or via social media or apps - often helps to fill information gaps left by official data sources. For example, FixMyStreet allows residents in the UK to report problems like graffiti or potholes in their neighborhood and sends the information directly to local authorities. OpenStreetMap is a popular crowdsourced mapping dataset with billions of data points representing buildings, streets, transit lines, and much more.\n\n\n\nOpenStreetMap in Vancouver\n\n\nWeb scraping, or extracting information from the internet, is another method for creating datasets. Since the data do not already exist and must be created, this can be more time-intensive than using existing datasets. However, packages like beautifulsoup or selenium in Python make this process easier.",
    "crumbs": [
      "Urban Data Analytics",
      "What and where of data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#common-data-sources",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#common-data-sources",
    "title": "What and where of data",
    "section": "",
    "text": "The table below lists a handful of websites where you can find data about the environment, cities, land use, transportation, Indigenous communities, housing and homelessness. For more information about census data specifically, see (LINK TO ‘canadian-census-data’ PAGE?).\n\n\n\nTopic\nCommon data sources\n\n\n\n\nEnvironmental\n- NASA’s Earth Science Data Systems (ESDS) Program- The Canadian Urban Environmental Health Research Consortium- Natural Resources Canada- Environment and Climate Change Canada\n\n\nCities\n- Open data portals (e.g., Toronto, Montreal, & Vancouver)"
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#common-data-sources-for-urban-analysis",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#common-data-sources-for-urban-analysis",
    "title": "What and where of data",
    "section": "",
    "text": "The table below lists a handful of websites where you can find data about cities, the environment, land use, transportation, Indigenous communities, housing and homelessness. Note that municipalities’ open data portals typically contain information on all or most of these topics. For more information about census data specifically, see (LINK TO ‘canadian-census-data’ PAGE?).\n\n\n\nTopic\nCommon data sources\n\n\n\n\nCities\n- Open data portals (e.g., Toronto, Montreal, & Vancouver)\n\n\nEnvironmental\n- NASA’s Earth Science Data Systems (ESDS) Program- The Canadian Urban Environmental Health Research Consortium- Natural Resources Canada- Environment and Climate Change Canada\n\n\nLand use\n- OpenStreetMap- Land cover of Canada- Municipal-level zoning maps (e.g., in Toronto)\n\n\nTransportation\n- Metrolinx Open Data for the Greater Toronto Area- {Canadian Urban Transit Association}()- Mobilizing Justice Hub- Transitland"
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#data-sources-for-urban-analysis",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#data-sources-for-urban-analysis",
    "title": "What and where of data",
    "section": "",
    "text": "The table below lists a handful of websites where you can find freely available data about cities, the environment, land use, transportation, Indigenous communities, housing and homelessness. This is a non-exhaustive list; there are many other great data sources available. Note that municipalities’ open data portals typically contain information on all or most of these topics. For more information about census data specifically, see Canadian census data.\n\n\n\nTopic\nData sources\n\n\n\n\nCities\n- Open data portals (e.g., Toronto, Montreal, & Vancouver)\n\n\nEnvironment\n- NASA’s Earth Science Data Systems (ESDS) Program- The Canadian Urban Environmental Health Research Consortium- Natural Resources Canada- Environment and Climate Change Canada\n\n\nLand use\n- OpenStreetMap- Land cover of Canada- Municipal-level zoning maps (e.g., in Toronto)\n\n\nTransportation\n- Metrolinx Open Data for the Greater Toronto Area- Canadian Urban Transit Association- Mobilizing Justice Hub- Transitland\n\n\nIndigenous communities\n- First Nations Data Centre- Native Land Digital\n\n\nHousing and homelessness\n- Housing - Statistics Canada- Housing data from Canada Mortgage and Housing Corporation\n\n\n\nIn addition to free, open data like the datasets in the above table, there are also proprietary datasets that are useful for analyzing urban phenomena require payment to access. For example, cell phone mobility data from Spectus can be used to measure post-pandemic downtown recovery trends, real estate data from Costar can be used to assess vacancy rates or rent prices, and consumer data from Data Axle can be used to study the impact of new housing on migration patterns.",
    "crumbs": [
      "Urban Data Analytics",
      "What and where of data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#data-formats",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#data-formats",
    "title": "What and where of data",
    "section": "",
    "text": "Data format refers to how data is stored and structured. In practice, this is most relevant when loading and saving data. The data format you choose to use depends on the data’s size, structure, use, how it is being stored, and whether it is spatial (has a geometry column) or not.\nSome of the most common data formats for non-spatial data are:\n\nCSV (comma separated values) .csv\nJSON (JavaScript Object Notation) .json\nXML (Extensible Markup Language) .xml\n\nSome of the most common data formats for spatial data are (see Spatial data & GIS for more information):\n\nGeoJSON .geojson\nGeoPackage .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nWhile the file formats above suffice for relatively small or simple datasets, very large or complex datasets require more efficient storage via formats like Parquet (see instructions for Python). Relational databases are another commonly used data storage format for “big data” because they are more efficient, faster to query, more secure, and can be accessed by multiple users.",
    "crumbs": [
      "Urban Data Analytics",
      "What and where of data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#data-types",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#data-types",
    "title": "What and where of data",
    "section": "",
    "text": "It’s important to make sure that each variable in your dataset is in the right format so the computer interpets it correctly. For example, if you load a CSV file with a column representing the population of a census tract, you want to make sure this variable is interpreted as a number and not a string of characters. See the table below for a list of common data types, and see this link for a more detailed explanation of each one.\n\n\n\nTable of common data types (source)\n\n\nWhen coding in Python, you’ll also need to get acquainted with Python’s built-in data types, some of which are explained here - but don’t worry about this yet; we’ll cover this in more detail later.",
    "crumbs": [
      "Urban Data Analytics",
      "What and where of data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#software-and-coding-tools",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#software-and-coding-tools",
    "title": "What and where of data",
    "section": "",
    "text": "When choosing which software to use to analyze or visualize data, one of the main considerations is whether the software is open source or proprietary. Open source software has its source code publicly available and can be modified by anyone on the internet. Proprietary software’s source code is not publicly available, is typically developed and updated by a closed group, and is licensed to users in exchange for payment. Read this link if you’re interested in learning more about the difference between the two.\nIn this course, we will focus on open source software because it is free and available to everyone. Below is a list of some of the main open source programming languages, software, and tools that we recommend using for data analysis and mapping.\n\n\n\nPurpose\nSoftware/Tools\n\n\n\n\nMapping\nQGIS, Python, R\n\n\nAnalyzing / visualizing data\nPython, R, SQL\n\n\nGraphic design / layouts\nInkscape, GIMP\n\n\nWeb development\nHTML, CSS, Javascript, Svelte\n\n\nWeb-based maps/visualization\nD3, MapLibre, PMtiles\n\n\nHosting / project management\nGitHub",
    "crumbs": [
      "Urban Data Analytics",
      "What and where of data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#software-and-tools",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#software-and-tools",
    "title": "What and where of data",
    "section": "",
    "text": "When choosing which software to use to analyze or visualize data, one of the main considerations is whether the software is open source or proprietary. Open source software has its source code publicly available and can be modified by anyone on the internet. Proprietary software’s source code is not publicly available, is typically developed and updated by a closed group, and is licensed to users in exchange for payment. Read this link if you’re interested in learning more about the difference between the two.\nIn this course, we will focus on open source software because it is free and available to everyone. Below is a list of some of the main open source programming languages, software, and tools that we recommend using for data analysis and mapping.\n\n\n\nPurpose\nSoftware/Tools\n\n\n\n\nMapping\nQGIS, Python, R\n\n\nAnalyzing / visualizing data\nPython, R, SQL\n\n\nGraphic design / layouts\nInkscape, GIMP\n\n\nWeb development\nHTML, CSS, Javascript, Svelte\n\n\nWeb-based maps/visualization\nD3, MapLibre, PMtiles\n\n\nHosting / project management\nGitHub",
    "crumbs": [
      "Urban Data Analytics",
      "What and where of data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#software-and-tools-for-data-analysis-and-visualization",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#software-and-tools-for-data-analysis-and-visualization",
    "title": "What and where of data",
    "section": "",
    "text": "When choosing which software to use to analyze or visualize data, one of the main considerations is whether the software is open source or proprietary. Open source software has its source code publicly available and can be modified by anyone on the internet. Proprietary software’s source code is not publicly available, is typically developed and updated by a closed group, and is licensed to users in exchange for payment. Read this link if you’re interested in learning more about the difference between the two.\nIn this course, we will focus on open source software because it is free and available to everyone. Below is a list of some of the main open source programming languages, software, and tools that we recommend using for data analysis and mapping.\n\n\n\nPurpose\nSoftware/Tools\n\n\n\n\nMapping\nQGIS, Python, R\n\n\nAnalyzing / visualizing data\nPython, R, SQL\n\n\nGraphic design / layouts\nInkscape, GIMP\n\n\nWeb development\nHTML, CSS, Javascript, Svelte\n\n\nWeb-based maps/visualization\nD3, MapLibre, PMtiles\n\n\nHosting / project management\nGitHub",
    "crumbs": [
      "Urban Data Analytics",
      "What and where of data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#data-analysis-process",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#data-analysis-process",
    "title": "What and where of data",
    "section": "",
    "text": "While is no set of specific step-by-step instructions for data analysis – each project requires unique data sources, variables, methodologies, and outputs – there is a general framework you should follow:\n\nDefine the problem or research question. What question are you trying to answer with data? Is data analysis the best way to answer that question? Who is the audience for your data analysis, and what do they want to know?\nCollect data. What kind of data do you need to answer your research question, and where can you find it? Does it exist? In what format?\nClean data. Make sure the data has appropriate variable names, does not have misspellings or other errors, and the variables are the correct data types. Get rid of any redundant or irrelevant data that you don’t need, and determine a method for dealing with any missing values.\nAnalyze data. Start by exploring the data to understand its structure and any statistical patterns. Then perform your analysis – for example, are you trying to uncover trends, or measure the relationship among variables?\nVisualize data. Create plots, maps, or other visual representations that illustrate the structure, trends, or relationships present in your data.\nPresent data. Clearly communicate your results to your intended audience. This could involve writing a report, or creating a presentation or interactive dashboard. Whatever gets your message across!",
    "crumbs": [
      "Urban Data Analytics",
      "What and where of data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#getting-help",
    "href": "notebooks/urban-data-analytics/1-what-and-where-of-data/what-and-where-of-data.html#getting-help",
    "title": "What and where of data",
    "section": "",
    "text": "Learning new software for data analysis or mapping can be confusing and frustrating. Luckily, there are a lot of great resources that can help!\nThe first place you should look when you’re confused about how to do something is the official documentation. For example, if you’re having trouble loading a CSV file in Python using the pandas package, take a look at this page on the pandas website. If you’re not sure how to create a spatial buffer in QGIS, check out the QGIS Buffer operations page.\nIf you’re still stuck on a question, Google it! Chances are, someone else has dealt with a similar issue, and there is likely a community of people helping them out. For example, one of the most popular resources for coding is Stack Overflow, a website where programmers ask and answer questions. Responses with the most votes are shown at the top of the page, making it easy to find helpful code snippets and explanations that you can adapt for your own needs. The website is so widely used that Stack Overflow posts will usually show up towards the top when you Google search coding questions.\nThere are also websites like W3Schools and GeeksforGeeks that offer online courses and tutorials covering everything from sorting a list in Python to building complicated statistical models. These websites show up often as results for relevant Google searches.\nFinally, a word of caution about using AI chatbots like ChatGPT - while these chatbots can be extremely helpful for debugging code or providing instructions about how to do a spatial join in QGIS, be careful. They are often wrong, and sometimes make up packages or functions that don’t exist. Also, if you use chatbots for coding help, make sure you understand what they are telling you. Asking for guidance or hints about specific, discrete questions is much better than asking the chatbot to write an entire Python script for you. The more you rely on the chatbots, the less you will learn, and the less you will be able to do on your own. Learning coding in particular can feel like an uphill battle, but if you start with a solid foundation and thorough understanding of how it works, you will feel much more confident in your ability to tackle more complicated problems later on.\nAlong the same lines, there are tools like Jupyter AI that offer AI assistance integrated into your coding environment. While these can be very useful and help you code more efficiently, remember that the more you rely on tools like this early on in your coding journey, the more dependent you will be on them and the less you will learn.",
    "crumbs": [
      "Urban Data Analytics",
      "What and where of data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html",
    "href": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html",
    "title": "Intro to Python and Jupyter",
    "section": "",
    "text": "This notebook introduces the Python programming language and Jupyter notebooks. It will cover:",
    "crumbs": [
      "Urban Data Analytics",
      "Intro to Python and Jupyter"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#what-is-programming",
    "href": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#what-is-programming",
    "title": "Intro to Python and Jupyter",
    "section": "What is programming?",
    "text": "What is programming?\nCoding, or computer programming, essentially describes the act of giving a computer written instructions for a task or set of tasks. According to Wikipedia, it “involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages.”",
    "crumbs": [
      "Urban Data Analytics",
      "Intro to Python and Jupyter"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#what-is-coding",
    "href": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#what-is-coding",
    "title": "Intro to Python and Jupyter",
    "section": "What is coding?",
    "text": "What is coding?\nCoding, or computer programming, is the act of giving a computer written instructions for a task or set of tasks. According to Wikipedia, it “involves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages.”",
    "crumbs": [
      "Urban Data Analytics",
      "Intro to Python and Jupyter"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#what-is-python",
    "href": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#what-is-python",
    "title": "Intro to Python and Jupyter",
    "section": "What is Python?",
    "text": "What is Python?\nPython is “an interpreted, object-oriented, high-level programming language with dynamic semantics” (python.org). It supports modules, which are “files containing Python definitions and statements”, and packages, which are “a way to organize and structure code by grouping related modules into directories.”\nIn other words, Python is a computer programming language, just like how R and SQL and Javascript are computer programming languages. You can think of these similarly to languages that people speak and write, like Spanish or Arabic – each language has its own set of rules and different syntax, but (most) ideas can be translated from one language to another.\nInstead of using language to communicate with other people, you write code to give directions to a computer. In data analysis, these directions could be to multiply two variables in a dataframe, create a plot, or run a model, for example. In this course, we’ll use Python because it is one of the most commonly used coding languages.",
    "crumbs": [
      "Urban Data Analytics",
      "Intro to Python and Jupyter"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#what-is-a-computational-notebook",
    "href": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#what-is-a-computational-notebook",
    "title": "Intro to Python and Jupyter",
    "section": "What is a computational notebook?",
    "text": "What is a computational notebook?\nWhen you write something in English or another natural language, you need to choose an environment to write in. For example, if you’re writing an email, you might use Gmail or Outlook. If you’re writing a report, you might use Google Docs or Microsoft Word. Or maybe you prefer writing on parchment with a quill pen!\nWhen coding, you also have to choose an environment to write in. This is typically called a “code editor” or “integrated development environment” (IDE). There are many IDEs to choose from, but some of the most popular ones for Python are Visual Studio Code (“VS Code”) and PyCharm.\nThere are many different ways to execute code, or tell the computer to perform the instructions you’ve written. If you write your code in a file that’s saved with the .py file extension, you can run the entire “script” (i.e., document with code) at once, and the computer will follow all of your instructions, one line at a time. This works well for scripts that are complete and do not need to be run in separate chunks.\nIf you are exploring, analyzing, or visualizing data, it might be easier to work in a computational notebook. Computational notebooks are coding environments that not only allow you to write code, but also let you write explanations and show the outputs of your analysis. Jupyter Notebook, used mostly for data analysis and machine learning, is one of the most popular computational notebooks. When using a Jupyter Notebook, you can run code chunk-by-chunk and see the output right below each chunk. For example, you can write a chunk of code that manipulates a dataframe and then look at the first few rows of the dataframe right below the code.\nIn this course, we recommend using Jupyter Notebook (a type of computational notebook) in VS Code (an IDE). The VS Code software has a Jupyter Notebook “extension” that allows you to work in the notebook format. We’ll also teach you how to execute simple Python scripts (which have have the .py file extension).\nRemember that there are many different ways to run code! If you prefer working in Jupyter Lab (a web-based environment for Jupyter Notebooks) or Sublime Text (a code editor), for example, go for it – we just won’t be providing specific instructions for those.",
    "crumbs": [
      "Urban Data Analytics",
      "Intro to Python and Jupyter"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#run-a-python-script-in-the-terminal",
    "href": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#run-a-python-script-in-the-terminal",
    "title": "Intro to Python and Jupyter",
    "section": "Run a Python script in the terminal",
    "text": "Run a Python script in the terminal\nOnce you have VS Code open and ready to use, follow the instructions in this tutorial, doing everything through the “Run Python code” section (stop right before the section called “Configure and run the debugger”). Note that if you don’t already have Python downloaded on your computer, you’ll need to download it.\nYou can also run a .py script directly from your terminal, which is basically a tool where you type commands to control your computer. See instructions here on how to do that.",
    "crumbs": [
      "Urban Data Analytics",
      "Intro to Python and Jupyter"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#download-visual-studio-code",
    "href": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#download-visual-studio-code",
    "title": "Intro to Python and Jupyter",
    "section": "Download Visual Studio Code",
    "text": "Download Visual Studio Code\nStart by downloading the appropriate version of VS Code for your computer from the official website. You can learn the basics of how to use VS Code here, or you can look up tutorials on Youtube.",
    "crumbs": [
      "Urban Data Analytics",
      "Intro to Python and Jupyter"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#run-code-in-a-jupyter-notebook",
    "href": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#run-code-in-a-jupyter-notebook",
    "title": "Intro to Python and Jupyter",
    "section": "Run code in a Jupyter Notebook",
    "text": "Run code in a Jupyter Notebook\nNow let’s run code in a Jupyter Notebook, which will make it easier to explore and analyze data as you work. Start by downloading the Jupyter extension for VS Code. You can do this by clicking on the “Extension” button on the left sidebar of VS Code (the one that looks like a couple squares), searching “Jupyter” in the search bar, selecting the first option, and clicking “Install”. Ignore the fact that the screenshot below says “Uninstall”; this is what it will say when it’s already installed.\n\n\n\nJupyter extension in VS Code\n\n\nTo get started coding, follow these instructions. To create a notebook, you just need to create a file with the .ipynb extension in whichever directory you want to save it in. To write and run code in the notebook, you will use “cells” that contain chunks of information.\nWhen you run an individual cell, you are telling the computer to follow the instructions you’ve provided in only that cell, ignoring any other cells that are in the notebook. Be careful of the order in which you “execute” the cells. For example, if you run cell B before running cell A, it doesn’t matter if cell B is located below cell A – the computer will still follow the instructions in B first and A second.",
    "crumbs": [
      "Urban Data Analytics",
      "Intro to Python and Jupyter"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#python-basics",
    "href": "notebooks/urban-data-analytics/2-intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#python-basics",
    "title": "Intro to Python and Jupyter",
    "section": "Python basics",
    "text": "Python basics\nNow that you have Jupyter Notebook set up in VS Code, let’s code! Below, we’ll cover some of the basic building blocks of Python.\n\nVariables\nA variable is like a labeled entity that stores information (numbers, text, etc.). You can create a variable, give it a name, and assign it a value.\nNote that in the below code, some of the lines are written with a # at the beginning - these are comments. Putting a # in front of a line of code tells the computer not to execute it. You should use comments often to explain what your code is doing, either for someone else who might need to understand it or for your future self.\n\n# Assign a value to a variable\nname = \"Alice\"\nage = 30\n\nIn the above cell, we created two variables, one called name and another called age. The name variable is a string because it is a sequence of characters. The computer knows this is a string because we enclosed the text, Alice, in quotes. Single or double quotes both work here.\nThe age variable is an integer because it is a numeric value without decimals. You can see the data type with type([name of object]) like below:\n\ntype(age)\n\nint\n\n\nIf we print the variable, it will show us the variable’s value:\n\nprint(age)\n\n30\n\n\nWe can also re-assign variables, which will change their value. Now when we print the value of age, it will show 31 instead of 30:\n\n# Re-assign `age` variable with a new value\nage = 31\nprint(age)\n\n31\n\n\nRemember that the computer interprets your code in the order you run the cells, not in the order of the cells in the notebook. For example, if you ran the above cell that assigns a value of 31 to the age variable before running the cell that assigns the value of 30 to age, the computer would store the value of 31.\n\n\nSimple math\nPython can do simple math, like a calculator:\n\n4 + 3\n\n7\n\n\n\n10/3\n\n3.3333333333333335\n\n\nYou can also use the math module to access more advanced functions, like taking the square root. To use this module, you have to import it first:\n\nimport math # import module\nmath.sqrt(25)\n\n5.0\n\n\n\n\nLists\nA list is a collection of elements which can be accessed by their position. Python uses something called zero-based indexing, which means the first element of a sequence has an index (position) of 0 instead of 1.\nIn the below example, fruits is a variable whose type is a list.\n\n# Assign list to variable called 'fruits'\nfruits = [\"apple\", \"banana\", \"cherry\"]\ntype(fruits)\n\nlist\n\n\n\n# Access first item in list\nprint(fruits[0])\n\napple\n\n\n\n# Access second item in list\nprint(fruits[1])\n\nbanana\n\n\nItems can be appended to lists:\n\n# Add \"orange\" to the list\nfruits.append(\"orange\")\nprint(fruits)\n\n['apple', 'banana', 'cherry', 'orange']\n\n\nWe can check the length of the new list to see how many elements it has:\n\nlen(fruits)\n\n4\n\n\nLearn more about lists here.\n\n\nDictionaries\nA dictionary is a type of object that stores information in pairs: each “entry” in the dictionary has both a key and a value. In the example below, person is a dictionary that contains characteristics – specifically the name and age – of a person.\n\nperson = {\"name\": \"Alice\", \"age\": 30}\n\nWe can access the value associated with the name of the person:\n\nprint(person[\"name\"])\n\nAlice\n\n\nWe can also add a new key-value pair to the dictionary that represents, in this case, the person’s job:\n\nperson[\"job\"] = \"Engineer\"\nprint(person)\n\n{'name': 'Alice', 'age': 30, 'job': 'Engineer'}\n\n\nLearn more about dictionaries here.\n\n\nIf statements\nIf statements let your code make decisions. You check a condition (e.g., whether age &gt;= 18), and run different code depending on whether it’s true or false.\n\nage = 18\n\nif age &gt;= 18:\n    print(\"You're an adult!\")\nelse:\n    print(\"You're a minor.\")\n\nYou're an adult!\n\n\nLearn more about if statements here.\n\n\nFor loops\nA for loop repeats code for each item in a list or range. For example:\n\nfor fruit in [\"apple\", \"banana\", \"cherry\"]:\n    print(fruit)\n\napple\nbanana\ncherry\n\n\nLearn more about for loops here.\n\n\nWhile loops\nA while loop repeats code as long as a condition is true. In the below example, we start with 0 and keep adding 1 until we get to 3, after which we stop counting:\n\ncount = 0\nwhile count &lt;= 3:\n    print(\"Counting:\", count)\n    count += 1\n\nCounting: 0\nCounting: 1\nCounting: 2\nCounting: 3\n\n\nLearn more about while loops here.\n\n\nFunctions\nA function is a reusable block of code that performs a task. You “define” it (write the code that performs the task) once and “call” it (run that pre-defined code) whenever you want. In the below example, we define the function greet so that when it is called, it prints “Hello, [name]!” where name is an argument (also known as a parameter) that’s passed into the function.\n\n# Define the function\ndef greet(name):\n    print(f\"Hello, {name}!\")\n\n# Call the function\ngreet(\"Alice\")\n\nHello, Alice!\n\n\nNot all functions need arguments. For example:\n\n# Define the function\ndef howdy():\n    print(\"Howdy!\")\n\n# Call the function\nhowdy()\n\nHowdy!\n\n\nSome functions have more than one argument. For example:\n\n# Define the function\ndef add_numbers(a, b):\n    c = a + b\n    print(c)\n\n# Call the function\nadd_numbers(8, 7)\n\n15\n\n\nWhile all of the example functions listed above result in something being printed out, most functions do more than that. For example, a single function can filter a dataset based on a set of values, manipulate the resulting dataset, and create a plot.\nLearn more about functions here.",
    "crumbs": [
      "Urban Data Analytics",
      "Intro to Python and Jupyter"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/7-measuring-the-city/measuring-the-city.html",
    "href": "notebooks/urban-data-analytics/7-measuring-the-city/measuring-the-city.html",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "In this module, we will explore how to measure urban phenomena using data, specifically focusing on:\n\nCommon spatial units and measures of aggregation\nCommon metrics and indicators used in urban analysis\nLimitations & potential errors in data collection\n\n\n\nWhen analyzing geospatial data, it is often necessary to select a spatial unit or unit of aggregation to define the boundaries of the areas you are representing. Below are some units of aggregation that are often used in urban analysis:\n\n\nBoundaries for spatial census data are either defined by federal or provincial/territorial governments or “according to a set of rules based on geographic attributes and one or more characteristics of the resident population” (Statistics Canada). See Canadian census data for more information.\n\n\n\nCommon census boundaries in Toronto\n\n\n\n\n\nPolitical boundaries delineate areas that are governed by different authorities, like countries or states. Canadian federal electoral districts, or ridings, are geographic constituencies that are each represented by a Member of Parliament. There are also provincial and territorial electoral districts, which are smaller.\n\n\n\nCanadian electoral districts (source)\n\n\n\n\n\nA grid is “a network of evenly spaced horizontal and vertical lines used to identify locations on a map” (source). Grids are used in spatial analysis when existing boundaries are unavailable, unsuitable, or when evenly sized, uniform areas are required. Geohashes – which uniquely identify specific regions according to their latitude and longitude everywhere on Earth – are one type of commonly used grid.\n\n\n\nGeohashes in Québec City (source)\n\n\n\n\n\nWhile streets are often added to maps to provide geographic context, streets can also be their own unit of analysis. For example, traffic flow could be measured on street segments throughout a city. In the image below, streets are categorized by how friendly they are to pedestrians and other forms of traffic.\n\n\n\nStreet types (source)\n\n\n\n\n\nSome urban data is\n\n\n\n\n\n\n\nTopic\nCommon metrics\n\n\n\n\nPeople / socioeconomics\n?\n\n\nHousing\n\n\n\nLand use\n\n\n\nEconomics / employment\n\n\n\nTransportation\n\n\n\nEnvironment\n\n\n\nHealth\n\n\n\n\n\n\n\nModifiable Areal Unit Problem (MAUP) Ecological fallacy",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/7-measuring-the-city/measuring-the-city.html#common-metrics-and-indicators-for-urban-analyses",
    "href": "notebooks/urban-data-analytics/7-measuring-the-city/measuring-the-city.html#common-metrics-and-indicators-for-urban-analyses",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "Topic\nCommon metrics\n\n\n\n\nPeople / socioeconomics\n?\n\n\nHousing\n\n\n\nLand use\n\n\n\nEconomics / employment\n\n\n\nTransportation\n\n\n\nEnvironment\n\n\n\nHealth",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/7-measuring-the-city/measuring-the-city.html#limitations-potential-errors-in-data-collection",
    "href": "notebooks/urban-data-analytics/7-measuring-the-city/measuring-the-city.html#limitations-potential-errors-in-data-collection",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "Modifiable Areal Unit Problem (MAUP) Ecological fallacy",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/7-measuring-the-city/measuring-the-city.html#common-spatial-units-and-measures-of-aggregation",
    "href": "notebooks/urban-data-analytics/7-measuring-the-city/measuring-the-city.html#common-spatial-units-and-measures-of-aggregation",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "When analyzing geospatial data, it is often necessary to select a spatial unit or unit of aggregation to define the boundaries of the areas you are representing. Below are some units of aggregation that are often used in urban analysis:\n\n\nBoundaries for spatial census data are either defined by federal or provincial/territorial governments or “according to a set of rules based on geographic attributes and one or more characteristics of the resident population” (Statistics Canada). See Canadian census data for more information.\n\n\n\nCommon census boundaries in Toronto\n\n\n\n\n\nPolitical boundaries delineate areas that are governed by different authorities, like countries or states. Canadian federal electoral districts, or ridings, are geographic constituencies that are each represented by a Member of Parliament. There are also provincial and territorial electoral districts, which are smaller.\n\n\n\nCanadian electoral districts (source)\n\n\n\n\n\nA grid is “a network of evenly spaced horizontal and vertical lines used to identify locations on a map” (source). Grids are used in spatial analysis when existing boundaries are unavailable, unsuitable, or when evenly sized, uniform areas are required. Geohashes – which uniquely identify specific regions according to their latitude and longitude everywhere on Earth – are one type of commonly used grid.\n\n\n\nGeohashes in Québec City (source)\n\n\n\n\n\nWhile streets are often added to maps to provide geographic context, streets can also be their own unit of analysis. For example, traffic flow could be measured on street segments throughout a city. In the image below, streets are categorized by how friendly they are to pedestrians and other forms of traffic.\n\n\n\nStreet types (source)\n\n\n\n\n\nSome urban data is",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "",
    "text": "Data exploration is an important step in the data analysis process. It can highlight anomalies or interesting trends, and reveal how the data are distributed. This notebook explains how to visually explore data by creating plots using the popular seaborn and matplotlib libraries.\nIt will include the following plots:",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#seaborn-plot-overview",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#seaborn-plot-overview",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "Seaborn Plot Overview",
    "text": "Seaborn Plot Overview\n\nHistograms\n\nHistogram\nStacked Histogram\nDensity Plot\nFrequency Polygon Histogram\n\nScatterplot\n\nScatterplot with color classification\n\nBar plot\nLine plot\nMultiple plots in one figure\n\n\nThe dataset we’ll be working with is Bike Share ridership data from the City of Toronto Open Data portal.\nWe can download it and save it in a folder as follows:\n\nimport urllib.request\nimport os\n\n#os.makedirs(\"data\") #only run once\nyear = 2022\nurl = \"https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/7e876c24-177c-4605-9cef-e50dd74c617f/resource/db10a7b1-2702-481c-b7f0-0c67070104bb/download/bikeshare-ridership-\" + str(year) + \".zip\"\nfolder = \"data\"\nurllib.request.urlretrieve(url, folder + \"/bike-share-ridership-\" + str(year) + \".zip\")\n\n('data/bike-share-ridership-2022.zip',\n &lt;http.client.HTTPMessage at 0x7f21e4039510&gt;)\n\n\nThe zip folder has .csv data for each month in the selected year.\nSince our data are zipped, we can either unzip the folder manually and run df = pd.read_csv(path_to_csv_file).\nOr we can load using the zipfile library. I’m feeding in variables for year and month that can easily allow for switching these out or looping over multiple in the future.\n\nimport zipfile\n\nmonth = '01'\n\nwith zipfile.ZipFile(\"data/bike-share-ridership-\" + str(year) + \".zip\") as myzip:\n    with myzip.open(\"bikeshare-ridership-\" + str(year) + \"/Bike share ridership \" + str(year) + \"-\" + month + \".csv\") as myfile:\n        df = pd.read_csv(myfile)\n        \ndf.head()\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n\n\n2\n14805112\n942\n7399\n01/01/2022 00:04\nLower Jarvis / Queens Quay E\n7686.0\n01/01/2022 00:19\nNaN\n361\nAnnual Member\n\n\n3\n14805113\n4256\n7334\n01/01/2022 00:04\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n4350\nCasual Member\n\n\n4\n14805114\n4353\n7334\n01/01/2022 00:05\nSimcoe St / Wellington St North\n7038.0\n01/01/2022 01:17\nDundas St W / Yonge St\n5074\nCasual Member\n\n\n\n\n\n\n\nGreat!\nLet’s start by looking at the trip duration column. I’m curious how long people are travelling by Bike Share.\nThe “Trip Duration” column is in seconds, that can be a bit a difficult to picture, let’s create a column for minutes by dividing by 60. Also notice that the initial column has an extra space, probably just a typo when the data were created.\nWe can then compute some simple summary statistics on the column.\n\ndf[\"Trip Duration Minutes\"] = df[\"Trip  Duration\"] / 60\ndf[\"Trip Duration Minutes\"].describe()\n\ncount    56765.000000\nmean        14.930107\nstd        206.125166\nmin          0.000000\n25%          6.183333\n50%         10.016667\n75%         16.050000\nmax      38095.650000\nName: Trip Duration Minutes, dtype: float64\n\n\n Cool! we’ve got the mean, standard deviation, and quantiles. The max trip is pretty crazy! Not sure if it’s an error in the data, or someone just forgot to return their bike for that long.\nThe median (50%) being lower than the mean shows how their are definetly outliers.\nLet’s plot a distribution of shorter trips (those less than 2 hours long).\nThis will be our first forray into seaborn. We will specifically use trips that are less than 120 minutes.\nNote as well that I am just plotting a random sample of 1000 observations. Could do them all, but plotting is slower.\n\nsns.displot(df.loc[df[\"Trip Duration Minutes\"] &lt;= 120, \"Trip Duration Minutes\"].sample(1000),\n           bins = 15,\n           color = \"green\")\n\n\n\n\n\n\n\n\nLet’s add some colour for user type \n\nsns.displot(data = df.loc[df[\"Trip Duration Minutes\"] &lt;= 120].sample(1000),\n            x = \"Trip Duration Minutes\",\n            multiple = \"stack\",\n            bins = 15,\n            hue = \"User Type\",\n            palette = [\"pink\", \"orange\"])\n\n\n\n\n\n\n\n\nHow about a plot of trips by day of the month, and colour by user type? We can comment on/off the colour parameter do add different lines by user type. We will do this in two ways: a kernal density plot \n\ndf['Start Date'] = pd.to_datetime(df['Start Time'], format='%m/%d/%Y %H:%M')\nddf = df.sort_values('Start Date')\n\nsns.displot(ddf.loc[ddf[\"Trip Duration Minutes\"] &lt;= 120].sample(1000), \n            x=\"Start Date\", \n            hue=\"User Type\", \n            kind=\"kde\", \n            fill=True,\n            height=6, \n            aspect=11/8.5)\n\n\n\n\n\n\n\n\nand a frequency polygon histogram   Note: to adjust the size using the figsize function, you must import plt\n\n#fig, ax = plt.subplots(figsize=(11, 8.5))\n\nsns.histplot(ddf.loc[ddf[\"Trip Duration Minutes\"] &lt;= 120].sample(1000), \n            x=\"Start Date\", \n            hue=\"User Type\",\n           # ax = ax,\n            element = \"poly\")\n\n\n\n\n\n\n\n\nI’m curious if both types of members are likely to use Bike Share on the same dates. It’s a bit difficult in this plot to see if there is correlation between the Annual Member and Casual Members. Let’s make a scatter plot! Let’s first do a group by to generate a smaller DataFrame of counts for each type. We can use the pivot_table function, very similar to Excel\n\ndf_date = df.pivot_table(index=df['Start Date'].dt.date, columns='User Type', aggfunc='size', fill_value=0).reset_index()\n\ndf_date[\"Start Date\"] = df_date[\"Start Date\"].astype(str)\n\ndf_date.head(5)\n\n\n\n\n\n\n\nUser Type\nStart Date\nAnnual Member\nCasual Member\n\n\n\n\n0\n2022-01-01\n1613\n1238\n\n\n1\n2022-01-02\n830\n305\n\n\n2\n2022-01-03\n1488\n669\n\n\n3\n2022-01-04\n2430\n941\n\n\n4\n2022-01-05\n2175\n695\n\n\n\n\n\n\n\nand create a scatterplot \n\nax = sns.scatterplot(data=df_date, \n                x=\"Annual Member\", \n                y=\"Casual Member\",\n               # hue = \"Start Date\",\n               # palette = \"flare\",\n                legend=True)\n\n# sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n\n# uncomment the above lines to see what changes\n\n\n\n\n\n\n\n\nWe can also do a quickly compute a correlation between the two variables\n\nfrom scipy.stats import pearsonr\n\npearsonr(df_date['Annual Member'], df_date['Casual Member'])\n\nPearsonRResult(statistic=0.9037800081711291, pvalue=3.308526276108086e-12)\n\n\nLet’s try something a bit more analytical. Which stations have the most trips of people taking out and returning at the same place? \n\ndfs = df.loc[df[\"Start Station Id\"] == df[\"End Station Id\"]]\ndfs = dfs.groupby(\"Start Station Name\").size().reset_index(name = \"count\")\n\nsns.catplot(data= dfs.sort_values(\"count\", ascending = False).head(20), \n            x='count', \n            y='Start Station Name',\n            kind=\"bar\").set(title = \"Number of return trips to the same station\")\n\n\n# alt.Chart(\n#     dfs.sort_values(\"count\", ascending = False).head(20),\n#     title = \"Number of return trips to the same station\"\n# ).mark_bar(\n#     opacity=0.8\n# ).encode(\n#     y = alt.Y(\"Start Station Name\", sort='-x'),\n#     x = alt.X(\"count\"),\n#     tooltip = \"count\"\n# ).configure_axis(\n#     labelLimit=300,\n#     labelPadding=10,\n#     title=None\n# )\n\n\n\n\n\n\n\n\n\nTable Joins - Looking at Weather and Ridership\nOkay! Let’s do one last bit of analysis. Let’s try to see how ridership is related to weather.\nLet’s first load in ALL the ridership data, and compute the total number of trips per day. This might take a little while, it’s a lot of data to load!\n\ndf_months = []\n\nfor month in [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]:\n    \n    # for some reason, the data for November is zipped twice\n    if month == \"11\":\n        with zipfile.ZipFile(\"data/bike-share-ridership-\" + str(year) + \".zip\") as myzip:\n            with myzip.open(\"bikeshare-ridership-\" + str(year) + \"/Bike share ridership \" + str(year) + \"-\" + month + \".zip\") as inner_zip_file:\n                inner_zip = zipfile.ZipFile(inner_zip_file)\n                with inner_zip.open(\"Bike share ridership \" + str(year) + \"-\" + month + \".csv\") as myfile:\n                    df = pd.read_csv(myfile) \n                    df['Start Date'] = pd.to_datetime(df['Start Time'], format='%m/%d/%Y %H:%M').dt.date.astype(str)\n                    df_month = df.groupby('Start Date')['Start Date'].count().reset_index(name='Count')\n                    df_months.append(df_month)\n    else:\n        with zipfile.ZipFile(\"data/bike-share-ridership-\" + str(year) + \".zip\") as myzip:\n            with myzip.open(\"bikeshare-ridership-\" + str(year) + \"/Bike share ridership \" + str(year) + \"-\" + month + \".csv\") as myfile:\n                df = pd.read_csv(myfile)\n                df['Start Date'] = pd.to_datetime(df['Start Time'], format='%m/%d/%Y %H:%M').dt.date.astype(str)\n                df_month = df.groupby('Start Date')['Start Date'].count().reset_index(name='Count')\n                df_months.append(df_month)\n\ndf_by_day = pd.concat(df_months)\n\ndf_by_day['Start DateTime'] = pd.to_datetime(df_by_day['Start Date'])\n\ndel df_months\n\nGreat! lets plot this \n\nsns.lineplot(data=df_by_day, \n             x='Start DateTime', \n             y='Count',\n            marker = \"o\")\n\n\n\n\n\n\n\n\nLet’s load in our weather data! This was accessed from the federal governments historical climate data website: https://climate.weather.gc.ca/index_e.html\n\ndf_weather = pd.read_csv(\"toronto-historical-weather-2022.csv\")\n\ndf_weather.head()\n\n\n\n\n\n\n\n\nLongitude (x)\nLatitude (y)\nStation Name\nClimate ID\nDate/Time\nYear\nMonth\nDay\nData Quality\nMax Temp (°C)\n...\nTotal Snow (cm)\nTotal Snow Flag\nTotal Precip (mm)\nTotal Precip Flag\nSnow on Grnd (cm)\nSnow on Grnd Flag\nDir of Max Gust (10s deg)\nDir of Max Gust Flag\nSpd of Max Gust (km/h)\nSpd of Max Gust Flag\n\n\n\n\n0\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-01\n2022\n1\n1\nNaN\n5.1\n...\nNaN\nNaN\n2.4\nNaN\nNaN\nNaN\nNaN\nM\nNaN\nM\n\n\n1\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-02\n2022\n1\n2\nNaN\n-2.1\n...\nNaN\nNaN\n2.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n2\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-03\n2022\n1\n3\nNaN\n-4.0\n...\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n3\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-04\n2022\n1\n4\nNaN\n3.3\n...\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n4\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-05\n2022\n1\n5\nNaN\n4.9\n...\nNaN\nNaN\n0.3\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n\n\n5 rows × 31 columns\n\n\n\nThere’s a lot of data here we can look at, but let’s keep it simple for now, just look at mean temperature (°C) and total precipitation (mm) and join it to our daily ridership DataFrame\n\ndf_ridership_weather = df_by_day.merge(df_weather[[\"Date/Time\", \"Mean Temp (°C)\", \"Total Precip (mm)\"]], left_on=\"Start Date\", right_on=\"Date/Time\")\ndf_ridership_weather.head(5)\n\n\n\n\n\n\n\n\nStart Date\nCount\nStart DateTime\nDate/Time\nMean Temp (°C)\nTotal Precip (mm)\n\n\n\n\n0\n2022-01-01\n2851\n2022-01-01\n2022-01-01\n1.5\n2.4\n\n\n1\n2022-01-02\n1135\n2022-01-02\n2022-01-02\n-6.3\n2.0\n\n\n2\n2022-01-03\n2157\n2022-01-03\n2022-01-03\n-8.4\n0.0\n\n\n3\n2022-01-04\n3371\n2022-01-04\n2022-01-04\n-1.2\n0.0\n\n\n4\n2022-01-05\n2870\n2022-01-05\n2022-01-05\n0.2\n0.3\n\n\n\n\n\n\n\n\nsns.lineplot(data=df_ridership_weather, \n             x='Start DateTime', \n             y='Mean Temp (°C)',\n            marker = \"o\")\n\n\n\n\n\n\n\n\n Let’s look at the last two plots side by side\n\nfig, axes = plt.subplots(1,2) #rows, columns\n\n#first plot\nax = axes[0]\n\nsns.lineplot(data=df_by_day, \n             x='Start DateTime', \n             y='Count',\n            marker = \"o\",\n            ax = ax)\n\n#second plot\nax = axes[1]\n\nsns.lineplot(data=df_ridership_weather, \n             x='Start DateTime', \n             y='Mean Temp (°C)',\n            marker = \"o\",\n            ax = ax)\n\n#resize\nplots = plt.gcf() #get current figure\nplots.set_size_inches(13,8.5)\nplots.suptitle(\"Time & Temperature vs Bike Share Ridership \"); #Set title\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data = df_ridership_weather,\n               x=\"Mean Temp (°C)\",\n                y=\"Count\")\n\n\n\n\n\n\n\n\nClearly pretty correlated! (except for the one outlier). How about we include a simple classification for precipitation and add it on the chart as a colour \n\ndf_ridership_weather['Precip Category'] = df_ridership_weather['Total Precip (mm)'].apply(\n    lambda x: '0mm' if x == 0 else ('0mm &lt; X &lt; 10mm' if 0 &lt; x &lt; 10 else '10mm +')\n)\n\nrange_ = ['#DC4633', '#8DBF2E', '#007FA3']\n\nsns.scatterplot(data = df_ridership_weather,\n               x=\"Mean Temp (°C)\",\n                y=\"Count\",\n               hue = 'Precip Category',\n               palette = range_)\n\n\n\n\n\n\n\n\nCool! clearly there is a trend here.\nWe can try to statistical model this trend via a linear regression model.\nHow does temperature and precipitation predict ridership per day?\nscikit-learn is a commonly used library for statistical and machine learning modelling in Python.\nLet’s first just do a bivariate model:\n\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf_ridership_weather.dropna(inplace=True)\n\nX = df_ridership_weather[['Mean Temp (°C)']]\ny = df_ridership_weather['Count']\n\n# Add constant to the X matrix\nX = sm.add_constant(X)\n\n# Fit an OLS model and print the results\nols_model = sm.OLS(y, X).fit()\nprint(ols_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Count   R-squared:                       0.808\nModel:                            OLS   Adj. R-squared:                  0.808\nMethod:                 Least Squares   F-statistic:                     1525.\nDate:                Thu, 02 May 2024   Prob (F-statistic):          7.54e-132\nTime:                        14:37:02   Log-Likelihood:                -3495.4\nNo. Observations:                 364   AIC:                             6995.\nDf Residuals:                     362   BIC:                             7003.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nconst           5824.3977    256.883     22.673      0.000    5319.226    6329.569\nMean Temp (°C)   694.8456     17.793     39.052      0.000     659.855     729.836\n==============================================================================\nOmnibus:                       16.574   Durbin-Watson:                   1.264\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               24.740\nSkew:                          -0.339   Prob(JB):                     4.24e-06\nKurtosis:                       4.083   Cond. No.                         19.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nAnd now with the precipitation categories!\n\ndummies = pd.get_dummies(df_ridership_weather['Precip Category'], prefix='Precip')\n\n# Concatenate the dummy variables with the original dataframe\ndf_ridership_weather = pd.concat([df_ridership_weather, dummies], axis=1)\n\n# Define the X and y variables for the regression model\nX = df_ridership_weather[['Mean Temp (°C)', 'Precip_0mm', 'Precip_0mm &lt; X &lt; 10mm', 'Precip_10mm +']]\ny = df_ridership_weather['Count']\n\n# Add constant to the X matrix\nX = sm.add_constant(X)\n\n# Fit an OLS model and print the results\nols_model = sm.OLS(y, X.astype(float)).fit()\nprint(ols_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Count   R-squared:                       0.850\nModel:                            OLS   Adj. R-squared:                  0.849\nMethod:                 Least Squares   F-statistic:                     680.6\nDate:                Thu, 02 May 2024   Prob (F-statistic):          6.09e-148\nTime:                        14:37:02   Log-Likelihood:                -3450.5\nNo. Observations:                 364   AIC:                             6909.\nDf Residuals:                     360   BIC:                             6925.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nconst                  3243.5366    226.000     14.352      0.000    2799.090    3687.983\nMean Temp (°C)          686.3384     15.798     43.446      0.000     655.271     717.406\nPrecip_0mm             3863.4625    251.270     15.376      0.000    3369.322    4357.603\nPrecip_0mm &lt; X &lt; 10mm  1024.6425    290.413      3.528      0.000     453.523    1595.762\nPrecip_10mm +         -1644.5684    530.016     -3.103      0.002   -2686.884    -602.253\n==============================================================================\nOmnibus:                       36.879   Durbin-Watson:                   1.223\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              119.305\nSkew:                          -0.394   Prob(JB):                     1.24e-26\nKurtosis:                       5.692   Cond. No.                     4.74e+16\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 3.39e-29. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\nCount = 3243.54 + 686.34 * “Mean Temp (°C)” + 3863.46 * “Precip_0mm” + 1024.64 * “Precip_0mm” &lt; X &lt; 10mm - 1644.57 * “Precip_10mm +”",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#common-errors",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#common-errors",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "Common Errors",
    "text": "Common Errors\nData visualization is more straight forward because you can rely on documentation, and errors often come from data wrangling errors than data viz errors. As a general note, Seaborn likes long format instead of wide format. Our data is already in long format, so we will make a month column and use it to convert this into wide format.\n\ndf_ridership_weather.tail(68)\n\n\ndf_ridership_weather[\"Date/Time\"].dtype\n\n\nobj = 'hello'\n\nobj[0:2]\n\n\nCreate a Month column with your groups\n\n# write code to create a month column here\n\n\ndf_ridership_weather.head()\n\n\n\nConvert into wide format\n\ndf_wide = df_ridership_weather.pivot(columns=\"Month\", values = \"Count\", index = \"index\")\ndf_wide\n\nLet’s see what it would look like if we tried to plot this using wide data.\n\nsns.lineplot(data=df_wide, \n             x='Month', \n             y='Count',\n            marker = \"o\")\n\n\n\nBack into Long Format\nOf course our original dataset is in the long format, but let’s convert it back into long using the pandas melt function.\n\ndf_long = pd.melt(df_wide)\ndf_long.head()\n\n\nsns.lineplot(data=df_long, \n             x='Month', \n             y='value',\n            marker = \"o\")",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#looking-at-documentation",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#looking-at-documentation",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "Looking at Documentation",
    "text": "Looking at Documentation\nFeel free to read more about the Seaborn library and what you can do with it here. Try to play around with different kinds of graphs using the the bikeshare data. We will also look at it together and work through how to read the documentation together.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#install-and-import-libraries",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#install-and-import-libraries",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "Install and import libraries",
    "text": "Install and import libraries\nIf you haven’t already done so, install the following libraries using pip:\n\n!pip install pandas\n!pip install seaborn\n!pip install matplotlib\n\nNow import them using their common aliases. Note that pyplot is a module of the matplotlib library:\n\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#load-bike-share-data",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#load-bike-share-data",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "Load bike share data",
    "text": "Load bike share data\nThe dataset we’ll be working with is Bike Share Toronto ridership data from January 2022. It’s already been downloaded for you, so you can just load it below using the read_csv function from the pandas library.\nHowever, if you want to download the data yourself…\n\nGo to this page from the City of Toronto’s Open Data portal\nClick on the blue “Download” bar towards the bottom\nDownload the bikeshare-ridership-2022 zip file\nUnzip the file and open the folder\nSave the Bike share ridership 2022-01.csv file\nMake sure you read the file using the correct filepath\n\n\ndf = pd.read_csv(\"data/Bike share ridership 2022-01.csv\")\n\nLet’s take a look at the first 5 rows of the dataframe:\n\ndf.head()\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n\n\n2\n14805112\n942\n7399\n01/01/2022 00:04\nLower Jarvis / Queens Quay E\n7686.0\n01/01/2022 00:19\nNaN\n361\nAnnual Member\n\n\n3\n14805113\n4256\n7334\n01/01/2022 00:04\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n4350\nCasual Member\n\n\n4\n14805114\n4353\n7334\n01/01/2022 00:05\nSimcoe St / Wellington St North\n7038.0\n01/01/2022 01:17\nDundas St W / Yonge St\n5074\nCasual Member\n\n\n\n\n\n\n\nWhat do you notice about the data? Which variables are you interested in exploring?",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-long-do-people-bike-for",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-long-do-people-bike-for",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "How long do people bike for?",
    "text": "How long do people bike for?\nLet’s start by looking at the trip duration column to see how long people are travelling when using the bike share.\nThe “Trip Duration” column is in seconds, that can be a bit a difficult to picture, let’s create a column for minutes by dividing by 60. We can then compute some simple summary statistics on the column using the describe method from pandas.\n\ndf[\"Trip Duration Minutes\"] = df[\"Trip  Duration\"] / 60\ndf[\"Trip Duration Minutes\"].describe()\n\ncount    56765.000000\nmean        14.930107\nstd        206.125166\nmin          0.000000\n25%          6.183333\n50%         10.016667\n75%         16.050000\nmax      38095.650000\nName: Trip Duration Minutes, dtype: float64\n\n\nNow we have the mean, standard deviation, and quantiles. The average trip length is about 15 minutes, which seems reasonable. However, the maximum trip length is about 635 hours! That doesn’t seem right – it’s unclear if it’s an error in the data, or someone just forgot to return their bike.\nThe fact that the median (the “50%” statistic from above) is lower than the mean shows that the data are right-skewed – most values are clustered at the lower end of the range – and there are some large outliers.\nLet’s plot a distribution of shorter trips (those less than 2 hours long) using the displot function from the seaborn package. Refer to this documentation to understand each of the parameters/arguments used to create the plots below.\nWe’ll start by filtering the data to only rows where the “Trip Duration Minutes” column value is less than or equal to 120, using loc. Note that we’re only selecting the column itself because that’s the only data we need for this plot.\n\nsns.set_style(\"whitegrid\") # set style to make plots look nicer - this applies to all following plots\n\n# Filter the data\ntrips_120_duration = df.loc[df[\"Trip Duration Minutes\"] &lt;= 120, \"Trip Duration Minutes\"]\n\n# Create the plot, with 15 bins, and make it green\nsns.displot(trips_120_duration,\n           bins = 15,\n           color = \"green\")\n\nsns.despine()",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-does-trip-duration-vary-by-user-type",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-does-trip-duration-vary-by-user-type",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "How does trip duration vary by user type?",
    "text": "How does trip duration vary by user type?\nNext, let’s create a stacked histogram that shows trip duration for casual versus annual members. Unlike the previous plot, we’ll need more than just the “Trip Duration Minutes” column because we’re also plotting by “User Type”.\n\ntrips_120 = df.loc[df[\"Trip Duration Minutes\"] &lt;= 120]\n\nsns.displot(data=trips_120,\n            x=\"Trip Duration Minutes\",\n            multiple=\"stack\",\n            bins=15,\n            hue=\"User Type\",\n            palette=[\"pink\", \"orange\"]\n            ).set(title = \"Bike Share Trip Duration by User Type\")\n\nsns.despine()\n\n\n\n\n\n\n\n\nThis stacked histogram shows that shorter trips (&lt;20 minutes) are most common for both types of members. There are far fewer trips taken by casual members compared to annual members.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-long-do-bike-share-trips-last",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-long-do-bike-share-trips-last",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "How long do bike share trips last?",
    "text": "How long do bike share trips last?\nLet’s start by looking at the trip duration column to see how long people are travelling when using the bike share.\nThe “Trip Duration” column is in seconds, which is hard to interpret – to make it easier, let’s create a column for minutes by dividing by 60. We can then compute some simple summary statistics on the column using the describe method from pandas.\n\ndf[\"Trip Duration Minutes\"] = df[\"Trip  Duration\"] / 60\ndf[\"Trip Duration Minutes\"].describe()\n\ncount    56765.000000\nmean        14.930107\nstd        206.125166\nmin          0.000000\n25%          6.183333\n50%         10.016667\n75%         16.050000\nmax      38095.650000\nName: Trip Duration Minutes, dtype: float64\n\n\nNow we have the mean, standard deviation, and quantiles. The average trip length is about 15 minutes, which seems reasonable. However, the maximum trip length is about 635 hours! That doesn’t seem right – when analyzing this data, we would need to make a decision about how to handle this value. For example, we might filter it out if we’re confident it’s an error. Or maybe it’s part of a larger pattern of people who forget to dock their bikes.\nThe fact that the median (the “50%” statistic from above) is lower than the mean shows that the data are right-skewed – most values are clustered at the lower end of the range – and there are some large outliers.\nLet’s plot a histogram to show the distribution of shorter trips (those less than 2 hours long) using the displot function from the seaborn package. Refer to this documentation to understand each of the parameters/arguments used to create the plots below.\nWe’ll start by filtering the data to only rows where the “Trip Duration Minutes” column value is less than or equal to 120, using loc. Note that we’re only selecting the column itself because that’s the only data we need for this plot.\n\nsns.set_style(\"whitegrid\") # set style to make plots look nicer\n\n# Filter the data\ntrips_120_duration = df.loc[df[\"Trip Duration Minutes\"] &lt;= 120, \"Trip Duration Minutes\"]\n\n# Create the plot, with 15 bins, and make it green\nsns.displot(trips_120_duration,\n            bins = 15,\n            color = \"green\"\n            ).set(title = \"Number of Trips by Trip Duration\")\n\nsns.despine() # get rid of plot borders for cleaner look\n\n\n\n\n\n\n\n\nAccording to this plot, trips are typically fairly short, with the majority of trip durations well below 20 minutes. There are a small number of trips that are longer than 40 minutes but these are relatively rare.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#how",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#how",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "How",
    "text": "How\nWhat if we want to plot the number of trips by day of the month, colored by user type? Let’s start with a kernel density plot.\nFirst we need to convert the “Start Time” column to a datetime object using pandas.to_datetime so that we can sort by date.\n\ndf['Start Date'] = pd.to_datetime(df['Start Time'], format='%m/%d/%Y %H:%M')\ndf_sorted_date = df.sort_values('Start Date')\ndf_sorted_120 = df_sorted_date.loc[df_sorted_date[\"Trip Duration Minutes\"] &lt;= 120]\n\nsns.displot(df_sorted_120, \n            x=\"Start Date\", \n            hue=\"User Type\", # try commenting this out and see what happens\n            kind=\"kde\", \n            fill=True,\n            height=6, \n            aspect=11/8.5)\n\nsns.despine()\n\n\n\n\n\n\n\n\nNext, let’s create a frequency polygon histogram. We’ll make sure the plot is the right size using the figsize parameter of the plt.subplots() function.\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\nsns.histplot(df_sorted_120, \n            x=\"Start Date\", \n            hue=\"User Type\",\n            element = \"poly\")\n\nsns.despine()\n\n\n\n\n\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\nTrip Duration Minutes\nStart Date\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n72.250000\n2022-01-01 00:02:00\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n2.100000\n2022-01-01 00:02:00\n\n\n2\n14805112\n942\n7399\n01/01/2022 00:04\nLower Jarvis / Queens Quay E\n7686.0\n01/01/2022 00:19\nNaN\n361\nAnnual Member\n15.700000\n2022-01-01 00:04:00\n\n\n3\n14805113\n4256\n7334\n01/01/2022 00:04\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n4350\nCasual Member\n70.933333\n2022-01-01 00:04:00\n\n\n4\n14805114\n4353\n7334\n01/01/2022 00:05\nSimcoe St / Wellington St North\n7038.0\n01/01/2022 01:17\nDundas St W / Yonge St\n5074\nCasual Member\n72.550000\n2022-01-01 00:05:00\n\n\n\n\n\n\n\nLet’s see if both types of members (casual and annual) are likely to use Bike Share on the same dates. It’s a bit difficult in the above plot to see if there is correlation between the annual members and casual members.\nFirst, we’ll create a smaller dataframe that includes the number of trips taken by each type of member. We can use the pivot_table function, which works similarly to creating a pivot table in Excel.\n\ndf_date = df.pivot_table(index=df['Start Date'].dt.date, \n                         columns='User Type', \n                         aggfunc='size', \n                         fill_value=0).reset_index()\n\ndf_date[\"Start Date\"] = df_date[\"Start Date\"].astype(str)\n\ndf_date.head() # look at the first 5 rows of the resulting dataframe\n\n\n\n\n\n\n\nUser Type\nStart Date\nAnnual Member\nCasual Member\n\n\n\n\n0\n2022-01-01\n1613\n1238\n\n\n1\n2022-01-02\n830\n305\n\n\n2\n2022-01-03\n1488\n669\n\n\n3\n2022-01-04\n2430\n941\n\n\n4\n2022-01-05\n2175\n695\n\n\n\n\n\n\n\nNow let’s make a scatterplot that compares the number of daily trips for casual versus annual members. Try uncommenting the commented lines (i.e., remove the ‘#’ symbols) to see what happens when those parameters are added back in.\n\nax = sns.scatterplot(data=df_date, \n                     x=\"Annual Member\", \n                     y=\"Casual Member\",\n                     # hue = \"Start Date\",\n                     # palette = \"flare\",\n                     # legend=True\n                     )\nsns.despine()\nplt.ylim(bottom=0)\n\n# sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n\n\n\n\n\n\n\n\nWe can also determine how highly correlated the two variables are by calculating the Pearson correlation coefficient using the pearsonr function from the stats module of the scipy library.\n\nfrom scipy.stats import pearsonr\n\npearsonr(df_date['Annual Member'], df_date['Casual Member'])\n\nPearsonRResult(statistic=0.9037800081711292, pvalue=3.3085262761079782e-12)\n\n\nThe Pearson correlation coefficient (0.9038) is very close to 1, which means there is a very strong positive linear relationship between the number of trips taken by annual and casual members. A value of 0 would indicate no correlation, while a value close to -1 would indicate a strong negative relationship.\nThe p-value is very small (&lt;0.05), meaning the correlation is statistically significant (i.e., there is strong evidence that the correlation is real).\nNow let’s try creating a visualization that’s a bit more analytical. We’ll answer the question: which stations have the largest number of trips that both start and end at that station?\n\n# Filter the data to trips where the ID of the start station is the same as the ID of the end station\ndfs = df.loc[df[\"Start Station Id\"] == df[\"End Station Id\"]]\n\n# Group by station and count the number of trips\ndfs = dfs.groupby(\"Start Station Name\").size().reset_index(name = \"count\")\n\n# Create a bar plot of the top 20 stations, shown in descending order of trips\nsns.catplot(data= dfs.sort_values(\"count\", ascending = False).head(20), \n            x='count', \n            y='Start Station Name',\n            kind=\"bar\").set(title = \"Number of return trips to the same station\")\n\nsns.despine()\n\n\n\n\n\n\n\n\n\nWeather and Ridership\nOkay! Let’s do one last bit of analysis. Let’s try to see how ridership is related to weather.\nLet’s first load in ALL the ridership data, and compute the total number of trips per day. This might take a little while, it’s a lot of data to load!\n\ndf_months = []\n\nfor month in [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]:\n    \n    # for some reason, the data for November is zipped twice\n    if month == \"11\":\n        with zipfile.ZipFile(\"data/bike-share-ridership-\" + str(year) + \".zip\") as myzip:\n            with myzip.open(\"bikeshare-ridership-\" + str(year) + \"/Bike share ridership \" + str(year) + \"-\" + month + \".zip\") as inner_zip_file:\n                inner_zip = zipfile.ZipFile(inner_zip_file)\n                with inner_zip.open(\"Bike share ridership \" + str(year) + \"-\" + month + \".csv\") as myfile:\n                    df = pd.read_csv(myfile) \n                    df['Start Date'] = pd.to_datetime(df['Start Time'], format='%m/%d/%Y %H:%M').dt.date.astype(str)\n                    df_month = df.groupby('Start Date')['Start Date'].count().reset_index(name='Count')\n                    df_months.append(df_month)\n    else:\n        with zipfile.ZipFile(\"data/bike-share-ridership-\" + str(year) + \".zip\") as myzip:\n            with myzip.open(\"bikeshare-ridership-\" + str(year) + \"/Bike share ridership \" + str(year) + \"-\" + month + \".csv\") as myfile:\n                df = pd.read_csv(myfile)\n                df['Start Date'] = pd.to_datetime(df['Start Time'], format='%m/%d/%Y %H:%M').dt.date.astype(str)\n                df_month = df.groupby('Start Date')['Start Date'].count().reset_index(name='Count')\n                df_months.append(df_month)\n\ndf_by_day = pd.concat(df_months)\n\ndf_by_day['Start DateTime'] = pd.to_datetime(df_by_day['Start Date'])\n\ndel df_months\n\nGreat! lets plot this \n\nsns.lineplot(data=df_by_day, \n             x='Start DateTime', \n             y='Count',\n            marker = \"o\")\n\nLet’s load in our weather data! This was accessed from the federal governments historical climate data website: https://climate.weather.gc.ca/index_e.html\n\ndf_weather = pd.read_csv(\"toronto-historical-weather-2022.csv\")\n\ndf_weather.head()\n\nThere’s a lot of data here we can look at, but let’s keep it simple for now, just look at mean temperature (°C) and total precipitation (mm) and join it to our daily ridership DataFrame\n\ndf_ridership_weather = df_by_day.merge(df_weather[[\"Date/Time\", \"Mean Temp (°C)\", \"Total Precip (mm)\"]], left_on=\"Start Date\", right_on=\"Date/Time\")\ndf_ridership_weather.head(5)\n\n\nsns.lineplot(data=df_ridership_weather, \n             x='Start DateTime', \n             y='Mean Temp (°C)',\n            marker = \"o\")\n\n Let’s look at the last two plots side by side\n\nfig, axes = plt.subplots(1,2) #rows, columns\n\n#first plot\nax = axes[0]\n\nsns.lineplot(data=df_by_day, \n             x='Start DateTime', \n             y='Count',\n            marker = \"o\",\n            ax = ax)\n\n#second plot\nax = axes[1]\n\nsns.lineplot(data=df_ridership_weather, \n             x='Start DateTime', \n             y='Mean Temp (°C)',\n            marker = \"o\",\n            ax = ax)\n\n#resize\nplots = plt.gcf() #get current figure\nplots.set_size_inches(13,8.5)\nplots.suptitle(\"Time & Temperature vs Bike Share Ridership \"); #Set title\n\n\nsns.scatterplot(data = df_ridership_weather,\n               x=\"Mean Temp (°C)\",\n                y=\"Count\")\n\nClearly pretty correlated! (except for the one outlier). How about we include a simple classification for precipitation and add it on the chart as a colour \n\ndf_ridership_weather['Precip Category'] = df_ridership_weather['Total Precip (mm)'].apply(\n    lambda x: '0mm' if x == 0 else ('0mm &lt; X &lt; 10mm' if 0 &lt; x &lt; 10 else '10mm +')\n)\n\nrange_ = ['#DC4633', '#8DBF2E', '#007FA3']\n\nsns.scatterplot(data = df_ridership_weather,\n               x=\"Mean Temp (°C)\",\n                y=\"Count\",\n               hue = 'Precip Category',\n               palette = range_)\n\nCool! clearly there is a trend here.\nWe can try to statistical model this trend via a linear regression model.\nHow does temperature and precipitation predict ridership per day?\nscikit-learn is a commonly used library for statistical and machine learning modelling in Python.\nLet’s first just do a bivariate model:\n\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf_ridership_weather.dropna(inplace=True)\n\nX = df_ridership_weather[['Mean Temp (°C)']]\ny = df_ridership_weather['Count']\n\n# Add constant to the X matrix\nX = sm.add_constant(X)\n\n# Fit an OLS model and print the results\nols_model = sm.OLS(y, X).fit()\nprint(ols_model.summary())\n\nAnd now with the precipitation categories!\n\ndummies = pd.get_dummies(df_ridership_weather['Precip Category'], prefix='Precip')\n\n# Concatenate the dummy variables with the original dataframe\ndf_ridership_weather = pd.concat([df_ridership_weather, dummies], axis=1)\n\n# Define the X and y variables for the regression model\nX = df_ridership_weather[['Mean Temp (°C)', 'Precip_0mm', 'Precip_0mm &lt; X &lt; 10mm', 'Precip_10mm +']]\ny = df_ridership_weather['Count']\n\n# Add constant to the X matrix\nX = sm.add_constant(X)\n\n# Fit an OLS model and print the results\nols_model = sm.OLS(y, X.astype(float)).fit()\nprint(ols_model.summary())\n\n\nCount = 3243.54 + 686.34 * “Mean Temp (°C)” + 3863.46 * “Precip_0mm” + 1024.64 * “Precip_0mm” &lt; X &lt; 10mm - 1644.57 * “Precip_10mm +”",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-does-the-number-of-trips-vary-by-day-for-each-user-type",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-does-the-number-of-trips-vary-by-day-for-each-user-type",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "How does the number of trips vary by day, for each user type?",
    "text": "How does the number of trips vary by day, for each user type?\nWhat if we want to plot the number of trips by day of the month, colored by user type? Let’s start with a kernel density plot.\nFirst we need to convert the “Start Time” column to a datetime object using pandas.to_datetime so that we can sort by date.\n\ndf['Start Date'] = pd.to_datetime(df['Start Time'], format='%m/%d/%Y %H:%M')\ndf_sorted_date = df.sort_values('Start Date')\ndf_sorted_120 = df_sorted_date.loc[df_sorted_date[\"Trip Duration Minutes\"] &lt;= 120]\n\nNow let’s make the plot:\n\nsns.displot(df_sorted_120, \n            x=\"Start Date\", \n            hue=\"User Type\", # try commenting this out and see what happens\n            kind=\"kde\", \n            fill=True,\n            height=6, \n            aspect=11/8.5\n            ).set(title = \"Trip Density by Date and Member Type\")\n\nsns.despine()\n\n\n\n\n\n\n\n\nIt appears that trips peak in the first half of the month, with more trips taken by annual members than casual members, and are less frequent in the later half.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#are-bike-share-trips-more-common-when-the-weather-is-warmer",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#are-bike-share-trips-more-common-when-the-weather-is-warmer",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "Are bike share trips more common when the weather is warmer?",
    "text": "Are bike share trips more common when the weather is warmer?\nWhat if we are curious about the relationship between bike share trips and temperature? Maybe we have a hypothesis that more people tend to use bike share when it’s warmer. Let’s test this hypothesis by loading in another dataset, which includes the daily temperature in January 2022, and joining it with our bike share data.\nLet’s load in our weather data, which was already downloaded for you from the federal government’s historical climate data website.\n\ndf_weather = pd.read_csv(\"data/toronto-historical-weather-2022.csv\")\n\n# don't hide any of the column names, even though there are a lot\npd.set_option('display.max_columns', None)\n\ndf_weather.head() # show the first 5 rows\n\n\n\n\n\n\n\n\nLongitude (x)\nLatitude (y)\nStation Name\nClimate ID\nDate/Time\nYear\nMonth\nDay\nData Quality\nMax Temp (°C)\nMax Temp Flag\nMin Temp (°C)\nMin Temp Flag\nMean Temp (°C)\nMean Temp Flag\nHeat Deg Days (°C)\nHeat Deg Days Flag\nCool Deg Days (°C)\nCool Deg Days Flag\nTotal Rain (mm)\nTotal Rain Flag\nTotal Snow (cm)\nTotal Snow Flag\nTotal Precip (mm)\nTotal Precip Flag\nSnow on Grnd (cm)\nSnow on Grnd Flag\nDir of Max Gust (10s deg)\nDir of Max Gust Flag\nSpd of Max Gust (km/h)\nSpd of Max Gust Flag\n\n\n\n\n0\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-01\n2022\n1\n1\nNaN\n5.1\nNaN\n-2.1\nNaN\n1.5\nNaN\n16.5\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2.4\nNaN\nNaN\nNaN\nNaN\nM\nNaN\nM\n\n\n1\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-02\n2022\n1\n2\nNaN\n-2.1\nNaN\n-10.5\nNaN\n-6.3\nNaN\n24.3\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n2\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-03\n2022\n1\n3\nNaN\n-4.0\nNaN\n-12.9\nNaN\n-8.4\nNaN\n26.4\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n3\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-04\n2022\n1\n4\nNaN\n3.3\nNaN\n-5.7\nNaN\n-1.2\nNaN\n19.2\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n4\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-05\n2022\n1\n5\nNaN\n4.9\nNaN\n-4.5\nNaN\n0.2\nNaN\n17.8\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.3\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n\n\n\n\n\nGiven that there are so many column names in the dataframe, it might also be easier to look only at the column names, instead of the first 5 rows of data:\n\ndf_weather.columns\n\nIndex(['Longitude (x)', 'Latitude (y)', 'Station Name', 'Climate ID',\n       'Date/Time', 'Year', 'Month', 'Day', 'Data Quality', 'Max Temp (°C)',\n       'Max Temp Flag', 'Min Temp (°C)', 'Min Temp Flag', 'Mean Temp (°C)',\n       'Mean Temp Flag', 'Heat Deg Days (°C)', 'Heat Deg Days Flag',\n       'Cool Deg Days (°C)', 'Cool Deg Days Flag', 'Total Rain (mm)',\n       'Total Rain Flag', 'Total Snow (cm)', 'Total Snow Flag',\n       'Total Precip (mm)', 'Total Precip Flag', 'Snow on Grnd (cm)',\n       'Snow on Grnd Flag', 'Dir of Max Gust (10s deg)',\n       'Dir of Max Gust Flag', 'Spd of Max Gust (km/h)',\n       'Spd of Max Gust Flag'],\n      dtype='object')\n\n\nWe only need to keep the Date/Time and Mean Temp (°C) variables, so let’s subset the dataframe to only those columns:\n\ndf_weather_simp = df_weather[[\"Date/Time\", \"Mean Temp (°C)\"]]\ndf_weather_simp.head()\n\n\n\n\n\n\n\n\nDate/Time\nMean Temp (°C)\n\n\n\n\n0\n2022-01-01\n1.5\n\n\n1\n2022-01-02\n-6.3\n\n\n2\n2022-01-03\n-8.4\n\n\n3\n2022-01-04\n-1.2\n\n\n4\n2022-01-05\n0.2\n\n\n\n\n\n\n\nNow let’s join the weather dataframe with our original bike share dataframe on their respective date/time columns. First, let’s remind ourselves which column in the df bike share dataframe has the date of the trip.\n\ndf.head(2) # only show the first 2 rows\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\nTrip Duration Minutes\nStart Date\nDate Only\ndate\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n72.25\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n2.10\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n\n\n\n\n\n\n\nThe Start Date column is the one we want to use – Start Time also contains the date but we don’t need to know what time the trip started, just the date. We have to make sure that this variable has the same data type in each of the dataframes – if not, the join (merge) won’t work.\n\ndf['Start Date'].dtype\n\ndtype('&lt;M8[ns]')\n\n\n\ndf_weather_simp['Date/Time'].dtype\n\ndtype('O')\n\n\nThe variables have different types, so we need to convert them so they’re the same type:\n\n# Create new 'Date Only' column by extracting only the date from the datetime variable in the bike share data\ndf['date'] = df['Start Date'].dt.date\n\n# Convert the 'Date/Time' variable from string to date \ndf_weather_simp.loc[:, 'Date/Time'] = pd.to_datetime(df_weather_simp['Date/Time']).dt.date\n\n\ndf['date'].dtype\n\ndtype('O')\n\n\n\ndf_weather_simp['Date/Time'].dtype\n\ndtype('O')\n\n\nNow we can merge the dataframes, using the date column in the bike share dataset and the Date/Time column in the weather dataset.\n\ndf_ridership_weather = df.merge(df_weather_simp, \n                                left_on=\"date\", \n                                right_on=\"Date/Time\")\n\ndf_ridership_weather.head(2)\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\nTrip Duration Minutes\nStart Date\nDate Only\ndate\nDate/Time\nMean Temp (°C)\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n72.25\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n2022-01-01\n1.5\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n2.10\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n2022-01-01\n1.5\n\n\n\n\n\n\n\nLet’s first explore the average temperature for each day in January 2022.\n\nfig, ax = plt.subplots(figsize=(11, 5))\n\nsns.lineplot(data=df_ridership_weather, \n             x='date', \n             y='Mean Temp (°C)',\n             marker = \"o\"\n             ).set(title = \"Daily Average Temperature in January 2022\")\n\n\n\n\n\n\n\n\nNext, we’ll calculate the number of trips (i.e., rows) for each date, and then plot the number of trips versus the average temperature for every day in January 2022. We’ll include a trend line to illustrate the relationship between the two variables.\n\n# Count the number of rows (trips) by date\ndf_ridership_weather['Count'] = df_ridership_weather.groupby('date')['date'].transform('count')\n\nsns.regplot(data=df_ridership_weather, \n            x=\"Mean Temp (°C)\", \n            y=\"Count\", \n            scatter_kws={\"s\": 8}\n            ).set(title = \"Number of Bike Share Trips vs Daily Temperature in January 2022\")\n\n\n\n\n\n\n\n\nThe trend line has a positive slope, indicating there is a positive relationship between average daily temperature and number of trips.\nWe can also determine how highly correlated the two variables are by calculating the Pearson correlation coefficient using the pearsonr function from the stats module of the scipy library.\n\nfrom scipy.stats import pearsonr\n\npearsonr(df_ridership_weather[\"Mean Temp (°C)\"], df_ridership_weather[\"Count\"])\n\nPearsonRResult(statistic=0.6499094259264812, pvalue=0.0)\n\n\nThe Pearson correlation coefficient (0.65) is positive and relatively close to 1, which means there is a moderately strong positive linear relationship between the number of daily trips and the temperature. A value of 0 would indicate no correlation, while a value close to -1 would indicate a strong negative relationship.\nThe p-value is very small (&lt;0.05), meaning the correlation is statistically significant (i.e., there is strong evidence that the correlation is real).\nWe can conclude that our hypothesis was correct: more people tend to use bike share when it’s warmer. However, given that we only looked at the correlation between these two variables, we are not able to establish causation. In other words, we can say that there is a relationship, but we cannot say why. We are just in the exploratory phase of the data analysis process, and to say anything more about the reason for the relationship we would need to do more in-depth modeling.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#which-stations-have-the-largest-number-of-trips-that-both-start-and-end-at-that-station",
    "href": "notebooks/urban-data-visualization/2-exploratory-data-visualization-python/exploratory-data-visualization-python.html#which-stations-have-the-largest-number-of-trips-that-both-start-and-end-at-that-station",
    "title": "Exploratory Data Analysis and Visualization in pandas",
    "section": "Which stations have the largest number of trips that both start and end at that station?",
    "text": "Which stations have the largest number of trips that both start and end at that station?\nNow let’s try creating a visualization that’s a bit more analytical. We’ll answer the question: which stations have the largest number of trips that both start and end at that station?\n\n# Filter the data to trips where start station ID = end station ID\ndfs = df.loc[df[\"Start Station Id\"] == df[\"End Station Id\"]]\n\n# Group by station and count the number of trips\ndfs = dfs.groupby(\"Start Station Name\").size().reset_index(name = \"count\")\n\n# Create a bar plot of the top 20 stations, shown in descending order of trips\nsns.catplot(data= dfs.sort_values(\"count\", ascending = False).head(20), \n            x='count', \n            y='Start Station Name',\n            kind=\"bar\"\n            ).set(title = \"Number of Return Trips to the Same Station\")\n\nsns.despine()\n\n\n\n\n\n\n\n\nTommy Thompson Park is the station with the most round trip bike rides, followed by Nassau St / Bellevue Ave and Spadina Ave / Adelaide St W.\nWhy do you think these stations have the most “round trip” rides? What additional analysis could you do to explore the potential reasons why?",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory Data Analysis and Visualization in pandas"
    ]
  }
]