[
  {
    "objectID": "outline-example.html",
    "href": "outline-example.html",
    "title": "<img src='https://raw.githubusercontent.com/schoolofcities/gta-immigration/refs/heads/main/src/assets/top-logo-full.svg'  style='height:auto;width:220px'></img><br>Urban Data Storytelling üìäüìàüèôÔ∏è",
    "section": "",
    "text": "Data storytelling (V1, V2)\nAssessment: - Describe your goals, stakeholders, narrative ideas, list of potential data, etc.\nMandatory readings: - intro to data storytelling (video) - intro to urban data (video and notebook) - measuring the city (notebook) - data ethics / literacy (videos) Optional readings (if they want a background in coding) - intro to python and jupyter (notebook)\nData analysis (V3)\nAssessment: - List each dataset you have downloaded/have access to, and briefly describe why it‚Äôs important for your work - Analyze and explore your data to provide at least 3 summaries that super important to your research and story. Describe what the data tells you. These can include, but are not limited to,summary statistics (means, percents, etc.), summary tables (e.g.¬†cross-tabs, pivot tables, etc.), correlation statistics or summary trend from a regression model, or other results from a statistical analysis\nMandatory readings: - data analysis and processing (notebook) - spatial data and gis (notebook) Optional readings (pick and choose depending on which relate to your project): - statistics fundementals (notebook) - intro to census data (notebook) - intro to openstreetmap data (notebook) - REV - spatial data in python (notebook) - spatial data processing (notebook)\nData visualization (V4, V5)\nAssessment: - Create at least 3 maps and/or charts of your data that help communicate patterns and help tell your story. Try to make at least 1 non-spatial chart and at least 1 map/spatial data visualization.\nMandatory readings: - data visualization (notebook) - maps and spatial data visualization (notebook) Optional readings (pick and choose depending on which relate to your project): - exploratory data viz in Python (notebook) - choropleth maps (notebook) - proportional symbol maps (notebook) - mapping density (notebook) (might add 1 or 2 more in this section depending on time)"
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html",
    "href": "notebooks/urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html",
    "title": "Data ethics and literacy",
    "section": "",
    "text": "Data ethics and literacy\nInsert videos and any text?",
    "crumbs": [
      "Urban Data Analytics",
      "Data ethics and literacy"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "National censuses, like the Canadian census, are very common data sources analyzing demographic and socio-economic data pertaining to specific places.\nStatistics Canada conducts a national census of the population every five years, asking a range of demographic and socio-economic questions. The results paint a demographic portrait of the country at the time period the census was conducted.\nThe most recent census at the time of writing was in 2021. Lots of census data are publicly available for download, across the following topics:\n\nAge\nCommuting to work\nEducation\nEthnocultural and religious diversity\nFamilies, households, and marital status\nHousing\nImmigration, place of birth, and citizenship\nIncome\nIndigenous peoples\nLabour\nLanguage and language of work\nMobility and migration\nPopulation and dwelling counts\nTypes of dwellings\n\nMost data are pre-aggregated to a variety of geographic boundaries (e.g.¬†provinces, cities, neighbourhoods, blocks, etc.), which allow for finding a variety of demographic and socio-economic statistics for specific places as well as for making a range of maps.\nFor example, here‚Äôs a map of population density in the Greater Toronto Area (GTA), clearly showing where people are clustered throughout the region.\n\n\n\nMap of population density in Toronto\n\n\nThis notebook covers:\n\nan overview of Canadian Census data\nwhere to find census data on the Statistics Canada website\nhow to explore maps of census data using CensusMapper\nhow to download census data to use in your own projects\n\n\n\nThere are two parts to the census, the short-form survey and the long-form survey. The short-form survey asks a set of basic household and demographic questions (e.g.¬†address, age, marital status, etc.) and is sent to all households in Canada. The long-from survey is sent to 25% of households in Canada. It asks additional questions pertaining to a broader range of demographic, social, and economic topics (e.g.¬†religion, education, journey to work, etc.). Statistics Canada also augments collected census survey data by joining in data from other administrative sources, including income data collected by the Canadian Revenue Agency (CRA).\nCensus data are collected primarily on a household-by-household basis (one adult member in each household usually fills out the census on behalf of everyone in the household). Data of individual responses from the census are often called census ‚Äúmicro-data‚Äù. Because of personal identification concerns, this data is only accessible by accredited researchers. (A public use microdata file called the PUMF is available though. It is a random sample of the overall population, with several of the identifying variables removed, such as home addresses and postal code).\n\n\n\nSummaries (i.e.¬†aggregations) of census data to a range of geographic areas are publicly available to view online or download. These are super useful for understanding the demographics of a place. For example, the total population in a province, the number of people who speak Spanish in Toronto, or the average income in a specific neighbourhood.\nThe Census Profile tables on Statistics Canada‚Äôs website allow for searching for census data for specific variables and geographic areas. For example, here‚Äôs an output of ‚ÄúKnowledge of Official Languages‚Äù in Ontario.\n\n\n\nTable of knowledge of language in Ontario\n\n\nWhen working with census data, it‚Äôs often advisable to use the Census Dictionary, the main reference guidebook, to understand what different data variables and geographies in the census represent. For example, here‚Äôs the entry for Knowledge of official languages.\nCensus profile data is typically limited to single categories totals (e.g.¬†number of people who speak French) by gender, as shown in the table above. However, if you are interested cross-tabulations, summaries across multiple categories (e.g.¬†number of people who have knowledge of French who also speak French at work, e.g.¬†total number low-income residents who live in different types of housing), then there are a variety of Data Tables available for this purpose.\nIf neither the Census Profile or Data Tables fit your purpose, there is also a Public Use Microdata File (PUMF). This is a non-aggregated (i.e.¬†each row is a disaggregated individual-level response) dataset covering a sample of the Canadian population. This data can be queried and cross-tabulated across any number of categories included. For privacy reasons, the data only include larger geographic linkages (e.g.¬†provinces, large metro areas), and are only a sample of the overall census.\n\n\n\nThe are a number of geographic boundaries available with associated census data, ranging in scale from city blocks to the entire country. Below is an example of commonly used boundaries for urban-scale maps and analysis.\n\n\n\nCommon census boundaries in Toronto\n\n\nEach polygon on this map has associated publicly available summary census data. Joining this tabular data to these spatial boundaries allows for making a wide range of maps showing the distribution of demographics and socio-economic variables\nYou can bulk download census data for a number of geographic levels and formats from the Statistics Canada website. These downloads are essentially copies of the Census Profile data, but for all regions noted in each row.\nOne issue to be aware of is that census boundaries can change over time each time a census is conducted. Doing a longitudinal analysis of spatial census data often requires using a technique like areal interpolation, in which data are joined to a common set of spatial units prior to analyses.\n\n\n\nExample of census tract boundaries changing in Victoria\n\n\n\n\n\nCensusMapper is a website for exploring and downloading census data across Canada. When we first land on the website, it defaults to a map of Population Density in Vancouver and it shares a number of preset options for making maps.\nIf we want to search for a specific census variable, we can click Make a Map in the top right, and then select the year (e.g.¬†2021). Here we can search and explore all available data. Using the search icon at the top-left or by clicking inset Canada map can help us to navigate elsewhere in the country. Just through a few clicks, I was able to create this map of knowledge of Portuguese in Toronto. Try making your own map!\n\n\n\nPortuguese in Toronto on CensusMapper\n\n\nWe can also use CensusMapper to download census data for specified geographic boundaries. To do so, click on API on the top right. First select the census year. Variable Selection is used for searching for and selecting the variables (i.e.¬†attribute data such as knowledge of a particular language) to download. Region Selection is the geographic area that we want download data for (e.g.¬†for Toronto). In the Overview panel, we can view what we‚Äôve selected as well as pick the geographic aggregation level (e.g.¬†Census Tracts, Dissemination Areas, etc.). Once selected, we can then download the attribute table and/or geographic boundaries.\nCensusMapper is partly built on an R library for downloading census data called cancensus. If you work with R, it is definitely worth checking out!\n\n\n\nWhile CensusMapper (and other online tools like it) are great for exploring and downloading data, we often want to make more customized maps (e.g.¬†for a report, a paper, a website, etc.) or analyze census data in conjunction with other data sources (e.g.¬†comparing demographic data to the location of libraries, public transit, or grocery stores, etc.).\nTo do so, the general process is to first download census data directly from the Statistics Canada website above or from CensusMapper and then load it into whatever software or library that you are working with.\nLINK TO CHROPLETH TUTORIAL AND OTHERS?",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#overview-of-the-canadian-census",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#overview-of-the-canadian-census",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "There are two parts to the census, the short-form survey and the long-form survey. The short-form survey asks a set of basic household and demographic questions (e.g.¬†address, age, marital status, etc.) and is sent to all households in Canada. The long-from survey is sent to 25% of households in Canada. It asks additional questions pertaining to a broader range of demographic, social, and economic topics (e.g.¬†religion, education, journey to work, etc.). Statistics Canada also augments collected census survey data by joining in data from other administrative sources, including income data collected by the Canadian Revenue Agency (CRA).\nCensus data are collected primarily on a household-by-household basis (one adult member in each household usually fills out the census on behalf of everyone in the household). Data of individual responses from the census are often called census ‚Äúmicro-data‚Äù. Because of personal identification concerns, this data is only accessible by accredited researchers. (A public use microdata file called the PUMF is available though. It is a random sample of the overall population, with several of the identifying variables removed, such as home addresses and postal code).",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#finding-census-data",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#finding-census-data",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "Summaries (i.e.¬†aggregations) of census data to a range of geographic areas are publicly available to view online or download. These are super useful for understanding the demographics of a place. For example, the total population in a province, the number of people who speak Spanish in Toronto, or the average income in a specific neighbourhood.\nThe Census Profile tables on Statistics Canada‚Äôs website allow for searching for census data for specific variables and geographic areas. For example, here‚Äôs an output of ‚ÄúKnowledge of Official Languages‚Äù in Ontario.\n\n\n\nTable of knowledge of language in Ontario\n\n\nWhen working with census data, it‚Äôs often advisable to use the Census Dictionary, the main reference guidebook, to understand what different data variables and geographies in the census represent. For example, here‚Äôs the entry for Knowledge of official languages.\nCensus profile data is typically limited to single categories totals (e.g.¬†number of people who speak French) by gender, as shown in the table above. However, if you are interested cross-tabulations, summaries across multiple categories (e.g.¬†number of people who have knowledge of French who also speak French at work, e.g.¬†total number low-income residents who live in different types of housing), then there are a variety of Data Tables available for this purpose.\nIf neither the Census Profile or Data Tables fit your purpose, there is also a Public Use Microdata File (PUMF). This is a non-aggregated (i.e.¬†each row is a disaggregated individual-level response) dataset covering a sample of the Canadian population. This data can be queried and cross-tabulated across any number of categories included. For privacy reasons, the data only include larger geographic linkages (e.g.¬†provinces, large metro areas), and are only a sample of the overall census.",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#census-geography",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#census-geography",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "The are a number of geographic boundaries available with associated census data, ranging in scale from city blocks to the entire country. Below is an example of commonly used boundaries for urban-scale maps and analysis.\n\n\n\nCommon census boundaries in Toronto\n\n\nEach polygon on this map has associated publicly available summary census data. Joining this tabular data to these spatial boundaries allows for making a wide range of maps showing the distribution of demographics and socio-economic variables\nYou can bulk download census data for a number of geographic levels and formats from the Statistics Canada website. These downloads are essentially copies of the Census Profile data, but for all regions noted in each row.\nOne issue to be aware of is that census boundaries can change over time each time a census is conducted. Doing a longitudinal analysis of spatial census data often requires using a technique like areal interpolation, in which data are joined to a common set of spatial units prior to analyses.\n\n\n\nExample of census tract boundaries changing in Victoria",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#making-maps-with-censusmapper",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#making-maps-with-censusmapper",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "CensusMapper is a website for exploring and downloading census data across Canada. When we first land on the website, it defaults to a map of Population Density in Vancouver and it shares a number of preset options for making maps.\nIf we want to search for a specific census variable, we can click Make a Map in the top right, and then select the year (e.g.¬†2021). Here we can search and explore all available data. Using the search icon at the top-left or by clicking inset Canada map can help us to navigate elsewhere in the country. Just through a few clicks, I was able to create this map of knowledge of Portuguese in Toronto. Try making your own map!\n\n\n\nPortuguese in Toronto on CensusMapper\n\n\nWe can also use CensusMapper to download census data for specified geographic boundaries. To do so, click on API on the top right. First select the census year. Variable Selection is used for searching for and selecting the variables (i.e.¬†attribute data such as knowledge of a particular language) to download. Region Selection is the geographic area that we want download data for (e.g.¬†for Toronto). In the Overview panel, we can view what we‚Äôve selected as well as pick the geographic aggregation level (e.g.¬†Census Tracts, Dissemination Areas, etc.). Once selected, we can then download the attribute table and/or geographic boundaries.\nCensusMapper is partly built on an R library for downloading census data called cancensus. If you work with R, it is definitely worth checking out!",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#further-analysis-and-visualization-of-census-data",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#further-analysis-and-visualization-of-census-data",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "While CensusMapper (and other online tools like it) are great for exploring and downloading data, we often want to make more customized maps (e.g.¬†for a report, a paper, a website, etc.) or analyze census data in conjunction with other data sources (e.g.¬†comparing demographic data to the location of libraries, public transit, or grocery stores, etc.).\nTo do so, the general process is to first download census data directly from the Statistics Canada website above or from CensusMapper and then load it into whatever software or library that you are working with.\nLINK TO CHROPLETH TUTORIAL AND OTHERS?",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html",
    "title": "Introduction to urban data",
    "section": "",
    "text": "[Insert video here]\n\n\nIn data analysis, understanding the type and origin of data is essential to choosing the right methods for analysis, limitations in the data, and interpreting results. The following categories outline common types of data sources that we often encounter. These sources vary in how and why they are collected.\n\n\n\n\n\n\n\n\nData source type\nDescription\nExamples\n\n\n\n\nDesigned/Survey\nData collected through surveys, experiments, or designed methods.\nCensus data (historical or current), research surveys, opinion polls\n\n\nAdministrative\nData from routine operations or official records. Often open when government-run.\nCity records (public housing locations, public transit data), tax records, healthcare data (historical and current), school enrollment data\n\n\nCrowdsourced\nData contributed by the public or community-driven platforms.\nOpenStreetMap (OSM), Wikipedia, social media data, 311 data, Google reviews\n\n\nEvent-Driven/Real-Time\nData generated from sensors, transactions, or interactions. Often continuous and time-sensitive.\nSatellite data, mobile phone GPS, IoT sensors, e-commerce transactions, website clicks\n\n\nDerived data\nData created by transforming or calculating metrics from existing data or simulations.\nIndices like low-income prevalence or social deprivation, environmental quality scores, simulated datasets for testing or model training\n\n\n\nSometimes data can be a combination, for example, both the United States Census Bureau and Statistics Canada collect a combination of survey data and administrative data.\n\n\n\n\nMap of average household income in Toronto using data from Statistics Canada (2020)\n\n\n\n\n\nData availability refers to how accessible data is and the conditions under which it can be used. Key categories include:\nPublic / Open data - Data that is freely accessible to anyone and can be reused without restrictions. Open data is often provided by governments, research institutions, and public organizations. It is typically non-sensitive and available in machine-readable formats. Examples include aggregated census data, municipal open data, and OpenStreetMap.\nRestricted data Data that is available but comes with limitations due to privacy, security, or legal concerns. This includes sensitive datasets such as health data, dis-aggregate census data, and government records with personal identifiers that are redacted or protected.\nProprietary data - Data that is owned by a specific entity (e.g., a corporation or private organization) and is not freely available. Access is typically granted through licenses, paid subscriptions, or agreements. For example, cell phone mobility data from Spectus can be used to measure post-pandemic downtown recovery trends, real estate data from Costar can be used to assess vacancy rates or rent prices, and consumer data from Data Axle can be used to study the impact of new housing on migration patterns.\n\n\n\n\nCrowdsourced OpenStreetMap data in Vancouver\n\n\n\n\n\nThe table below lists a handful of websites where you can find publicly available data about cities, the environment, land use, transportation, Indigenous communities, housing and homelessness. This is a non-exhaustive list; there are many other great data sources available. Note that municipalities‚Äô open data portals typically contain information on all or most of these topics.\n\n\n\nTopic\nData sources\n\n\n\n\nDemographic data\n- Canadian census data\n\n\nMunicipal data\n- Open data portals (e.g., Toronto, Montreal, & Vancouver)\n\n\nEnvironment\n- NASA‚Äôs Earth Science Data Systems (ESDS) Program- The Canadian Urban Environmental Health Research Consortium- Natural Resources Canada- Environment and Climate Change Canada\n\n\nLand use and built environment\n- OpenStreetMap- Land cover of Canada- Municipal-level zoning maps (e.g., in Toronto)\n\n\nTransportation\n- Metrolinx Open Data for the Greater Toronto Area- Canadian Urban Transit Association- Mobilizing Justice Hub- Transitland\n\n\nIndigenous communities\n- First Nations Data Centre- Native Land Digital\n\n\nHousing and homelessness\n- Housing - Statistics Canada- Housing data from Canada Mortgage and Housing Corporation\n\n\n\nWeb scraping, or extracting information from the internet, is another method for creating datasets. Since the data do not already exist and must be created, this can be more time-intensive than using existing datasets. However, packages like beautifulsoup or selenium in Python make this process easier.\n\n\n\nData format refers to how data is stored and structured. In practice, this is most relevant when loading and saving data. The data format you choose to use depends on the data‚Äôs size, structure, use, how it is being stored, and whether it is spatial (has a geometry column) or not.\nSome of the most common data formats for non-spatial data are:\n\nCSV (comma separated values) .csv\nExcel .xlsx\nJSON (JavaScript Object Notation) .json\nXML (Extensible Markup Language) .xml\n\nSome of the most common data formats for spatial data are (see Spatial data & GIS for more information):\n\nGeoJSON .geojson\nGeoPackage .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nWhile the file formats above suffice for relatively small or simple datasets, very large or complex datasets require more efficient storage via formats like Parquet (see instructions for Python). Relational databases are another commonly used data storage format for ‚Äúbig data‚Äù because they are more efficient, faster to query, more secure, and can be accessed by multiple users.\n\n\n\nIt‚Äôs important to make sure that each variable in your dataset is in the right format so the computer interprets it correctly. For example, if you load a .csv file with a column representing the population of a neighbourhood, you would want to make sure this variable is interpreted as a number and not a string of characters so you can easily use this column to calculate additional statistics (e.g.¬†sum of population in all neighbourhoods, population density in each neighbourhood).\nSee the table below for a list of common data types used in data analysis software.\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample\n\n\n\n\nInteger (int)\nWhole numbers without decimal points.\n5, -3, 42\n\n\nFloat\nNumbers with decimal points, representing real numbers.\n3.14, -0.001, 2.718\n\n\nString (str)\nA sequence of characters, used for textual data.\n\"Hello\", \"Data analysis\", \"123\"\n\n\nBoolean (bool)\nRepresents binary values: True or False.\nTrue, False\n\n\nList\nOrdered collection of items, can contain different data types.\n[1, 2, 3], [\"apple\", \"banana\"]\n\n\nTuple\nImmutable sequence of items, like lists but cannot be modified.\n(1, 2, 3), (\"apple\", \"banana\")\n\n\nDictionary (dict)\nCollection of key-value pairs, often used for mapping.\n{\"country\": \"Alice\", \"age\": 30}\n\n\nDateTime\nUsed for representing date and time.\n2025-04-09 14:32:00, 2021-01-01\n\n\nSet\nUnordered collection of unique items.\n{1, 2, 3}, {\"apple\", \"banana\"}\n\n\nNoneType\nRepresents the absence of a value, or null.\nNone\n\n\n\nDifferent software (e.g.¬†Excel, Python, R, QGIS, etc.) might have slightly different names for each of the above. For example, this page provides a nice summary of data types in Python.\n\n\n\nWhile data types specify what kind of data a variable can hold, levels of measurement describe how data is structured and the relationships between different values. They refer to how we can classify and interpret the data in terms of its inherent ordering, spacing, and possible mathematical operations.\n\nNominal: Categorical data with no inherent order (e.g., colors, countries, land-use types e.g.¬†Urban, Wetlands, Forest, etc.).\nOrdinal: Data with a meaningful order, but unknown or not always equal differences between values (e.g., movie ratings like Good,Okay, or Bad, or levels of education e.g.¬†High School, Bachelors, Masters ).\nInterval: Ordered data with equal intervals between values but no true zero (e.g., temperature in Celsius or Fahrenheit, datetime).\nRatio: Ordered data with equal intervals and a true zero point, allowing for meaningful ratios (e.g., length, area, income).\n\n\n\n\nThere are hundreds of software and tools for processing, analyzing, and visualizing data.\nWhen choosing which software to use to analyze or visualize data, one of the main considerations is whether the software is open source or proprietary. Open source software has its source code publicly available and can be modified by anyone on the internet. Proprietary software‚Äôs source code is not publicly available, is typically developed and updated by a closed group, and is licensed to users in exchange for payment. Read this link if you‚Äôre interested in learning more about the difference between the two.\nIn this course, we will focus on open source software because it is free and available to everyone. Below is a list of some of the main open source programming languages, software, and tools that we recommend using for data analysis and mapping.\n\n\n\nPurpose\nSoftware/Tools\n\n\n\n\nAnalyzing and visualizing data\nPython, R, SQL\n\n\nMaking pretty graphics\nInkscape, GIMP\n\n\nWeb development\nHTML, CSS, Javascript\n\n\nWeb-based maps and visualization\nD3, MapLibre, Protomaps\n\n\nHosting / project management\nGitHub\n\n\n\n\n\n\nWhile is no set of specific step-by-step instructions for data analysis ‚Äì each project involves unique data sources, variables, methodologies, and outputs ‚Äì but there is a general framework that we recommend following:\n\nDefine the problem or research question. What question are you trying to answer with data? Is data analysis the best way to answer that question? Who is the audience for your data analysis, and what do they want to know?\nCollect data. What kind of data do you need to answer your research question, and where can you find it? Does it exist? In what format?\nClean data. Make sure the data has appropriate variable names, does not have misspellings or other errors, and the variables are the correct data types. Get rid of any redundant or irrelevant data that you don‚Äôt need, and determine a method for dealing with any missing values.\nAnalyze data. Start by exploring the data to understand its structure and any statistical patterns. Then perform your analysis ‚Äì for example, are you trying to uncover trends, or measure the relationship among variables?\nVisualize data. Create plots, maps, or other visual representations that illustrate the structure, trends, or relationships present in your data.\nPresent data. Clearly communicate your results to your intended audience. This could involve writing a report, or creating a presentation or interactive dashboard. Whatever gets your message across!\n\nIt is important to iterate through some of these steps and make updates as needed. For example, if step 5 (visualize your data) reveals that you have an imbalanced dataset, you may need to go back to step 3 or 4 to address this. And of course once you hit steps 5. and 6., you might find something super interesting in your data that you‚Äôll want to collect more data and repeat the process all over again! :)\n\n\n\nLearning new software for data analysis or mapping can be confusing and frustrating. Luckily, there are a lot of great resources that can help!\nThe first place you should look when you‚Äôre confused about how to do something is the official documentation. For example, if you‚Äôre having trouble loading a CSV file in Python using the pandas package, take a look at the documentation for .read_csv on the pandas website. Or if you‚Äôre not sure how to create a spatial buffer in QGIS, check out the QGIS buffer operations page.\nIf you‚Äôre still stuck on a question, Google it! Chances are, someone else has dealt with a similar issue, and there is likely a community of people helping them out. For example, one of the most popular resources for coding is Stack Overflow, a website where programmers ask and answer questions. Responses with the most votes are shown at the top, making it easy to find helpful code snippets and explanations that you can adapt for your own needs. The website is so widely used that Stack Overflow posts will usually show up towards the top when you Google search coding questions.\nThere are also websites like W3Schools and GeeksforGeeks that offer online courses and tutorials covering everything from sorting a list in Python to building complicated statistical models. These websites show up often as results for relevant Google searches.\nAnd of course there are AI chatbots like ChatGPT. These tools can be extremely helpful for debugging code, writing code, or providing instructions for GIS. However, be careful and don‚Äôt trust them blindly, as they are often wrong, and sometimes make up packages or functions that don‚Äôt exist. Also, if you use chatbots for help, make sure you understand what they are telling you. Asking for guidance or hints about specific, discrete questions is much better than asking the chatbot to write an entire Python script for you. The more you rely on chatbots, the less you will learn, and the less you will be able to do on your own. Learning data analysis or coding in particular can feel like an uphill battle, but if you start with a solid foundation and thorough understanding of how it works, the better you will be able to prompt and efficiently use chatbots, and importantly you will have a stronger base to then tackle complicated problems in the future.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#where-does-data-come-from",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#where-does-data-come-from",
    "title": "Introduction to urban data",
    "section": "",
    "text": "In data analysis, understanding the type and origin of data is essential to choosing the right methods for analysis, limitations in the data, and interpreting results. The following categories outline common types of data sources that we often encounter. These sources vary in how and why they are collected.\n\n\n\n\n\n\n\n\nData source type\nDescription\nExamples\n\n\n\n\nDesigned/Survey\nData collected through surveys, experiments, or designed methods.\nCensus data (historical or current), research surveys, opinion polls\n\n\nAdministrative\nData from routine operations or official records. Often open when government-run.\nCity records (public housing locations, public transit data), tax records, healthcare data (historical and current), school enrollment data\n\n\nCrowdsourced\nData contributed by the public or community-driven platforms.\nOpenStreetMap (OSM), Wikipedia, social media data, 311 data, Google reviews\n\n\nEvent-Driven/Real-Time\nData generated from sensors, transactions, or interactions. Often continuous and time-sensitive.\nSatellite data, mobile phone GPS, IoT sensors, e-commerce transactions, website clicks\n\n\nDerived data\nData created by transforming or calculating metrics from existing data or simulations.\nIndices like low-income prevalence or social deprivation, environmental quality scores, simulated datasets for testing or model training\n\n\n\nSometimes data can be a combination, for example, both the United States Census Bureau and Statistics Canada collect a combination of survey data and administrative data.\n\n\n\n\nMap of average household income in Toronto using data from Statistics Canada (2020)",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-availability",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-availability",
    "title": "Introduction to urban data",
    "section": "",
    "text": "Data availability refers to how accessible data is and the conditions under which it can be used. Key categories include:\nPublic / Open data - Data that is freely accessible to anyone and can be reused without restrictions. Open data is often provided by governments, research institutions, and public organizations. It is typically non-sensitive and available in machine-readable formats. Examples include aggregated census data, municipal open data, and OpenStreetMap.\nRestricted data Data that is available but comes with limitations due to privacy, security, or legal concerns. This includes sensitive datasets such as health data, dis-aggregate census data, and government records with personal identifiers that are redacted or protected.\nProprietary data - Data that is owned by a specific entity (e.g., a corporation or private organization) and is not freely available. Access is typically granted through licenses, paid subscriptions, or agreements. For example, cell phone mobility data from Spectus can be used to measure post-pandemic downtown recovery trends, real estate data from Costar can be used to assess vacancy rates or rent prices, and consumer data from Data Axle can be used to study the impact of new housing on migration patterns.\n\n\n\n\nCrowdsourced OpenStreetMap data in Vancouver",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-sources-for-urban-analysis",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-sources-for-urban-analysis",
    "title": "Introduction to urban data",
    "section": "",
    "text": "The table below lists a handful of websites where you can find publicly available data about cities, the environment, land use, transportation, Indigenous communities, housing and homelessness. This is a non-exhaustive list; there are many other great data sources available. Note that municipalities‚Äô open data portals typically contain information on all or most of these topics.\n\n\n\nTopic\nData sources\n\n\n\n\nDemographic data\n- Canadian census data\n\n\nMunicipal data\n- Open data portals (e.g., Toronto, Montreal, & Vancouver)\n\n\nEnvironment\n- NASA‚Äôs Earth Science Data Systems (ESDS) Program- The Canadian Urban Environmental Health Research Consortium- Natural Resources Canada- Environment and Climate Change Canada\n\n\nLand use and built environment\n- OpenStreetMap- Land cover of Canada- Municipal-level zoning maps (e.g., in Toronto)\n\n\nTransportation\n- Metrolinx Open Data for the Greater Toronto Area- Canadian Urban Transit Association- Mobilizing Justice Hub- Transitland\n\n\nIndigenous communities\n- First Nations Data Centre- Native Land Digital\n\n\nHousing and homelessness\n- Housing - Statistics Canada- Housing data from Canada Mortgage and Housing Corporation\n\n\n\nWeb scraping, or extracting information from the internet, is another method for creating datasets. Since the data do not already exist and must be created, this can be more time-intensive than using existing datasets. However, packages like beautifulsoup or selenium in Python make this process easier.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-formats",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-formats",
    "title": "Introduction to urban data",
    "section": "",
    "text": "Data format refers to how data is stored and structured. In practice, this is most relevant when loading and saving data. The data format you choose to use depends on the data‚Äôs size, structure, use, how it is being stored, and whether it is spatial (has a geometry column) or not.\nSome of the most common data formats for non-spatial data are:\n\nCSV (comma separated values) .csv\nExcel .xlsx\nJSON (JavaScript Object Notation) .json\nXML (Extensible Markup Language) .xml\n\nSome of the most common data formats for spatial data are (see Spatial data & GIS for more information):\n\nGeoJSON .geojson\nGeoPackage .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nWhile the file formats above suffice for relatively small or simple datasets, very large or complex datasets require more efficient storage via formats like Parquet (see instructions for Python). Relational databases are another commonly used data storage format for ‚Äúbig data‚Äù because they are more efficient, faster to query, more secure, and can be accessed by multiple users.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-types",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-types",
    "title": "Introduction to urban data",
    "section": "",
    "text": "It‚Äôs important to make sure that each variable in your dataset is in the right format so the computer interprets it correctly. For example, if you load a .csv file with a column representing the population of a neighbourhood, you would want to make sure this variable is interpreted as a number and not a string of characters so you can easily use this column to calculate additional statistics (e.g.¬†sum of population in all neighbourhoods, population density in each neighbourhood).\nSee the table below for a list of common data types used in data analysis software.\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample\n\n\n\n\nInteger (int)\nWhole numbers without decimal points.\n5, -3, 42\n\n\nFloat\nNumbers with decimal points, representing real numbers.\n3.14, -0.001, 2.718\n\n\nString (str)\nA sequence of characters, used for textual data.\n\"Hello\", \"Data analysis\", \"123\"\n\n\nBoolean (bool)\nRepresents binary values: True or False.\nTrue, False\n\n\nList\nOrdered collection of items, can contain different data types.\n[1, 2, 3], [\"apple\", \"banana\"]\n\n\nTuple\nImmutable sequence of items, like lists but cannot be modified.\n(1, 2, 3), (\"apple\", \"banana\")\n\n\nDictionary (dict)\nCollection of key-value pairs, often used for mapping.\n{\"country\": \"Alice\", \"age\": 30}\n\n\nDateTime\nUsed for representing date and time.\n2025-04-09 14:32:00, 2021-01-01\n\n\nSet\nUnordered collection of unique items.\n{1, 2, 3}, {\"apple\", \"banana\"}\n\n\nNoneType\nRepresents the absence of a value, or null.\nNone\n\n\n\nDifferent software (e.g.¬†Excel, Python, R, QGIS, etc.) might have slightly different names for each of the above. For example, this page provides a nice summary of data types in Python.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#levels-of-measurement",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#levels-of-measurement",
    "title": "Introduction to urban data",
    "section": "",
    "text": "While data types specify what kind of data a variable can hold, levels of measurement describe how data is structured and the relationships between different values. They refer to how we can classify and interpret the data in terms of its inherent ordering, spacing, and possible mathematical operations.\n\nNominal: Categorical data with no inherent order (e.g., colors, countries, land-use types e.g.¬†Urban, Wetlands, Forest, etc.).\nOrdinal: Data with a meaningful order, but unknown or not always equal differences between values (e.g., movie ratings like Good,Okay, or Bad, or levels of education e.g.¬†High School, Bachelors, Masters ).\nInterval: Ordered data with equal intervals between values but no true zero (e.g., temperature in Celsius or Fahrenheit, datetime).\nRatio: Ordered data with equal intervals and a true zero point, allowing for meaningful ratios (e.g., length, area, income).",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#software-and-tools",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#software-and-tools",
    "title": "Introduction to urban data",
    "section": "",
    "text": "There are hundreds of software and tools for processing, analyzing, and visualizing data.\nWhen choosing which software to use to analyze or visualize data, one of the main considerations is whether the software is open source or proprietary. Open source software has its source code publicly available and can be modified by anyone on the internet. Proprietary software‚Äôs source code is not publicly available, is typically developed and updated by a closed group, and is licensed to users in exchange for payment. Read this link if you‚Äôre interested in learning more about the difference between the two.\nIn this course, we will focus on open source software because it is free and available to everyone. Below is a list of some of the main open source programming languages, software, and tools that we recommend using for data analysis and mapping.\n\n\n\nPurpose\nSoftware/Tools\n\n\n\n\nAnalyzing and visualizing data\nPython, R, SQL\n\n\nMaking pretty graphics\nInkscape, GIMP\n\n\nWeb development\nHTML, CSS, Javascript\n\n\nWeb-based maps and visualization\nD3, MapLibre, Protomaps\n\n\nHosting / project management\nGitHub",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-analysis-process",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-analysis-process",
    "title": "Introduction to urban data",
    "section": "",
    "text": "While is no set of specific step-by-step instructions for data analysis ‚Äì each project involves unique data sources, variables, methodologies, and outputs ‚Äì but there is a general framework that we recommend following:\n\nDefine the problem or research question. What question are you trying to answer with data? Is data analysis the best way to answer that question? Who is the audience for your data analysis, and what do they want to know?\nCollect data. What kind of data do you need to answer your research question, and where can you find it? Does it exist? In what format?\nClean data. Make sure the data has appropriate variable names, does not have misspellings or other errors, and the variables are the correct data types. Get rid of any redundant or irrelevant data that you don‚Äôt need, and determine a method for dealing with any missing values.\nAnalyze data. Start by exploring the data to understand its structure and any statistical patterns. Then perform your analysis ‚Äì for example, are you trying to uncover trends, or measure the relationship among variables?\nVisualize data. Create plots, maps, or other visual representations that illustrate the structure, trends, or relationships present in your data.\nPresent data. Clearly communicate your results to your intended audience. This could involve writing a report, or creating a presentation or interactive dashboard. Whatever gets your message across!\n\nIt is important to iterate through some of these steps and make updates as needed. For example, if step 5 (visualize your data) reveals that you have an imbalanced dataset, you may need to go back to step 3 or 4 to address this. And of course once you hit steps 5. and 6., you might find something super interesting in your data that you‚Äôll want to collect more data and repeat the process all over again! :)",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#getting-help",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#getting-help",
    "title": "Introduction to urban data",
    "section": "",
    "text": "Learning new software for data analysis or mapping can be confusing and frustrating. Luckily, there are a lot of great resources that can help!\nThe first place you should look when you‚Äôre confused about how to do something is the official documentation. For example, if you‚Äôre having trouble loading a CSV file in Python using the pandas package, take a look at the documentation for .read_csv on the pandas website. Or if you‚Äôre not sure how to create a spatial buffer in QGIS, check out the QGIS buffer operations page.\nIf you‚Äôre still stuck on a question, Google it! Chances are, someone else has dealt with a similar issue, and there is likely a community of people helping them out. For example, one of the most popular resources for coding is Stack Overflow, a website where programmers ask and answer questions. Responses with the most votes are shown at the top, making it easy to find helpful code snippets and explanations that you can adapt for your own needs. The website is so widely used that Stack Overflow posts will usually show up towards the top when you Google search coding questions.\nThere are also websites like W3Schools and GeeksforGeeks that offer online courses and tutorials covering everything from sorting a list in Python to building complicated statistical models. These websites show up often as results for relevant Google searches.\nAnd of course there are AI chatbots like ChatGPT. These tools can be extremely helpful for debugging code, writing code, or providing instructions for GIS. However, be careful and don‚Äôt trust them blindly, as they are often wrong, and sometimes make up packages or functions that don‚Äôt exist. Also, if you use chatbots for help, make sure you understand what they are telling you. Asking for guidance or hints about specific, discrete questions is much better than asking the chatbot to write an entire Python script for you. The more you rely on chatbots, the less you will learn, and the less you will be able to do on your own. Learning data analysis or coding in particular can feel like an uphill battle, but if you start with a solid foundation and thorough understanding of how it works, the better you will be able to prompt and efficiently use chatbots, and importantly you will have a stronger base to then tackle complicated problems in the future.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "This notebook provides an introduction for analyzing urban data. It will cover ‚Ä¶\n\nExploring, filtering, and sorting data\nCleaning data and removing missing data\nCreating new columns from existing data\nJoining data from multiple tables\nComputing descriptive statistics (sum, mean, etc.)\nAggregating data via cross-tabulations and pivot tables\n\nTo do this, we‚Äôll primarily be using pandas, a Python library for analyzing and organizing tabular data.\nIf you haven‚Äôt installed pandas you‚Äôll have to install it.\n\n!pip install pandas\n\nNext, let‚Äôs import the pandas library using the pd alias. An alias in Python is an alternate name for a library that‚Äôs typically shorter and easier to reference in the code later on.\n\nimport pandas as pd\n\npandas is probably the most common library for working with both big and small datasets in Python, and is the basis for working with more analytical packages (e.g.¬†numpy, scipy, scikit-learn) and analyzing geographic data (e.g.¬†geopandas). For each section, we‚Äôll also link to relevant documentation for doing similar tasks in Microsoft Excel and Google Sheets.\nHere are download links to this notebook and example datasets.\n\nDownload Notebook\n\ncities.csv\n\ncapitals.csv\n\nnew_york_cities.csv\n\n\n\nA very common way of structuring and analyzing a dataset is in a 2-dimensional table format, where rows are individual records or observations, while columns are attributes (often called variables) about those observations. For example, rows could each be a city and columns could indicate the population for different time periods. Data stored in spreadsheets often take this format.\nIn pandas, a DataFrame is a tabular data structure similar to a spreadsheet, where data is organized in rows and columns. Columns can contain different kinds of data, such as numbers, strings, dates, and so on. When we load data in pandas, we typically load it into the structure of a DataFrame.\nLet‚Äôs first take a look at a small dataset, Canadian municipalities and their population in 2021 and 2016, based on Census data. In Statistics Canada lingo, these are called Census Subdivisions. This dataset only includes municipalities with a population greater than 25,000 in 2021.\nThe main method for loading csv data is to use the read_csv function, but pandas can also read and write many other data formats.\n\ndf = pd.read_csv(\"data/cities.csv\")\n\nGreat! Now our data is stored in the variable df in the structure of a DataFrame.\n\n\n\nIn spreadsheet software, when we open a data file, we will see the top rows of the data right away.\nIn pandas, we can simply type the name of the DataFrame, in this case df, in the cell to view it. By default, it will print the top and bottom rows in the DataFrame\n\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nWinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nWe can specify which rows we want to view.\nLet‚Äôs explore what this data frame looks like. Adding the function .head(N) or .tail(N) prints the top or bottom N rows of the DataFrame. The following prints the first 10 rows.\nTry to edit this to print the bottom 10 rows or a different number of rows.\n\ndf.head(10)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n7\nBlainville\nQue.\n59819.0\nNaN\n\n\n8\nBoisbriand\nQue.\n28308.0\n26884.0\n\n\n9\nBoucherville\nQue.\n41743.0\n41671.0\n\n\n\n\n\n\n\nNotice that each column has a unique name. We can view the data of this column alone by using that name, and see what unique values exist using .unique().\nTry viewing the data of another column. Beware of upper and lower case ‚Äì exact names matter!\n\ndf['Prov/terr'].head(10)  # Top 10 only\n\n0     B.C.\n1    Alta.\n2     Ont.\n3     Que.\n4     Ont.\n5     Ont.\n6     Ont.\n7     Que.\n8     Que.\n9     Que.\nName: Prov/terr, dtype: object\n\n\n\ndf['Prov/terr'].unique()  # Unique values for the *full* dataset - what happens if you do df['Prov/terr'].head(10).unique()?\n\narray(['B.C.', 'Alta.', 'Ont.', 'Que.', 'Man.', nan, 'N.S.', 'P.E.I.',\n       'N.L.', 'N.B.', 'Sask.', 'Y.T.'], dtype=object)\n\n\n\n\n\nWe often want to look at only a portion of our data that fit some set of conditions (e.g.¬†all cities in a province that have a population more than 100,000). This is often called filtering, querying, or subsetting a dataset.\nLet‚Äôs try to do some filtering in pandas with our data of Canadian cities. Check out these links for filtering and sorting in spreadsheet software:\n\nFiltering in Excel\nFiltering in Google Sheets\n\nWe can use the columns to identify data that we might want to filter by. The line below shows data only for Ontario, but see if you can filter for another province or territory.\n\ndf.loc[df['Prov/terr'] == 'Ont.']\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n10\nBradford West Gwillimbury\nOnt.\n42880.0\n35325.0\n\n\n...\n...\n...\n...\n...\n\n\n171\nWhitby\nOnt.\n138501.0\n128377.0\n\n\n172\nWhitchurch-Stouffville\nOnt.\n49864.0\n45837.0\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n67 rows √ó 4 columns\n\n\n\nPandas allows us to use other similar mathematical concepts filter for data. Previously, we asked for all data in Ontario.\nNow, filter for all cities which had a population of at least 100,000 in 2021.\nHINT: in Python, ‚Äúgreater than or equals to‚Äù (i.e., ‚Äúat least‚Äù) is represented using the syntax &gt;=.\nPandas also allows us to combine filtering conditions.\nUse the template below to select for all cities in Ontario with a population of over 100,000 in 2021.\n\ndf.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n\n\n  Cell In[8], line 1\n    df.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n                                          ^\nSyntaxError: invalid syntax. Perhaps you forgot a comma?\n\n\n\n\nNow let‚Äôs count how many cities actually meet these conditions. Run the line below to see how many cities there are in this data set in Ontario.\n\ndf.loc[df['Prov/terr'] == 'Ont.'].count()\n\nName                66\nProv/terr           67\nPopulation, 2021    66\nPopulation, 2016    65\ndtype: int64\n\n\nThe function .count() tells us how much data there is for each column - but if we wanted to just see one column, we could also filter for that individual column using df[COL_NAME].\nTry a different condition and count the amount of data for it.\n\n\n\nYou might have noticed that these cities are in alphabetical order - what if we wanted to see them in the order of population? In pandas, we do this using the sort_values function. The default is to sort in ascending order, so we set this to be False (i.e.¬†descending) so the most populous cities are at the top.\n\ndf.sort_values(by='Population, 2021', ascending=False)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n\n\n89\nMontr√©al\nQue.\n1762949.0\n1704694.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n\n\n...\n...\n...\n...\n...\n\n\n115\nPrince Edward County\nOnt.\n25704.0\n24735.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n40\nDrummondville\nQue.\nNaN\n75423.0\n\n\n109\nPeterborough\nOnt.\nNaN\nNaN\n\n\n169\nWest Kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nLet‚Äôs put some in this together now.\nFilter the data to show all cities which are in the province of Quebec with at least a population of 50,000 in 2016, and then try to sort the cities by their 2016 population.\nHINT: You can do this in two steps (which is more readable) by storing the data that you filter into a variable called df_filtered, then running the command to sort the values on df_filtered.\n\n\n\nOnce we have processed and analyzed our data, we may want to save it for future use or share it with others. pandas makes it easy to export data to various formats, such as CSV or Excel files.\nBelow, we demonstrate how to export a DataFrame to both CSV and Excel formats. This is particularly useful for sharing results or viewing the data in other tools like spreadsheet software.\n\n# Save to CSV\ndf_filtered.to_csv('df_filtered.csv', index=False)\n\n# Save to Excel\ndf_filtered.to_excel('df_filtered.xlsx', index=False)\n\n\n\n\nOften, the data we have might not be in the condition want it to be in. Some data might be missing, and other data might have odd naming conventions.\nA simple example is that we might want all city names to be lowercase - which is what the code below does.\n\ndf['Name'] = df['Name'].str.lower()\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nabbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nairdrie\nAlta.\n74100.0\n61581.0\n\n\n2\najax\nOnt.\n126666.0\n119677.0\n\n\n3\nalma\nQue.\n30331.0\n30771.0\n\n\n4\naurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nwindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nwinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nwood buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nwoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nwoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows √ó 4 columns\n\n\n\npandas has a number of methods like str.lower() to alter data (see the full API). But the important thing to note here is that we directly modified the existing values of a column. We might not always want to do this, but often it is a good way of saving memory and shows that data frames are not just static forms but modifiable.\nLikewise, we might want better names for the columns we have. Take a look at the API for .rename() and modify the data frame df such that we rename the column Name to City. Pandas has more methods than we could ever remember - so learning to navigate the API is a crucial part of using the library.\nHINT: Take a look at the first example\n\n\n\nUnfortunately, it is pretty common that dataset we work with will be missing data values for some rows and columns. This can create complications when we want to produce summary statistics or visualizations. There are different strategies for dealing with this (i.e., imputing data), but the easiest is just to remove them. But first come out let‚Äôs check how much data is missing.\n\ndf.isnull().sum()\n\nName                3\nProv/terr           4\nPopulation, 2021    3\nPopulation, 2016    4\ndtype: int64\n\n\nIt seems that each column has a couple of data points missing. Let‚Äôs take a look at which rows these occur in. Similar to how we created a condition to filter for certain data, the code below creates a condition to filter for rows with missing data.\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n7\nblainville\nQue.\n59819.0\nNaN\n\n\n18\ncaledon\nNaN\n76581.0\n66502.0\n\n\n30\nNaN\nOnt.\n101427.0\n92013.0\n\n\n40\ndrummondville\nQue.\nNaN\n75423.0\n\n\n51\ngrimsby\nOnt.\n28883.0\nNaN\n\n\n64\nla prairie\nNaN\n26406.0\n24110.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n86\nmission\nNaN\n41519.0\n38554.0\n\n\n109\npeterborough\nOnt.\nNaN\nNaN\n\n\n131\nNaN\nQue.\n29954.0\n27359.0\n\n\n157\ntimmins\nNaN\n41145.0\n41788.0\n\n\n169\nwest kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n\n\n\nYou can see that some rows are missing multiple values, While others are just missing one. We can remove rows which have missing data using the function dropna and assign it to df so we‚Äôre working with complete data only going forward. Try to modify the code below to drop rows whose empty values are in one of the two population columns - that is, if the name or province is missing, we want to keep that row still. Look at the API to figure this out, specifically the argument subset for .dropna()\n\ndf = df.dropna()\n\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n\n\n\n\n\nGreat. Now let‚Äôs reset to our original data frame and exclude any rows with missing values.\n\ndf = pd.read_csv(\"data/cities.csv\")\ndf = df.dropna()\n\nInstead of dropping (i.e.¬†removing) rows with missing values, we can instead replace them with specific values with fillna. For example, df.fillna(0) would replace all missing data values with a 0. This wouldn‚Äôt make sense in our data of Canadian cities, but can be useful in other contexts.\nSimilarly we can promgramatically find and replace data in any other column or across our dataset. For example, if we wanted to rename 'Y.T.' to 'Yukon' we would run the replace function as so df = df.replace('Y.T.', 'Yukon')\n\n\nWe can add or delete columns as needed. Let‚Äôs first add a column which shows the change in population between 2021 and 2016 and then sort by the cities that lost the most people. We can calculate that via a simple subtraction as follows.\n\ndf[\"pop_change\"] = df[\"Population, 2021\"] - df[\"Population, 2016\"]\ndf.sort_values(\"pop_change\", ascending = False).head(5)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\npop_change\n\n\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n83206.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n77811.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n67564.0\n\n\n11\nBrampton\nOnt.\n656480.0\n593638.0\n62842.0\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n62785.0\n\n\n\n\n\n\n\nPandas supports mathematical equations between columns, just like the subtraction we did above. Create a new column called pct_pop_change that computes the percentage change in population between 2016 and 2021, and sort by the cities with the greatest increase.\nHINT: the way to compute percent change is 100 * (Y - X) / X.\nNow let‚Äôs clear these new columns out using drop to return to what we had originally.\n\ndf = df.drop(columns=['pop_change', 'pct_pop_change'])\n\n\n\n\nMuch of the time, we are working with multiple sets of data which may overlap in key ways. Perhaps we want to include measures of income in cities, or look at voting patterns - this may require us to combine multiple data frames so we can analyze them.\nPandas methods to combine data frames are quite similar to that of database operations - if you‚Äôve worked with SQL before, this will come very easily to you. There‚Äôs an extensive tutorial on this topic, but we‚Äôll focus on simple cases of pd.concat() and pd.merge(). If you are curious about how to do this in spreadsheet software like Excel, check out this tutorial\nFirst, we can use pd.concat() to stack DataFrames vertically when new data has the same columns but additional rows (e.g., adding cities from another region). This is like adding new entries to a database table.\n\ndf_ny_cities = pd.read_csv(\"./data/new_york_cities.csv\")\ncombined = pd.concat([df, df_ny_cities], ignore_index=True)\nprint(\"Original rows:\", len(df), \"| After append:\", len(combined))\ndisplay(combined.tail(5))  # Show new rows\n\nOriginal rows: 167 | After append: 169\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n164\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n165\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n166\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n167\nNew York City\nN.Y.\n8804190.0\n8537673.0\n\n\n168\nBuffalo\nN.Y.\n278349.0\n258071.0\n\n\n\n\n\n\n\nSecond, we can use pd_merge() (or pd.concat(axis=1)) to combine DataFrames side-by-side when they share a key column (e.g., city names). Here, we‚Äôll a column denoting whether the city is a provincial capital by matching city names.\nLet‚Äôs first load this data:\n\ndf_capitals = pd.read_csv('./data/capitals.csv')\ndf_capitals\n\n\n\n\n\n\n\n\nName\nIs_Capital\n\n\n\n\n0\nToronto\nTrue\n\n\n1\nQu√©bec\nTrue\n\n\n2\nVictoria\nTrue\n\n\n3\nEdmonton\nTrue\n\n\n4\nWinnipeg\nTrue\n\n\n5\nFredericton\nTrue\n\n\n6\nHalifax\nTrue\n\n\n7\nCharlottetown\nTrue\n\n\n8\nSt. John's\nTrue\n\n\n9\nRegina\nTrue\n\n\n10\nWhitehorse\nTrue\n\n\n\n\n\n\n\n\ndf_with_capitals = pd.merge(\n    df,  # Original data\n    df_capitals,  # New data\n    on=\"Name\",  # The same column \n    how=\"left\"  # Keep all data from the \"left\", ie. original\n)\n\n# Set non-capitals to False\ndf_with_capitals[\"Is_Capital\"] = df_with_capitals[\"Is_Capital\"].astype('boolean').fillna(False)\n\n# Verify: Show capitals and counts\nprint(f\"Found {df_with_capitals['Is_Capital'].sum()} capitals:\")\ndf_with_capitals[df_with_capitals[\"Is_Capital\"]]\n\nFound 11 capitals:\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\nIs_Capital\n\n\n\n\n23\nCharlottetown\nP.E.I.\n38809.0\n36094.0\nTrue\n\n\n38\nEdmonton\nAlta.\n1010899.0\n933088.0\nTrue\n\n\n41\nFredericton\nN.B.\n63116.0\n58721.0\nTrue\n\n\n49\nHalifax\nN.S.\n439819.0\n403131.0\nTrue\n\n\n108\nQu√©bec\nQue.\n549459.0\n531902.0\nTrue\n\n\n111\nRegina\nSask.\n226404.0\n215106.0\nTrue\n\n\n139\nSt. John's\nN.L.\n110525.0\n108860.0\nTrue\n\n\n147\nToronto\nOnt.\n2794356.0\n2731571.0\nTrue\n\n\n154\nVictoria\nB.C.\n91867.0\n85792.0\nTrue\n\n\n161\nWhitehorse\nY.T.\n28201.0\n25085.0\nTrue\n\n\n163\nWinnipeg\nMan.\n749607.0\n705244.0\nTrue\n\n\n\n\n\n\n\n\n\n\n\nThe data we have is only as good as we understand what‚Äôs going on. There‚Äôs some basic methods in pandas we can use to get an idea of what the data says.\nThe most basic function you can use to get an idea of what‚Äôs going on is .describe(). It shows you how much data there is, and a number of summary statistics.\nModify the code below to report summary statistics for cities in Quebec only.\n\ndf.describe()\n\n\n\n\n\n\n\n\nPopulation, 2021\nPopulation, 2016\n\n\n\n\ncount\n1.670000e+02\n1.670000e+02\n\n\nmean\n1.573169e+05\n1.487358e+05\n\n\nstd\n3.074452e+05\n2.959960e+05\n\n\nmin\n2.570400e+04\n2.378700e+04\n\n\n25%\n3.770050e+04\n3.449850e+04\n\n\n50%\n6.414100e+04\n6.316600e+04\n\n\n75%\n1.348910e+05\n1.255940e+05\n\n\nmax\n2.794356e+06\n2.731571e+06\n\n\n\n\n\n\n\nInstead of picking out an examining a subset of the data one by one, we can use the function .groupby(). Given a column name, it will group rows which have the same value. In the example below, that means grouping every row which has the same province name. We can then apply a function to this (or multiple functions using .agg()) to examine different aspects of the data.\n\n# Group by province and calculate total population\nprovince_pop = df.groupby('Prov/terr')['Population, 2021'].sum()\nprint(\"Total population by province:\")\nprovince_pop.sort_values(ascending=False)\n\nTotal population by province:\n\n\nProv/terr\nOnt.      11598308.0\nQue.       5474109.0\nB.C.       3662922.0\nAlta.      3192892.0\nMan.        800920.0\nSask.       563966.0\nN.S.        533513.0\nN.B.        240595.0\nN.L.        137693.0\nP.E.I.       38809.0\nY.T.         28201.0\nName: Population, 2021, dtype: float64\n\n\n\n# Multiple aggregation statistics\nstats = df.groupby('Prov/terr')['Population, 2021'].agg(['count', 'mean', 'max', 'sum'])\nstats\n\n\n\n\n\n\n\n\ncount\nmean\nmax\nsum\n\n\nProv/terr\n\n\n\n\n\n\n\n\nAlta.\n17\n187817.176471\n1306784.0\n3192892.0\n\n\nB.C.\n29\n126307.655172\n662248.0\n3662922.0\n\n\nMan.\n2\n400460.000000\n749607.0\n800920.0\n\n\nN.B.\n4\n60148.750000\n79470.0\n240595.0\n\n\nN.L.\n2\n68846.500000\n110525.0\n137693.0\n\n\nN.S.\n2\n266756.500000\n439819.0\n533513.0\n\n\nOnt.\n64\n181223.562500\n2794356.0\n11598308.0\n\n\nP.E.I.\n1\n38809.000000\n38809.0\n38809.0\n\n\nQue.\n41\n133514.853659\n1762949.0\n5474109.0\n\n\nSask.\n4\n140991.500000\n266141.0\n563966.0\n\n\nY.T.\n1\n28201.000000\n28201.0\n28201.0\n\n\n\n\n\n\n\nBelow, we‚Äôve added a column which shows the percent growth for each city. Use .groupby('Prov/terr') and use the function .median() (just as we used .sum() above) to observe the median growth rate per province or territory. Make sure to use sort_values() and set ascending to False.\n\ndf['Percent_Growth'] = 100 * (df['Population, 2021'] - df['Population, 2016']) / df['Population, 2016'] \n\n\n\n\nA cross tabulation, also called a frequency table or a contingency table, is a table that shows the summarizes two categorical variables by displaying the number of occurrences in each pair of categories.\nLet‚Äôs show an example by counting the number of cities in each province by a categorization of city size.\nWe will need two tools to do this: - cut, which bins continuous numbers (like population) into categories (e.g., ‚ÄúSmall‚Äù/‚ÄúMedium‚Äù/‚ÄúLarge‚Äù), turning numbers into meaningful groups. - crosstab, which counts how often these groups appear together‚Äîlike how many ‚ÄúMedium‚Äù cities exist per province‚Äîrevealing patterns that raw numbers hide.\n\n# Create a size category column\ndf['Size'] = pd.cut(df['Population, 2021'],\n                    bins=[0, 50000, 200000, float('inf')],\n                    labels=['Small', 'Medium', 'Large'])\n\n# Cross-tab: Province vs. Size\nsize_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\nsize_table\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Create a size category column\n----&gt; 2 df['Size'] = pd.cut(df['Population, 2021'],\n      3                     bins=[0, 50000, 200000, float('inf')],\n      4                     labels=['Small', 'Medium', 'Large'])\n      6 # Cross-tab: Province vs. Size\n      7 size_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\n\nNameError: name 'pd' is not defined\n\n\n\nRecall that we just created the column 'Percent_Growth' as well. Use these two functions to create different bins for different levels of growth and cross tabulate them.\nIf you‚Äôve worked with Excel or Google Sheets, this is very similar to doing a Pivot Table.\nPivot tables are able to summarize data across several categories, and for different types of summaries (e.g.¬†sum, mean, median, etc.)\nThe pivot_table function in pandas to aggregate data, and has more options than the crosstab function. Both are quite similar though. Here‚Äôs an example where we are using pivot_table to sum the population in each province by the 3 size groups.\nTry to edit this to compute the mean or median city Percent_Growth\n\npd.pivot_table(data = df, values = ['Population, 2021'], index = ['Prov/terr'], columns = ['Size'], aggfunc = 'sum', observed=True)\n\n\n\n\n\n\n\n\nPopulation, 2021\n\n\nSize\nSmall\nMedium\nLarge\n\n\nProv/terr\n\n\n\n\n\n\n\nAlta.\n234664.0\n640545.0\n2317683.0\n\n\nB.C.\n330537.0\n1642753.0\n1689632.0\n\n\nMan.\nNaN\n51313.0\n749607.0\n\n\nN.B.\n28114.0\n212481.0\nNaN\n\n\nN.L.\n27168.0\n110525.0\nNaN\n\n\nN.S.\nNaN\n93694.0\n439819.0\n\n\nOnt.\n930812.0\n2925641.0\n7741855.0\n\n\nP.E.I.\n38809.0\nNaN\nNaN\n\n\nQue.\n806267.0\n1371544.0\n3296298.0\n\n\nSask.\n71421.0\nNaN\n492545.0\n\n\nY.T.\n28201.0\nNaN\nNaN\n\n\n\n\n\n\n\nThere are often multiple ways to achieve similar results. In the previous section we looked at the groupby function to summarize data by one column, while above we used crosstab or pivot_table. However, we could also use the groupby function for this purpose. Check out the example below.\nYou should notice that it has created a long-table formate rather than a wide-table format. Both formats can be useful, wide-table formats are easier for viewing data for only 2 categories, while long-table formats are often used for inputting data into modelling or visualization libraries.\n\ndf.groupby(['Prov/terr', 'Size'], observed=False)['Population, 2021'].agg(['sum'])\n\n\n\n\n\n\n\n\n\nsum\n\n\nProv/terr\nSize\n\n\n\n\n\nAlta.\nSmall\n234664.0\n\n\nMedium\n640545.0\n\n\nLarge\n2317683.0\n\n\nB.C.\nSmall\n330537.0\n\n\nMedium\n1642753.0\n\n\nLarge\n1689632.0\n\n\nMan.\nSmall\n0.0\n\n\nMedium\n51313.0\n\n\nLarge\n749607.0\n\n\nN.B.\nSmall\n28114.0\n\n\nMedium\n212481.0\n\n\nLarge\n0.0\n\n\nN.L.\nSmall\n27168.0\n\n\nMedium\n110525.0\n\n\nLarge\n0.0\n\n\nN.S.\nSmall\n0.0\n\n\nMedium\n93694.0\n\n\nLarge\n439819.0\n\n\nOnt.\nSmall\n930812.0\n\n\nMedium\n2925641.0\n\n\nLarge\n7741855.0\n\n\nP.E.I.\nSmall\n38809.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0\n\n\nQue.\nSmall\n806267.0\n\n\nMedium\n1371544.0\n\n\nLarge\n3296298.0\n\n\nSask.\nSmall\n71421.0\n\n\nMedium\n0.0\n\n\nLarge\n492545.0\n\n\nY.T.\nSmall\n28201.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#tables-dataframes",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#tables-dataframes",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "A very common way of structuring and analyzing a dataset is in a 2-dimensional table format, where rows are individual records or observations, while columns are attributes (often called variables) about those observations. For example, rows could each be a city and columns could indicate the population for different time periods. Data stored in spreadsheets often take this format.\nIn pandas, a DataFrame is a tabular data structure similar to a spreadsheet, where data is organized in rows and columns. Columns can contain different kinds of data, such as numbers, strings, dates, and so on. When we load data in pandas, we typically load it into the structure of a DataFrame.\nLet‚Äôs first take a look at a small dataset, Canadian municipalities and their population in 2021 and 2016, based on Census data. In Statistics Canada lingo, these are called Census Subdivisions. This dataset only includes municipalities with a population greater than 25,000 in 2021.\nThe main method for loading csv data is to use the read_csv function, but pandas can also read and write many other data formats.\n\ndf = pd.read_csv(\"data/cities.csv\")\n\nGreat! Now our data is stored in the variable df in the structure of a DataFrame.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#viewing-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#viewing-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "In spreadsheet software, when we open a data file, we will see the top rows of the data right away.\nIn pandas, we can simply type the name of the DataFrame, in this case df, in the cell to view it. By default, it will print the top and bottom rows in the DataFrame\n\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nWinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nWe can specify which rows we want to view.\nLet‚Äôs explore what this data frame looks like. Adding the function .head(N) or .tail(N) prints the top or bottom N rows of the DataFrame. The following prints the first 10 rows.\nTry to edit this to print the bottom 10 rows or a different number of rows.\n\ndf.head(10)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n7\nBlainville\nQue.\n59819.0\nNaN\n\n\n8\nBoisbriand\nQue.\n28308.0\n26884.0\n\n\n9\nBoucherville\nQue.\n41743.0\n41671.0\n\n\n\n\n\n\n\nNotice that each column has a unique name. We can view the data of this column alone by using that name, and see what unique values exist using .unique().\nTry viewing the data of another column. Beware of upper and lower case ‚Äì exact names matter!\n\ndf['Prov/terr'].head(10)  # Top 10 only\n\n0     B.C.\n1    Alta.\n2     Ont.\n3     Que.\n4     Ont.\n5     Ont.\n6     Ont.\n7     Que.\n8     Que.\n9     Que.\nName: Prov/terr, dtype: object\n\n\n\ndf['Prov/terr'].unique()  # Unique values for the *full* dataset - what happens if you do df['Prov/terr'].head(10).unique()?\n\narray(['B.C.', 'Alta.', 'Ont.', 'Que.', 'Man.', nan, 'N.S.', 'P.E.I.',\n       'N.L.', 'N.B.', 'Sask.', 'Y.T.'], dtype=object)",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#filtering-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#filtering-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "We often want to look at only a portion of our data that fit some set of conditions (e.g.¬†all cities in a province that have a population more than 100,000). This is often called filtering, querying, or subsetting a dataset.\nLet‚Äôs try to do some filtering in pandas with our data of Canadian cities. Check out these links for filtering and sorting in spreadsheet software:\n\nFiltering in Excel\nFiltering in Google Sheets\n\nWe can use the columns to identify data that we might want to filter by. The line below shows data only for Ontario, but see if you can filter for another province or territory.\n\ndf.loc[df['Prov/terr'] == 'Ont.']\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n10\nBradford West Gwillimbury\nOnt.\n42880.0\n35325.0\n\n\n...\n...\n...\n...\n...\n\n\n171\nWhitby\nOnt.\n138501.0\n128377.0\n\n\n172\nWhitchurch-Stouffville\nOnt.\n49864.0\n45837.0\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n67 rows √ó 4 columns\n\n\n\nPandas allows us to use other similar mathematical concepts filter for data. Previously, we asked for all data in Ontario.\nNow, filter for all cities which had a population of at least 100,000 in 2021.\nHINT: in Python, ‚Äúgreater than or equals to‚Äù (i.e., ‚Äúat least‚Äù) is represented using the syntax &gt;=.\nPandas also allows us to combine filtering conditions.\nUse the template below to select for all cities in Ontario with a population of over 100,000 in 2021.\n\ndf.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n\n\n  Cell In[8], line 1\n    df.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n                                          ^\nSyntaxError: invalid syntax. Perhaps you forgot a comma?\n\n\n\n\nNow let‚Äôs count how many cities actually meet these conditions. Run the line below to see how many cities there are in this data set in Ontario.\n\ndf.loc[df['Prov/terr'] == 'Ont.'].count()\n\nName                66\nProv/terr           67\nPopulation, 2021    66\nPopulation, 2016    65\ndtype: int64\n\n\nThe function .count() tells us how much data there is for each column - but if we wanted to just see one column, we could also filter for that individual column using df[COL_NAME].\nTry a different condition and count the amount of data for it.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#sorting-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#sorting-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "You might have noticed that these cities are in alphabetical order - what if we wanted to see them in the order of population? In pandas, we do this using the sort_values function. The default is to sort in ascending order, so we set this to be False (i.e.¬†descending) so the most populous cities are at the top.\n\ndf.sort_values(by='Population, 2021', ascending=False)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n\n\n89\nMontr√©al\nQue.\n1762949.0\n1704694.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n\n\n...\n...\n...\n...\n...\n\n\n115\nPrince Edward County\nOnt.\n25704.0\n24735.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n40\nDrummondville\nQue.\nNaN\n75423.0\n\n\n109\nPeterborough\nOnt.\nNaN\nNaN\n\n\n169\nWest Kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nLet‚Äôs put some in this together now.\nFilter the data to show all cities which are in the province of Quebec with at least a population of 50,000 in 2016, and then try to sort the cities by their 2016 population.\nHINT: You can do this in two steps (which is more readable) by storing the data that you filter into a variable called df_filtered, then running the command to sort the values on df_filtered.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#exporting-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#exporting-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Once we have processed and analyzed our data, we may want to save it for future use or share it with others. pandas makes it easy to export data to various formats, such as CSV or Excel files.\nBelow, we demonstrate how to export a DataFrame to both CSV and Excel formats. This is particularly useful for sharing results or viewing the data in other tools like spreadsheet software.\n\n# Save to CSV\ndf_filtered.to_csv('df_filtered.csv', index=False)\n\n# Save to Excel\ndf_filtered.to_excel('df_filtered.xlsx', index=False)",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#updating-and-renaming-columns",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#updating-and-renaming-columns",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Often, the data we have might not be in the condition want it to be in. Some data might be missing, and other data might have odd naming conventions.\nA simple example is that we might want all city names to be lowercase - which is what the code below does.\n\ndf['Name'] = df['Name'].str.lower()\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nabbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nairdrie\nAlta.\n74100.0\n61581.0\n\n\n2\najax\nOnt.\n126666.0\n119677.0\n\n\n3\nalma\nQue.\n30331.0\n30771.0\n\n\n4\naurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nwindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nwinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nwood buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nwoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nwoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows √ó 4 columns\n\n\n\npandas has a number of methods like str.lower() to alter data (see the full API). But the important thing to note here is that we directly modified the existing values of a column. We might not always want to do this, but often it is a good way of saving memory and shows that data frames are not just static forms but modifiable.\nLikewise, we might want better names for the columns we have. Take a look at the API for .rename() and modify the data frame df such that we rename the column Name to City. Pandas has more methods than we could ever remember - so learning to navigate the API is a crucial part of using the library.\nHINT: Take a look at the first example",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#handling-missing-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#handling-missing-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Unfortunately, it is pretty common that dataset we work with will be missing data values for some rows and columns. This can create complications when we want to produce summary statistics or visualizations. There are different strategies for dealing with this (i.e., imputing data), but the easiest is just to remove them. But first come out let‚Äôs check how much data is missing.\n\ndf.isnull().sum()\n\nName                3\nProv/terr           4\nPopulation, 2021    3\nPopulation, 2016    4\ndtype: int64\n\n\nIt seems that each column has a couple of data points missing. Let‚Äôs take a look at which rows these occur in. Similar to how we created a condition to filter for certain data, the code below creates a condition to filter for rows with missing data.\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n7\nblainville\nQue.\n59819.0\nNaN\n\n\n18\ncaledon\nNaN\n76581.0\n66502.0\n\n\n30\nNaN\nOnt.\n101427.0\n92013.0\n\n\n40\ndrummondville\nQue.\nNaN\n75423.0\n\n\n51\ngrimsby\nOnt.\n28883.0\nNaN\n\n\n64\nla prairie\nNaN\n26406.0\n24110.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n86\nmission\nNaN\n41519.0\n38554.0\n\n\n109\npeterborough\nOnt.\nNaN\nNaN\n\n\n131\nNaN\nQue.\n29954.0\n27359.0\n\n\n157\ntimmins\nNaN\n41145.0\n41788.0\n\n\n169\nwest kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n\n\n\nYou can see that some rows are missing multiple values, While others are just missing one. We can remove rows which have missing data using the function dropna and assign it to df so we‚Äôre working with complete data only going forward. Try to modify the code below to drop rows whose empty values are in one of the two population columns - that is, if the name or province is missing, we want to keep that row still. Look at the API to figure this out, specifically the argument subset for .dropna()\n\ndf = df.dropna()\n\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n\n\n\n\n\nGreat. Now let‚Äôs reset to our original data frame and exclude any rows with missing values.\n\ndf = pd.read_csv(\"data/cities.csv\")\ndf = df.dropna()\n\nInstead of dropping (i.e.¬†removing) rows with missing values, we can instead replace them with specific values with fillna. For example, df.fillna(0) would replace all missing data values with a 0. This wouldn‚Äôt make sense in our data of Canadian cities, but can be useful in other contexts.\nSimilarly we can promgramatically find and replace data in any other column or across our dataset. For example, if we wanted to rename 'Y.T.' to 'Yukon' we would run the replace function as so df = df.replace('Y.T.', 'Yukon')\n\n\nWe can add or delete columns as needed. Let‚Äôs first add a column which shows the change in population between 2021 and 2016 and then sort by the cities that lost the most people. We can calculate that via a simple subtraction as follows.\n\ndf[\"pop_change\"] = df[\"Population, 2021\"] - df[\"Population, 2016\"]\ndf.sort_values(\"pop_change\", ascending = False).head(5)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\npop_change\n\n\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n83206.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n77811.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n67564.0\n\n\n11\nBrampton\nOnt.\n656480.0\n593638.0\n62842.0\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n62785.0\n\n\n\n\n\n\n\nPandas supports mathematical equations between columns, just like the subtraction we did above. Create a new column called pct_pop_change that computes the percentage change in population between 2016 and 2021, and sort by the cities with the greatest increase.\nHINT: the way to compute percent change is 100 * (Y - X) / X.\nNow let‚Äôs clear these new columns out using drop to return to what we had originally.\n\ndf = df.drop(columns=['pop_change', 'pct_pop_change'])\n\n\n\n\nMuch of the time, we are working with multiple sets of data which may overlap in key ways. Perhaps we want to include measures of income in cities, or look at voting patterns - this may require us to combine multiple data frames so we can analyze them.\nPandas methods to combine data frames are quite similar to that of database operations - if you‚Äôve worked with SQL before, this will come very easily to you. There‚Äôs an extensive tutorial on this topic, but we‚Äôll focus on simple cases of pd.concat() and pd.merge(). If you are curious about how to do this in spreadsheet software like Excel, check out this tutorial\nFirst, we can use pd.concat() to stack DataFrames vertically when new data has the same columns but additional rows (e.g., adding cities from another region). This is like adding new entries to a database table.\n\ndf_ny_cities = pd.read_csv(\"./data/new_york_cities.csv\")\ncombined = pd.concat([df, df_ny_cities], ignore_index=True)\nprint(\"Original rows:\", len(df), \"| After append:\", len(combined))\ndisplay(combined.tail(5))  # Show new rows\n\nOriginal rows: 167 | After append: 169\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n164\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n165\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n166\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n167\nNew York City\nN.Y.\n8804190.0\n8537673.0\n\n\n168\nBuffalo\nN.Y.\n278349.0\n258071.0\n\n\n\n\n\n\n\nSecond, we can use pd_merge() (or pd.concat(axis=1)) to combine DataFrames side-by-side when they share a key column (e.g., city names). Here, we‚Äôll a column denoting whether the city is a provincial capital by matching city names.\nLet‚Äôs first load this data:\n\ndf_capitals = pd.read_csv('./data/capitals.csv')\ndf_capitals\n\n\n\n\n\n\n\n\nName\nIs_Capital\n\n\n\n\n0\nToronto\nTrue\n\n\n1\nQu√©bec\nTrue\n\n\n2\nVictoria\nTrue\n\n\n3\nEdmonton\nTrue\n\n\n4\nWinnipeg\nTrue\n\n\n5\nFredericton\nTrue\n\n\n6\nHalifax\nTrue\n\n\n7\nCharlottetown\nTrue\n\n\n8\nSt. John's\nTrue\n\n\n9\nRegina\nTrue\n\n\n10\nWhitehorse\nTrue\n\n\n\n\n\n\n\n\ndf_with_capitals = pd.merge(\n    df,  # Original data\n    df_capitals,  # New data\n    on=\"Name\",  # The same column \n    how=\"left\"  # Keep all data from the \"left\", ie. original\n)\n\n# Set non-capitals to False\ndf_with_capitals[\"Is_Capital\"] = df_with_capitals[\"Is_Capital\"].astype('boolean').fillna(False)\n\n# Verify: Show capitals and counts\nprint(f\"Found {df_with_capitals['Is_Capital'].sum()} capitals:\")\ndf_with_capitals[df_with_capitals[\"Is_Capital\"]]\n\nFound 11 capitals:\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\nIs_Capital\n\n\n\n\n23\nCharlottetown\nP.E.I.\n38809.0\n36094.0\nTrue\n\n\n38\nEdmonton\nAlta.\n1010899.0\n933088.0\nTrue\n\n\n41\nFredericton\nN.B.\n63116.0\n58721.0\nTrue\n\n\n49\nHalifax\nN.S.\n439819.0\n403131.0\nTrue\n\n\n108\nQu√©bec\nQue.\n549459.0\n531902.0\nTrue\n\n\n111\nRegina\nSask.\n226404.0\n215106.0\nTrue\n\n\n139\nSt. John's\nN.L.\n110525.0\n108860.0\nTrue\n\n\n147\nToronto\nOnt.\n2794356.0\n2731571.0\nTrue\n\n\n154\nVictoria\nB.C.\n91867.0\n85792.0\nTrue\n\n\n161\nWhitehorse\nY.T.\n28201.0\n25085.0\nTrue\n\n\n163\nWinnipeg\nMan.\n749607.0\n705244.0\nTrue",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#summary-statistics",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#summary-statistics",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "The data we have is only as good as we understand what‚Äôs going on. There‚Äôs some basic methods in pandas we can use to get an idea of what the data says.\nThe most basic function you can use to get an idea of what‚Äôs going on is .describe(). It shows you how much data there is, and a number of summary statistics.\nModify the code below to report summary statistics for cities in Quebec only.\n\ndf.describe()\n\n\n\n\n\n\n\n\nPopulation, 2021\nPopulation, 2016\n\n\n\n\ncount\n1.670000e+02\n1.670000e+02\n\n\nmean\n1.573169e+05\n1.487358e+05\n\n\nstd\n3.074452e+05\n2.959960e+05\n\n\nmin\n2.570400e+04\n2.378700e+04\n\n\n25%\n3.770050e+04\n3.449850e+04\n\n\n50%\n6.414100e+04\n6.316600e+04\n\n\n75%\n1.348910e+05\n1.255940e+05\n\n\nmax\n2.794356e+06\n2.731571e+06\n\n\n\n\n\n\n\nInstead of picking out an examining a subset of the data one by one, we can use the function .groupby(). Given a column name, it will group rows which have the same value. In the example below, that means grouping every row which has the same province name. We can then apply a function to this (or multiple functions using .agg()) to examine different aspects of the data.\n\n# Group by province and calculate total population\nprovince_pop = df.groupby('Prov/terr')['Population, 2021'].sum()\nprint(\"Total population by province:\")\nprovince_pop.sort_values(ascending=False)\n\nTotal population by province:\n\n\nProv/terr\nOnt.      11598308.0\nQue.       5474109.0\nB.C.       3662922.0\nAlta.      3192892.0\nMan.        800920.0\nSask.       563966.0\nN.S.        533513.0\nN.B.        240595.0\nN.L.        137693.0\nP.E.I.       38809.0\nY.T.         28201.0\nName: Population, 2021, dtype: float64\n\n\n\n# Multiple aggregation statistics\nstats = df.groupby('Prov/terr')['Population, 2021'].agg(['count', 'mean', 'max', 'sum'])\nstats\n\n\n\n\n\n\n\n\ncount\nmean\nmax\nsum\n\n\nProv/terr\n\n\n\n\n\n\n\n\nAlta.\n17\n187817.176471\n1306784.0\n3192892.0\n\n\nB.C.\n29\n126307.655172\n662248.0\n3662922.0\n\n\nMan.\n2\n400460.000000\n749607.0\n800920.0\n\n\nN.B.\n4\n60148.750000\n79470.0\n240595.0\n\n\nN.L.\n2\n68846.500000\n110525.0\n137693.0\n\n\nN.S.\n2\n266756.500000\n439819.0\n533513.0\n\n\nOnt.\n64\n181223.562500\n2794356.0\n11598308.0\n\n\nP.E.I.\n1\n38809.000000\n38809.0\n38809.0\n\n\nQue.\n41\n133514.853659\n1762949.0\n5474109.0\n\n\nSask.\n4\n140991.500000\n266141.0\n563966.0\n\n\nY.T.\n1\n28201.000000\n28201.0\n28201.0\n\n\n\n\n\n\n\nBelow, we‚Äôve added a column which shows the percent growth for each city. Use .groupby('Prov/terr') and use the function .median() (just as we used .sum() above) to observe the median growth rate per province or territory. Make sure to use sort_values() and set ascending to False.\n\ndf['Percent_Growth'] = 100 * (df['Population, 2021'] - df['Population, 2016']) / df['Population, 2016']",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#cross-tabulations-and-pivot-tables",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#cross-tabulations-and-pivot-tables",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "A cross tabulation, also called a frequency table or a contingency table, is a table that shows the summarizes two categorical variables by displaying the number of occurrences in each pair of categories.\nLet‚Äôs show an example by counting the number of cities in each province by a categorization of city size.\nWe will need two tools to do this: - cut, which bins continuous numbers (like population) into categories (e.g., ‚ÄúSmall‚Äù/‚ÄúMedium‚Äù/‚ÄúLarge‚Äù), turning numbers into meaningful groups. - crosstab, which counts how often these groups appear together‚Äîlike how many ‚ÄúMedium‚Äù cities exist per province‚Äîrevealing patterns that raw numbers hide.\n\n# Create a size category column\ndf['Size'] = pd.cut(df['Population, 2021'],\n                    bins=[0, 50000, 200000, float('inf')],\n                    labels=['Small', 'Medium', 'Large'])\n\n# Cross-tab: Province vs. Size\nsize_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\nsize_table\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Create a size category column\n----&gt; 2 df['Size'] = pd.cut(df['Population, 2021'],\n      3                     bins=[0, 50000, 200000, float('inf')],\n      4                     labels=['Small', 'Medium', 'Large'])\n      6 # Cross-tab: Province vs. Size\n      7 size_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\n\nNameError: name 'pd' is not defined\n\n\n\nRecall that we just created the column 'Percent_Growth' as well. Use these two functions to create different bins for different levels of growth and cross tabulate them.\nIf you‚Äôve worked with Excel or Google Sheets, this is very similar to doing a Pivot Table.\nPivot tables are able to summarize data across several categories, and for different types of summaries (e.g.¬†sum, mean, median, etc.)\nThe pivot_table function in pandas to aggregate data, and has more options than the crosstab function. Both are quite similar though. Here‚Äôs an example where we are using pivot_table to sum the population in each province by the 3 size groups.\nTry to edit this to compute the mean or median city Percent_Growth\n\npd.pivot_table(data = df, values = ['Population, 2021'], index = ['Prov/terr'], columns = ['Size'], aggfunc = 'sum', observed=True)\n\n\n\n\n\n\n\n\nPopulation, 2021\n\n\nSize\nSmall\nMedium\nLarge\n\n\nProv/terr\n\n\n\n\n\n\n\nAlta.\n234664.0\n640545.0\n2317683.0\n\n\nB.C.\n330537.0\n1642753.0\n1689632.0\n\n\nMan.\nNaN\n51313.0\n749607.0\n\n\nN.B.\n28114.0\n212481.0\nNaN\n\n\nN.L.\n27168.0\n110525.0\nNaN\n\n\nN.S.\nNaN\n93694.0\n439819.0\n\n\nOnt.\n930812.0\n2925641.0\n7741855.0\n\n\nP.E.I.\n38809.0\nNaN\nNaN\n\n\nQue.\n806267.0\n1371544.0\n3296298.0\n\n\nSask.\n71421.0\nNaN\n492545.0\n\n\nY.T.\n28201.0\nNaN\nNaN\n\n\n\n\n\n\n\nThere are often multiple ways to achieve similar results. In the previous section we looked at the groupby function to summarize data by one column, while above we used crosstab or pivot_table. However, we could also use the groupby function for this purpose. Check out the example below.\nYou should notice that it has created a long-table formate rather than a wide-table format. Both formats can be useful, wide-table formats are easier for viewing data for only 2 categories, while long-table formats are often used for inputting data into modelling or visualization libraries.\n\ndf.groupby(['Prov/terr', 'Size'], observed=False)['Population, 2021'].agg(['sum'])\n\n\n\n\n\n\n\n\n\nsum\n\n\nProv/terr\nSize\n\n\n\n\n\nAlta.\nSmall\n234664.0\n\n\nMedium\n640545.0\n\n\nLarge\n2317683.0\n\n\nB.C.\nSmall\n330537.0\n\n\nMedium\n1642753.0\n\n\nLarge\n1689632.0\n\n\nMan.\nSmall\n0.0\n\n\nMedium\n51313.0\n\n\nLarge\n749607.0\n\n\nN.B.\nSmall\n28114.0\n\n\nMedium\n212481.0\n\n\nLarge\n0.0\n\n\nN.L.\nSmall\n27168.0\n\n\nMedium\n110525.0\n\n\nLarge\n0.0\n\n\nN.S.\nSmall\n0.0\n\n\nMedium\n93694.0\n\n\nLarge\n439819.0\n\n\nOnt.\nSmall\n930812.0\n\n\nMedium\n2925641.0\n\n\nLarge\n7741855.0\n\n\nP.E.I.\nSmall\n38809.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0\n\n\nQue.\nSmall\n806267.0\n\n\nMedium\n1371544.0\n\n\nLarge\n3296298.0\n\n\nSask.\nSmall\n71421.0\n\n\nMedium\n0.0\n\n\nLarge\n492545.0\n\n\nY.T.\nSmall\n28201.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html",
    "title": "Spatial data in Python",
    "section": "",
    "text": "There are a variety of libraries, methods, and functions for working with spatial data in Python.\nGeoPandas is a library for working with spatial (typically geographic) data. It is used a lot since it extends the functionality of pandas to support spatial data types and operations, making it easier to analyze, visualize, and manipulate spatial data.\nMany of the tasks that are typically done within a GIS can be done via geopandas. And often, it is preferable to work in geopandas versus point-and-click GIS (e.g.¬†QGIS) since it‚Äôs easier to chain together and automate processes, have access to the full capabilities of pandas and other Python libraries, as well as can be better scaled for handling very large datasets.\nThis tutorial provides and introduction to geopandas, its gemometry data types, how to quickly plot spatial data, and methods for spatial calculations (e.g.¬†computing areas, bounding boxes, etc.). The last section of this tutorial provides direction for next steps, including libraries and examples for doing advanced spatial data processing, analyses, and visualizations.\nHere are the links to download this notebook and example data: - Notebook - Toronto transit routes - Toronto transit stops - Toronto transit stops - Toronto wards\nThis tutorial assumes you have base knowledge of working with spatial data (e.g.¬†in GIS) and working with dataframes in pandas. Check out the following if you want a refresher: - Spatial data in GIS - Data analytics and processing\nWe‚Äôll begin this tutorial by loading in pandas, geopandas, and matplotlib (the latter for showing how to quickly plot and view data). If you don‚Äôt have these installed already, you‚Äôll have to install them via pip or conda.\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#loading-and-exploring-geometric-data",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#loading-and-exploring-geometric-data",
    "title": "Spatial data in Python",
    "section": "Loading and exploring geometric data",
    "text": "Loading and exploring geometric data\nGeospatial data represents real-world features using three primary geometric types: - Points: Single (x,y) coordinates for discrete locations like transit stops or landmarks. - Lines: Connected sequences of points forming paths, such as roads or rivers. - Polygons: Closed shapes defining areas like census tracts or property boundaries.\n\ntransit_stops = gpd.read_file(\"data/ttc_stops.geojson\")\ntransit_routes = gpd.read_file(\"data/ttc_routes.geojson\")\nwards = gpd.read_file(\"data/city-wards.geojson\")\n\nIn geopandas, we typically load data with a more agnostic read_file() function. For this workshop, we‚Äôre going to use four sources of data: - transit_stops: each of the stops for the TTC - transit_routes: each of the lines for the TTC - wards: polygons of City of Toronto wards (i.e.¬†council districts)\nLet‚Äôs take a look at the first layer, the transit stops. We have two columns with text, and a third with geometry data.\nHere the data are coded as a MULTILINESTRING, essentially a combination of lines that combine into one object.\n\ntransit_routes\n\n\n\n\n\n\n\n\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nExisting\nSubway\nLine 4: Sheppard Subway\nMULTILINESTRING ((-79.41092 43.76152, -79.4096...\n\n\n1\nExisting\nSubway\nLine 1: Yonge-University Subway\nMULTILINESTRING ((-79.52727 43.79351, -79.5261...\n\n\n2\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nMULTILINESTRING ((-79.26453 43.73227, -79.2669...\n\n\n3\nIn Delivery\nLRT / BRT\nEglinton Crosstown LRT\nMULTILINESTRING ((-79.26453 43.73227, -79.2679...\n\n\n4\nIn Delivery\nLRT / BRT\nEglinton Crosstown West Extension\nMULTILINESTRING ((-79.48726 43.68739, -79.4901...\n\n\n5\nIn Delivery\nSubway\nScarborough Subway Extension\nMULTILINESTRING ((-79.26453 43.73227, -79.2630...\n\n\n6\nIn Delivery\nLRT / BRT\nFinch West LRT\nMULTILINESTRING ((-79.49099 43.76349, -79.4922...\n\n\n7\nIn Delivery\nSubway\nOntario Line\nMULTILINESTRING ((-79.35168 43.69644, -79.3414...\n\n\n8\nIn Delivery\nSubway\nYonge North Subway Extension\nMULTILINESTRING ((-79.41557 43.77977, -79.4157...\n\n\n\n\n\n\n\nThe data can be manipulated like a regular pandas data frame. For example, if we want to filter out routes that currently do not operate, like the incomplete ‚ÄúEglinton Crosstown LRT‚Äù and defunct ‚ÄúScarborough RT‚Äù, we can do so by filtering the STATUS column.\n\ntransit_routes = transit_routes.loc[transit_routes[\"STATUS\"] == \"Existing\"]\ntransit_routes\n\n\n\n\n\n\n\n\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nExisting\nSubway\nLine 4: Sheppard Subway\nMULTILINESTRING ((-79.41092 43.76152, -79.4096...\n\n\n1\nExisting\nSubway\nLine 1: Yonge-University Subway\nMULTILINESTRING ((-79.52727 43.79351, -79.5261...\n\n\n2\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nMULTILINESTRING ((-79.26453 43.73227, -79.2669...\n\n\n\n\n\n\n\nLet‚Äôs do the same for the stops.\nThe geometry for the stops are simpler, just POINT.\n\ntransit_stops = transit_stops.loc[transit_stops[\"STATUS\"] == \"Existing\"]\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nKipling\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.53628 43.63694)\n\n\n1\nIslington\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.5246 43.64533)\n\n\n2\nRoyal York\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.51129 43.64812)\n\n\n3\nOld Mill\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.49509 43.65008)\n\n\n4\nJane\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.48446 43.6498)\n\n\n...\n...\n...\n...\n...\n...\n\n\n69\nVaughan Metropolitan Centre\nExisting\nSubway\nLine 1: Yonge-University Subway\nPOINT (-79.52727 43.79351)\n\n\n70\nSheppard-Yonge\nExisting\nSubway\nLine 4: Sheppard Subway\nPOINT (-79.41092 43.76152)\n\n\n71\nSpadina\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.40397 43.66729)\n\n\n72\nSt. George\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.39931 43.66828)\n\n\n73\nBloor-Yonge\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.38572 43.671)\n\n\n\n\n74 rows √ó 5 columns\n\n\n\nBefore we go on, it‚Äôs important to have an idea of the metadata of geometric files that we work with. There‚Äôs two key parts to this. - CRS (Coordinate Reference System): The crs attribute defines a geodataset‚Äôs spatial ‚Äúcoordinate system‚Äù (e.g., latitude/longitude, meters-based projections). We can use it to ensure layers align‚Äîfor example, combining Toronto census tracts (EPSG:3347) with a Web Mercator basemap (EPSG:3857). - Total Bounds: The total_bounds attribute returns the min/max coordinates (xmin, ymin, xmax, ymax) of your data‚Äôs extent. It‚Äôs useful for setting map zoom levels or clipping other datasets to the same area‚Äîlike focusing a transit map on Toronto‚Äôs downtown core.\n\ntransit_stops.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ntransit_stops.total_bounds\n\narray([-79.53628,  43.63694, -79.26453,  43.79351])",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#plotting-geographic-data",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#plotting-geographic-data",
    "title": "Spatial data in Python",
    "section": "Plotting geographic data",
    "text": "Plotting geographic data\nWe explore geometry simply by plotting using .plot(). We can do this for any row, or the entire GeoDataFrame\n\ntransit_stops.plot()\n\n\n\n\n\n\n\n\nThis is the default plot, but we can tweak the colours, add multiple layers, and change some of the layout options using matplotlib, a commonly used Python plotting library. Here‚Äôs a very simple schematic of rapid transit in Toronto.\n\nfig, ax = plt.subplots(ncols = 1, figsize=(4, 4))\n\nwards.plot(\n    linewidth = 1,\n    color=\"LightGray\",\n    edgecolor=\"White\",\n    ax = ax\n)\n\ntransit_stops.plot(\n    color=\"Black\",\n    markersize = 6,\n    ax = ax\n)\n\ntransit_routes.plot(\n    linewidth = 1,\n    color=\"Black\",\n    ax = ax\n).set_axis_off()",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#interactive-exploration",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#interactive-exploration",
    "title": "Spatial data in Python",
    "section": "Interactive Exploration",
    "text": "Interactive Exploration\nGeoPandas‚Äô explore() function generates an interactive Leaflet map (like Google Maps) from your geodata. We can use it to better understand the data we are working with and how it might be viewed from the user side on a web application (e.g., Svelte).\nYou‚Äôll need to install a couple libraries in order for this to work - matplotlib, folium, and mapclassify. This can be done in the environment that you‚Äôre working in with a command like pip install folium matplotlib mapclassify.\n\ntransit_stops.explore(\n    column='NAME',\n    tiles=\"CartoDB Positron\", \n    marker_kwds={\"radius\": 7}\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#geomtery-properties",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#geomtery-properties",
    "title": "Spatial data in Python",
    "section": "Geomtery properties",
    "text": "Geomtery properties\ngeopandas has a number of functions you can run on geometry columns that can be super useful for analyzing and summarizing data. Here‚Äôs a list of a few examples of using these to create a new column of data (more can be found in the official documentation)\n\n\n\n\n\n\n\n\nProperty\nDescription\nExample\n\n\n\n\n.area\nArea of the geometry (in CRS units)\ngdf[\"area\"] = gdf.geometry.area\n\n\n.length\nLine length (perimeter length for polygons)\ngdf[\"length\"] = gdf.geometry.length\n\n\n.is_valid\nGeometry validity check\ngdf[\"valid\"] = gdf.geometry.is_valid\n\n\n.is_empty\nWhether geometry is empty\ngdf[\"empty\"] = gdf.geometry.is_empty\n\n\n.is_simple\nNo self-intersections\ngdf[\"simple\"] = gdf.geometry.is_simple\n\n\n.type\nType of geometry (e.g., Polygon)\ngdf[\"geom_type\"] = gdf.geometry.type\n\n\n.bounds\nBounding box as (minx, miny, maxx, maxy)\ngdf[\"bounds\"] = gdf.geometry.bounds",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#processing-and-analyzing-spatial-data",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#processing-and-analyzing-spatial-data",
    "title": "Spatial data in Python",
    "section": "Processing and analyzing spatial data",
    "text": "Processing and analyzing spatial data\nThis tutorial provided quick overview of how we can working with spatial data in geopandas. However, this was just scratching the surface.\nIf you‚Äôre interested the why and how of spatial data processing, i.e.¬†about covnerting spatial data from one format to another (e.g.¬†generating centroids of polygons, buffers around points, joining and linking multiple spatial datasets to each other, etc.), check out our Processing spatial data tutorial.\nBeyond that, you can check out the following Python libraries for working with spatial data:\n\nShapely (wide variety of tools for manipulation of 2d geometry data, some of which geopandas uses.)\nPysal (spatial data analysis library, including various descriptive statistics and spatial modelling functions)\nRasterio (library for working with raster data)\nFolium (more interactive maps!)",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html",
    "href": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html",
    "title": "Spatial data processing",
    "section": "",
    "text": "Spatial data processing (or geoprocessing) is the use of tools and functions to analyze and manipulate geographic data, often to create new data or information.\nIt‚Äôs like doing applied math on map data ‚Äî combining, measuring, or filtering spatial data to get new information. Here are a couple of examples:\nThese help answer spatial questions like ‚ÄúWhat‚Äôs within walking distance?‚Äù or ‚ÄúHow much green-space is inside this area?‚Äù or ‚ÄúHow many libraries are in each neighbourhood?‚Äù\nThere are many ways to interact with spatial data - ranging from getting geometric points from addresses, to intersecting a different spatial datasets. In this notebook, we‚Äôll cover a number of commonly used spatial data processing functions.\nWe‚Äôll show the examples using geopandas, however every one of these examples can easily be done in GIS software and many other libraries for working with spatial data.\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nLet‚Äôs remind ourselves of some key concepts to start with. For a broader overview, check out our notebook on Spatial data and GIS\nFirst, there are two major types of spatial data. - Vector data represents geographic features as discrete points, lines, and polygons, making it ideal for boundaries, roads, or point locations like cities. - Raster data stores information in a grid of pixels (cells), often used for continuous data like elevation models or satellite imagery.\nIn this notebook, we‚Äôll mostly be working with vector data, which consists of three common types of geometry. 1. Points are zero-dimensional coordinates (e.g., a library‚Äôs latitude/longitude). 2. Lines are one-dimensional sequences of points (e.g., rivers, roads, or transit routes). 3. Polygons are two-dimensional enclosed shapes (e.g., political boundaries, lakes, or census tracts).\nThese can be projected into different Coordinate Reference Systems (CRS), which define how spatial data maps to the Earth‚Äôs surface. For example, WGS84 (EPSG:4326) is common for global latitude/longitude coordinates, while UTM zones (e.g., EPSG:32617) minimize distortion for local measurements.\nChoosing the right CRS ensures accurate distances, areas, and spatial relationships.\nWhen we are comparing and relating multiple spatial datasets to each other, we have to ensure that they are in the same CRS.\nIn order to process spatial data we will be working with three datasets today (you can also download them by clicking on the link): - Toronto Public Libraries (TPL) branches (tpl-branch-general-information-2023.csv), which contains information about each of the libraries in the TPL - Toronto‚Äôs wards (city-wards.gpkg), each of which have a city councillor - Toronto‚Äôs regions (toronto-regions.gpkg), which are the former municipalities that were amalgamated in 1998 to make modern Toronto\ndf_tpl = pd.read_csv('./data/tpl-branch-general-information-2023.csv')\ngdf_wards = gpd.read_file('./data/city-wards.gpkg')\ngdf_regions = gpd.read_file('./data/toronto-regions.gpkg')",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#geocoding",
    "href": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#geocoding",
    "title": "Spatial data processing",
    "section": "Geocoding",
    "text": "Geocoding\nGeocoding converts addresses (e.g.¬†\"100 Queen St, Toronto, ON M5H 2N2\") into geographic coordinates (e.g.¬†[-79.3840, 43.6536]) that can be plotted on a map and analyzed in GIS or Python.\nOf course we can simply input the the string for an address into Google Map, search for it, and manually record the coordinate location. However, this wouldn‚Äôt be practical if we had a big list of addresses. Let‚Äôs take a the .csv data that we loaded for library locations. There are 112 in Toronto!\n\ndf_tpl[[\"BranchName\",\"Address\"]]\n\nA geocoding function, either in Python or GIS, typically relies on an external geocoder, a service that turns addresses or place names into map coordinates. Large companies like Google, Apple, and Mapbox provide geocoding services. However, these require an API key and often have a tiered pricing model. There are free tools as well, for example Nominatim which queries OpenStreetMap‚Äôs database. This is what we will use in this notebook.\nIn GeoPandas, the function for geocoding is geopandas.tools.geocode(). This transforms a DataFrame of addresses into a GeoDataFrame with Point geometries, enabling spatial analysis.\nFree geocoding services usually allow about one query per second - since we have over 100 library branches, you‚Äôll have to wait a little bit! :)\n\ngdf_tpl = gpd.tools.geocode(\n    df_tpl[\"Address\"],  \n    provider=\"nominatim\",      \n    user_agent=\"tpl-workshop\",\n    timeout=10\n)\n\n# Add in and fix columns\ngdf_tpl['Address'] = df_tpl['Address']\ngdf_tpl['BranchName'] = df_tpl['BranchName']\ngdf_tpl = gdf_tpl.drop(columns=['address'])\ngdf_tpl = gdf_tpl.to_crs(4326)\n\n\n## In case you don't want to run and wait for the geocoding to work, we've precomputed it and you can load\ngdf_tpl = gpd.read_file('./data/tpl.gpkg')\n\n\ngdf_tpl.head()\n\nGreat! now we can map and analyze this data relative to other spatial data",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#buffer",
    "href": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#buffer",
    "title": "Spatial data processing",
    "section": "Buffer",
    "text": "Buffer\nBuffers create zones around geometries at a specified distance\nFor example, if we wanted to map the area that is within 1km of the nearest library, we would compute a buffer with the point locations of libraries as the input using the function .buffer(N) (or accompanying methods in QGIS).\nBuffers can be super useful on their own, but they can also be important steps in analyses. For example, you could take buffers of library locations and then count the number of households within each buffer.\nTry running the code below to compute buffers for library locations, you can also test different buffer distances! :)\n\n# converts to UTM Zone 17, which is in metres\ngdf_tpl = gdf_tpl.to_crs(\"EPSG:32617\") \n\n# generate buffer at 1km (1000m)\ngdf_tpl[\"buffer_1km\"] = gdf_tpl.buffer(1000)\n\nGreat! let‚Äôs see what this looks like\n\n# Compare the wards and their center points\nax = gdf_wards.plot(alpha=0.25, edgecolor='black', color=\"pink\", linewidth=1)\ngdf_tpl.set_geometry(\"buffer_1km\").plot(ax=ax, color=\"lightblue\", edgecolor=\"blue\")",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#dissolve",
    "href": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#dissolve",
    "title": "Spatial data processing",
    "section": "Dissolve",
    "text": "Dissolve\nDissolve merges multiple features into one, either based on a shared attribute or as a single unified shape.\nYou might have noticed that many of the buffered library polygons overlapped each other. We can use the method .dissolve() from geopandas, to combine all into a single polygon.\n\ngdf_tpl_dissolved = gdf_tpl.set_geometry(\"buffer_1km\").dissolve()\n\nLet‚Äôs plot it! Notice how all buffers that were overlapping are now merged into one much larger geometry\n\nax = gdf_wards.plot(alpha=0.25, edgecolor='black', color=\"pink\", linewidth=1)\ngdf_tpl_dissolved.plot(ax=ax, color=\"lightblue\", edgecolor=\"blue\")\n\nGreat! having this as one geometry can be useful for quick question like asking how much of the city (in terms of area) is within 1km of a public library. If we did this calculation on just the buffered polygon, we would be double counting areas of overlap.\n\n# computing area of the dissolved polygon, dividing to covert to km2\ngdf_tpl_dissolved.buffer_1km.area / (1000 * 1000)",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#centroids",
    "href": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#centroids",
    "title": "Spatial data processing",
    "section": "Centroids",
    "text": "Centroids\nCentroids are the geometric center point of polygons\nComputing centroids is useful for urban analyses because it gives a single representative point for each area, which is helpful for labeling, spatial joins, spatial selections, or other further processing of spatial data. For example, we could compute the centroid of each ward in Toronto, and then try to find the closest hospital to each.\nIn geopandas we can use the method .centroid to return a Point geometry for each feature. Here‚Äôs a quick example of finding the centroids of Toronto wards\n\n# Convert to UTM Zone 17N (meters)  \ngdf_wards = gdf_wards.to_crs(\"EPSG:32617\")  \n\n# Compute centroids (as Point geometries)  \ngdf_wards[\"centroid\"] = gdf_wards.centroid  \n\n# Compare the wards and their center points\nax = gdf_wards.plot(alpha=0.25, edgecolor='black', color=\"pink\", linewidth=1)\ngdf_wards.set_geometry(\"centroid\").plot(ax=ax, color=\"purple\", markersize=10)",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#spatial-selections",
    "href": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#spatial-selections",
    "title": "Spatial data processing",
    "section": "Spatial Selections",
    "text": "Spatial Selections\nSpatial selections, or spatial queries, are ways we can filter datasets based on their locations relative to other spatial features and datasets.\nThis can have a wide variety of applications. For example, you can filter ‚Ä¶\n\nLibrary locations that are within a neighbourhood\nSchools that are within a set distance of an expressway\nStreets that cross railroad tracks\nParks that contain baseball fields\nAnd more!\n\nLet‚Äôs try the first example, libraries that are within a neighbourhood.\nLet‚Äôs load in the data, and convert to a common CRS so the data can be related to each other accurately.\n\ngdf_tpl = gpd.read_file('./data/tpl.gpkg').to_crs(\"EPSG:32617\") \ngdf_regions = gpd.read_file('./data/toronto-regions.gpkg').to_crs(\"EPSG:32617\") \n\nThe gdf_regions are the 6 former municipalities of Toronto prior to amalgamation.\n\ngdf_regions\n\nLets try to query our dataset of public library locations to find only those within SCARBOROUGH, via .intersects\n\n# Get Scarborough's polygon from regions  \nscarborough = gdf_regions[gdf_regions[\"REGION_NAME\"] == \"SCARBOROUGH\"].geometry.iloc[0]  \n\n# filter the library dataset via a spatial selection, specifically .intersects\ngdf_tpl[gdf_tpl.intersects(scarborough)]\n\nIf we wanted to simply count the number of libraries in Scarborough, we can do that too!\n\nlen(gdf_tpl[gdf_tpl.intersects(scarborough)])\n\n.intersects is just one of several spatial selection methods. Here‚Äôs a list of several options in .geopandas. QGIS and other GIS software all have the same options, but they might just have different names.\n\n\n\nMethod\nDescription\nDocs\n\n\n\n\nintersects()\nReturns True if geometries intersect\nintersects\n\n\ncontains()\nTrue if geometry completely contains the other\ncontains\n\n\nwithin()\nTrue if geometry is within another\nwithin\n\n\ncrosses()\nTrue if geometry crosses the other\ncrosses\n\n\noverlaps()\nTrue if geometries partially overlap\noverlaps\n\n\ntouches()\nTrue if geometries touch at boundary only\ntouches\n\n\nequals()\nTrue if geometries are exactly equal\nequals\n\n\ndisjoint()\nTrue if geometries have no overlap at all\ndisjoint\n\n\ncovers()\nTrue if geometry covers the other\ncovers\n\n\ncovered_by()\nTrue if geometry is covered by the other\ncovered_by\n\n\ndistance()\nReturns distance to the other geometry\ndistance",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#spatial-joins",
    "href": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#spatial-joins",
    "title": "Spatial data processing",
    "section": "Spatial Joins",
    "text": "Spatial Joins\nSpatial joins combine data from two spatial datasets based their spatial (i.e.¬†geographic) relationship, allowing you to connect attributes from different layers. This is similar to a table join, but instead of joining data based tabular data (e.g.¬†matching ID columns), we are joining data based on spatial relationships.\nFor example, you can match points (libraries) to polygons (wards) to see which administrative district contains each facility. And further, count how many libraries are in each.\nThis is similar to doing spatial selections and queries, but instead of filtering one of our input datasets, we are creating a new dataset.\n\ncrs = \"EPSG:32617\"\ngdf_tpl = gpd.read_file('./data/tpl.gpkg').to_crs(crs) \ngdf_regions = gpd.read_file('./data/toronto-regions.gpkg').to_crs(crs) \n\nGreat! let‚Äôs begin by trying to join region names to library locations.\nWe can do this using the geopandas method .sjoin() (see similar documentation for QGIS).\n\ngdf_tpl_with_regions = gpd.sjoin(\n    gdf_tpl, \n    gdf_regions[['REGION_NAME', 'geometry']], \n    how='left', \n    predicate='within'\n)\ngdf_tpl_with_regions.head()\n\nThe parameter predicate= can be one of several options for spatial relationship noted in the above section (e.g.¬†.intersects, .within, .crosses, etc.). The parameter how= is for the type of joing (e.g.¬†left, inner, right, etc.)\nView more in the geopandas documentation page for spatial joins.\nDoing a spatial join can then lead to a range of subsequent analyses, for example counting the number of libraries in each region. If you had numeric data, for example the number of books in each branch, the following count be expanded to compute sum, mean, etc. for each group.\n\ngdf_tpl_with_regions.groupby('REGION_NAME').size()",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#overlay-operations",
    "href": "notebooks/urban-data-analytics/spatial-data-processing/spatial-data-processing.html#overlay-operations",
    "title": "Spatial data processing",
    "section": "Overlay Operations",
    "text": "Overlay Operations\n*Overlay** operations take two spatial datasets and produce a new one based on the spatial relationships between their geometries.\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nintersection\nFinds only the overlapping areas between two layers\n\n\nunion\nMerges all geometries from both layers into a single combined shape\n\n\nsymmetric_difference\nKeeps geometry that is in either layer but not both\n\n\ndifference\nExtracts parts of the first layer that are not covered by the second\n\n\n\nThe image below is adapted from the full tutorial in geopandas on this topic which you can look at for more details. QGIS also has similar functions available.\n!\nIn geopandas these operations are performed using gpd.overlay()\nLet‚Äôs take a look at quick example, trying to find the portions of wards in Toronto that overlap with the Old Toronto region area.\n\ncrs = 32617\ngdf_wards = gpd.read_file('./data/city-wards.gpkg').to_crs(crs)\ngdf_regions = gpd.read_file('./data/toronto-regions.gpkg').to_crs(crs)\n\n\n# Get Old Toronto polygon\nold_toronto = gdf_regions[gdf_regions[\"REGION_NAME\"] == \"TORONTO\"]\n\n# Find intersecting wards\nintersection = gpd.overlay(gdf_wards, old_toronto, how=\"intersection\", keep_geom_type=False)\n\nintersection.plot()\n\nThis has returned the portions of wards that overlaped with the Old Toronto polygon.\nBy overlaying all the wards, we can see below that the orange area has been cut out\n\nax = gdf_wards.plot(alpha=0.5, edgecolor='white', color=\"pink\", linewidth=1)\nintersection.plot(ax=ax, edgecolor='white', color=\"orange\", linewidth=1)\ngdf_wards.plot(ax=ax, edgecolor='blue', color=\"none\", linewidth=1)",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data processing"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/choropleth-maps/choropleth-maps.html",
    "href": "notebooks/urban-data-visualization/choropleth-maps/choropleth-maps.html",
    "title": "Choropleth Maps",
    "section": "",
    "text": "Choropleth Maps",
    "crumbs": [
      "Urban Data Visualization",
      "Choropleth Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html",
    "href": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html",
    "title": "Exploratory data visualization in Python",
    "section": "",
    "text": "Data exploration is an important step in the data analysis process. It can highlight anomalies or interesting trends, and reveal how the data are distributed.\nThis notebook explains how to visually explore data by creating plots using the popular seaborn and matplotlib libraries.\nIt will include the following plots:",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#install-and-import-libraries",
    "href": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#install-and-import-libraries",
    "title": "Exploratory data visualization in Python",
    "section": "Install and import libraries",
    "text": "Install and import libraries\nIf you haven‚Äôt already done so, install the following libraries using pip:\n\n!pip install pandas\n!pip install seaborn\n!pip install matplotlib\n\nNow import them using their common aliases. Note that pyplot is a module of the matplotlib library:\n\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#load-bike-share-data",
    "href": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#load-bike-share-data",
    "title": "Exploratory data visualization in Python",
    "section": "Load bike share data",
    "text": "Load bike share data\nThe dataset we‚Äôll be working with is Bike Share Toronto ridership data from January 2022. It‚Äôs already been downloaded for you, so you can just load it below using the read_csv function from the pandas library.\nHowever, if you want to download the data yourself‚Ä¶\n\nGo to this page from the City of Toronto‚Äôs Open Data portal\nClick on the blue ‚ÄúDownload‚Äù bar towards the bottom\nDownload the bikeshare-ridership-2022 zip file\nUnzip the file and open the folder\nSave the Bike share ridership 2022-01.csv file\nMake sure you read the file using the correct filepath\n\n\ndf = pd.read_csv(\"data/Bike share ridership 2022-01.csv\")\n\nLet‚Äôs take a look at the first 5 rows of the dataframe:\n\ndf.head()\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n\n\n2\n14805112\n942\n7399\n01/01/2022 00:04\nLower Jarvis / Queens Quay E\n7686.0\n01/01/2022 00:19\nNaN\n361\nAnnual Member\n\n\n3\n14805113\n4256\n7334\n01/01/2022 00:04\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n4350\nCasual Member\n\n\n4\n14805114\n4353\n7334\n01/01/2022 00:05\nSimcoe St / Wellington St North\n7038.0\n01/01/2022 01:17\nDundas St W / Yonge St\n5074\nCasual Member\n\n\n\n\n\n\n\nWhat do you notice about the data? Which variables are you interested in exploring?",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-long-do-bike-share-trips-last",
    "href": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-long-do-bike-share-trips-last",
    "title": "Exploratory data visualization in Python",
    "section": "How long do bike share trips last?",
    "text": "How long do bike share trips last?\nLet‚Äôs start by looking at the trip duration column to see how long people are travelling when using the bike share.\nThe ‚ÄúTrip Duration‚Äù column is in seconds, which is hard to interpret ‚Äì to make it easier, let‚Äôs create a column for minutes by dividing by 60. We can then compute some simple summary statistics on the column using the describe method from pandas.\n\ndf[\"Trip Duration Minutes\"] = df[\"Trip  Duration\"] / 60\ndf[\"Trip Duration Minutes\"].describe()\n\ncount    56765.000000\nmean        14.930107\nstd        206.125166\nmin          0.000000\n25%          6.183333\n50%         10.016667\n75%         16.050000\nmax      38095.650000\nName: Trip Duration Minutes, dtype: float64\n\n\nNow we have the mean, standard deviation, and quantiles. The average trip length is about 15 minutes, which seems reasonable. However, the maximum trip length is about 635 hours! That doesn‚Äôt seem right ‚Äì when analyzing this data, we would need to make a decision about how to handle this value. For example, we might filter it out if we‚Äôre confident it‚Äôs an error. Or maybe it‚Äôs part of a larger pattern of people who forget to dock their bikes.\nThe fact that the median (the ‚Äú50%‚Äù statistic from above) is lower than the mean shows that the data are right-skewed ‚Äì most values are clustered at the lower end of the range ‚Äì and there are some large outliers.\nLet‚Äôs plot a histogram to show the distribution of shorter trips (those less than 2 hours long) using the displot function from the seaborn package. Refer to this documentation to understand each of the parameters/arguments used to create the plots below.\nWe‚Äôll start by filtering the data to only rows where the ‚ÄúTrip Duration Minutes‚Äù column value is less than or equal to 120, using loc. Note that we‚Äôre only selecting the column itself because that‚Äôs the only data we need for this plot.\n\nsns.set_style(\"whitegrid\") # set style to make plots look nicer\n\n# Filter the data\ntrips_120_duration = df.loc[df[\"Trip Duration Minutes\"] &lt;= 120, \"Trip Duration Minutes\"]\n\n# Create the plot, with 15 bins, and make it green\nsns.displot(trips_120_duration,\n            bins = 15,\n            color = \"green\"\n            ).set(title = \"Number of Trips by Trip Duration\")\n\nsns.despine() # get rid of plot borders for cleaner look\n\n\n\n\n\n\n\n\nAccording to this plot, trips are typically fairly short, with the majority of trip durations well below 20 minutes. There are a small number of trips that are longer than 40 minutes but these are relatively rare.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-does-trip-duration-vary-by-user-type",
    "href": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-does-trip-duration-vary-by-user-type",
    "title": "Exploratory data visualization in Python",
    "section": "How does trip duration vary by user type?",
    "text": "How does trip duration vary by user type?\nNext, let‚Äôs create a stacked histogram that shows trip duration for casual versus annual members. Unlike the previous plot, we‚Äôll need more than just the ‚ÄúTrip Duration Minutes‚Äù column because we‚Äôre also plotting by ‚ÄúUser Type‚Äù.\n\ntrips_120 = df.loc[df[\"Trip Duration Minutes\"] &lt;= 120]\n\nsns.displot(data=trips_120,\n            x=\"Trip Duration Minutes\",\n            multiple=\"stack\",\n            bins=15,\n            hue=\"User Type\",\n            palette=[\"pink\", \"orange\"]\n            ).set(title = \"Bike Share Trip Duration by User Type\")\n\nsns.despine()\n\n\n\n\n\n\n\n\nThis stacked histogram shows that shorter trips (&lt;20 minutes) are most common for both types of members. There are far fewer trips taken by casual members compared to annual members.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-does-the-number-of-trips-vary-by-day-for-each-user-type",
    "href": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#how-does-the-number-of-trips-vary-by-day-for-each-user-type",
    "title": "Exploratory data visualization in Python",
    "section": "How does the number of trips vary by day, for each user type?",
    "text": "How does the number of trips vary by day, for each user type?\nWhat if we want to plot the number of trips by day of the month, colored by user type? Let‚Äôs start with a kernel density plot.\nFirst we need to convert the ‚ÄúStart Time‚Äù column to a datetime object using pandas.to_datetime so that we can sort by date.\n\ndf['Start Date'] = pd.to_datetime(df['Start Time'], format='%m/%d/%Y %H:%M')\ndf_sorted_date = df.sort_values('Start Date')\ndf_sorted_120 = df_sorted_date.loc[df_sorted_date[\"Trip Duration Minutes\"] &lt;= 120]\n\nNow let‚Äôs make the plot:\n\nsns.displot(df_sorted_120, \n            x=\"Start Date\", \n            hue=\"User Type\", # try commenting this out and see what happens\n            kind=\"kde\", \n            fill=True,\n            height=6, \n            aspect=11/8.5\n            ).set(title = \"Trip Density by Date and Member Type\")\n\nsns.despine()\n\n\n\n\n\n\n\n\nIt appears that trips peak in the first half of the month, with more trips taken by annual members than casual members, and are less frequent in the later half.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#which-stations-have-the-largest-number-of-trips-that-both-start-and-end-at-that-station",
    "href": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#which-stations-have-the-largest-number-of-trips-that-both-start-and-end-at-that-station",
    "title": "Exploratory data visualization in Python",
    "section": "Which stations have the largest number of trips that both start and end at that station?",
    "text": "Which stations have the largest number of trips that both start and end at that station?\nNow let‚Äôs try creating a visualization that‚Äôs a bit more analytical. We‚Äôll answer the question: which stations have the largest number of trips that both start and end at that station?\n\n# Filter the data to trips where start station ID = end station ID\ndfs = df.loc[df[\"Start Station Id\"] == df[\"End Station Id\"]]\n\n# Group by station and count the number of trips\ndfs = dfs.groupby(\"Start Station Name\").size().reset_index(name = \"count\")\n\n# Create a bar plot of the top 20 stations, shown in descending order of trips\nsns.catplot(data= dfs.sort_values(\"count\", ascending = False).head(20), \n            x='count', \n            y='Start Station Name',\n            kind=\"bar\"\n            ).set(title = \"Number of Return Trips to the Same Station\")\n\nsns.despine()\n\n\n\n\n\n\n\n\nTommy Thompson Park is the station with the most round trip bike rides, followed by Nassau St / Bellevue Ave and Spadina Ave / Adelaide St W.\nWhy do you think these stations have the most ‚Äúround trip‚Äù rides? What additional analysis could you do to explore the potential reasons why?",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#are-bike-share-trips-more-common-when-the-weather-is-warmer",
    "href": "notebooks/urban-data-visualization/exploratory-data-visualization-python/exploratory-data-visualization-python.html#are-bike-share-trips-more-common-when-the-weather-is-warmer",
    "title": "Exploratory data visualization in Python",
    "section": "Are bike share trips more common when the weather is warmer?",
    "text": "Are bike share trips more common when the weather is warmer?\nWhat if we are curious about the relationship between bike share trips and temperature? Maybe we have a hypothesis that more people tend to use bike share when it‚Äôs warmer. Let‚Äôs test this hypothesis by loading in another dataset, which includes the daily temperature in January 2022, and joining it with our bike share data.\nLet‚Äôs load in our weather data, which was already downloaded for you from the federal government‚Äôs historical climate data website.\n\ndf_weather = pd.read_csv(\"data/toronto-historical-weather-2022.csv\")\n\n# don't hide any of the column names, even though there are a lot\npd.set_option('display.max_columns', None)\n\ndf_weather.head() # show the first 5 rows\n\n\n\n\n\n\n\n\nLongitude (x)\nLatitude (y)\nStation Name\nClimate ID\nDate/Time\nYear\nMonth\nDay\nData Quality\nMax Temp (¬∞C)\nMax Temp Flag\nMin Temp (¬∞C)\nMin Temp Flag\nMean Temp (¬∞C)\nMean Temp Flag\nHeat Deg Days (¬∞C)\nHeat Deg Days Flag\nCool Deg Days (¬∞C)\nCool Deg Days Flag\nTotal Rain (mm)\nTotal Rain Flag\nTotal Snow (cm)\nTotal Snow Flag\nTotal Precip (mm)\nTotal Precip Flag\nSnow on Grnd (cm)\nSnow on Grnd Flag\nDir of Max Gust (10s deg)\nDir of Max Gust Flag\nSpd of Max Gust (km/h)\nSpd of Max Gust Flag\n\n\n\n\n0\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-01\n2022\n1\n1\nNaN\n5.1\nNaN\n-2.1\nNaN\n1.5\nNaN\n16.5\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2.4\nNaN\nNaN\nNaN\nNaN\nM\nNaN\nM\n\n\n1\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-02\n2022\n1\n2\nNaN\n-2.1\nNaN\n-10.5\nNaN\n-6.3\nNaN\n24.3\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n2.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n2\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-03\n2022\n1\n3\nNaN\n-4.0\nNaN\n-12.9\nNaN\n-8.4\nNaN\n26.4\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n3\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-04\n2022\n1\n4\nNaN\n3.3\nNaN\n-5.7\nNaN\n-1.2\nNaN\n19.2\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n4\n-79.4\n43.67\nTORONTO CITY\n6158355\n2022-01-05\n2022\n1\n5\nNaN\n4.9\nNaN\n-4.5\nNaN\n0.2\nNaN\n17.8\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\n0.3\nNaN\n3.0\nNaN\nNaN\nM\nNaN\nM\n\n\n\n\n\n\n\nGiven that there are so many column names in the dataframe, it might also be easier to look only at the column names, instead of the first 5 rows of data:\n\ndf_weather.columns\n\nIndex(['Longitude (x)', 'Latitude (y)', 'Station Name', 'Climate ID',\n       'Date/Time', 'Year', 'Month', 'Day', 'Data Quality', 'Max Temp (¬∞C)',\n       'Max Temp Flag', 'Min Temp (¬∞C)', 'Min Temp Flag', 'Mean Temp (¬∞C)',\n       'Mean Temp Flag', 'Heat Deg Days (¬∞C)', 'Heat Deg Days Flag',\n       'Cool Deg Days (¬∞C)', 'Cool Deg Days Flag', 'Total Rain (mm)',\n       'Total Rain Flag', 'Total Snow (cm)', 'Total Snow Flag',\n       'Total Precip (mm)', 'Total Precip Flag', 'Snow on Grnd (cm)',\n       'Snow on Grnd Flag', 'Dir of Max Gust (10s deg)',\n       'Dir of Max Gust Flag', 'Spd of Max Gust (km/h)',\n       'Spd of Max Gust Flag'],\n      dtype='object')\n\n\nWe only need to keep the Date/Time and Mean Temp (¬∞C) variables, so let‚Äôs subset the dataframe to only those columns:\n\ndf_weather_simp = df_weather[[\"Date/Time\", \"Mean Temp (¬∞C)\"]]\ndf_weather_simp.head()\n\n\n\n\n\n\n\n\nDate/Time\nMean Temp (¬∞C)\n\n\n\n\n0\n2022-01-01\n1.5\n\n\n1\n2022-01-02\n-6.3\n\n\n2\n2022-01-03\n-8.4\n\n\n3\n2022-01-04\n-1.2\n\n\n4\n2022-01-05\n0.2\n\n\n\n\n\n\n\nNow let‚Äôs join the weather dataframe with our original bike share dataframe on their respective date/time columns. First, let‚Äôs remind ourselves which column in the df bike share dataframe has the date of the trip.\n\ndf.head(2) # only show the first 2 rows\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\nTrip Duration Minutes\nStart Date\nDate Only\ndate\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n72.25\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n2.10\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n\n\n\n\n\n\n\nThe Start Date column is the one we want to use ‚Äì Start Time also contains the date but we don‚Äôt need to know what time the trip started, just the date. We have to make sure that this variable has the same data type in each of the dataframes ‚Äì if not, the join (merge) won‚Äôt work.\n\ndf['Start Date'].dtype\n\ndtype('&lt;M8[ns]')\n\n\n\ndf_weather_simp['Date/Time'].dtype\n\ndtype('O')\n\n\nThe variables have different types, so we need to convert them so they‚Äôre the same type:\n\n# Create new 'Date Only' column by extracting only the date from the datetime variable in the bike share data\ndf['date'] = df['Start Date'].dt.date\n\n# Convert the 'Date/Time' variable from string to date \ndf_weather_simp.loc[:, 'Date/Time'] = pd.to_datetime(df_weather_simp['Date/Time']).dt.date\n\n\ndf['date'].dtype\n\ndtype('O')\n\n\n\ndf_weather_simp['Date/Time'].dtype\n\ndtype('O')\n\n\nNow we can merge the dataframes, using the date column in the bike share dataset and the Date/Time column in the weather dataset.\n\ndf_ridership_weather = df.merge(df_weather_simp, \n                                left_on=\"date\", \n                                right_on=\"Date/Time\")\n\ndf_ridership_weather.head(2)\n\n\n\n\n\n\n\n\nTrip Id\nTrip Duration\nStart Station Id\nStart Time\nStart Station Name\nEnd Station Id\nEnd Time\nEnd Station Name\nBike Id\nUser Type\nTrip Duration Minutes\nStart Date\nDate Only\ndate\nDate/Time\nMean Temp (¬∞C)\n\n\n\n\n0\n14805109\n4335\n7334\n01/01/2022 00:02\nSimcoe St / Wellington St North\n7269.0\n01/01/2022 01:15\nToronto Eaton Centre (Yonge St)\n5139\nCasual Member\n72.25\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n2022-01-01\n1.5\n\n\n1\n14805110\n126\n7443\n01/01/2022 00:02\nDundas St E / George St\n7270.0\n01/01/2022 00:05\nChurch St / Dundas St E - SMART\n3992\nAnnual Member\n2.10\n2022-01-01 00:02:00\n2022-01-01\n2022-01-01\n2022-01-01\n1.5\n\n\n\n\n\n\n\nLet‚Äôs first explore the average temperature for each day in January 2022.\n\nfig, ax = plt.subplots(figsize=(11, 5))\n\nsns.lineplot(data=df_ridership_weather, \n             x='date', \n             y='Mean Temp (¬∞C)',\n             marker = \"o\"\n             ).set(title = \"Daily Average Temperature in January 2022\")\n\n\n\n\n\n\n\n\nNext, we‚Äôll calculate the number of trips (i.e., rows) for each date, and then plot the number of trips versus the average temperature for every day in January 2022. We‚Äôll include a trend line to illustrate the relationship between the two variables.\n\n# Count the number of rows (trips) by date\ndf_ridership_weather['Count'] = df_ridership_weather.groupby('date')['date'].transform('count')\n\nsns.regplot(data=df_ridership_weather, \n            x=\"Mean Temp (¬∞C)\", \n            y=\"Count\", \n            scatter_kws={\"s\": 8}\n            ).set(title = \"Number of Bike Share Trips vs Daily Temperature in January 2022\")\n\n\n\n\n\n\n\n\nThe trend line has a positive slope, indicating there is a positive relationship between average daily temperature and number of trips.\nWe can also determine how highly correlated the two variables are by calculating the Pearson correlation coefficient using the pearsonr function from the stats module of the scipy library.\n\nfrom scipy.stats import pearsonr\n\npearsonr(df_ridership_weather[\"Mean Temp (¬∞C)\"], df_ridership_weather[\"Count\"])\n\nPearsonRResult(statistic=0.6499094259264812, pvalue=0.0)\n\n\nThe Pearson correlation coefficient (0.65) is positive and relatively close to 1, which means there is a moderately strong positive linear relationship between the number of daily trips and the temperature. A value of 0 would indicate no correlation, while a value close to -1 would indicate a strong negative relationship.\nThe p-value is very small (&lt;0.05), meaning the correlation is statistically significant (i.e., there is strong evidence that the correlation is real).\nWe can conclude that our hypothesis was correct: more people tend to use bike share when it‚Äôs warmer. However, given that we only looked at the correlation between these two variables, we are not able to establish causation. In other words, we can say that there is a relationship, but we cannot say why. We are just in the exploratory phase of the data analysis process, and to say anything more about the reason for the relationship we would need to do more in-depth modeling.",
    "crumbs": [
      "Urban Data Visualization",
      "Exploratory data visualization in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Maps and spatial data visualizations can either created be at the research stage where we are trying to uncover patterns in the data, or as polished communication visuals where we want to convey patterns in the data to a our intended audience.\nCartography is often defined as the science and art of creating maps. It often involves decisions about ‚Ä¶\n\nselection of data to include on the map\ngeneralization of data to reduce complexity and clutter\nhow we want to style and symbolize our data\nhow to order and layer data\nwhat other map layout elements to include (e.g.¬†title, legend, north arrow), as well as how to design these to make a map easy to understand\n\nThis page will cover the basics of what goes into making maps and spatial data visualizations, following each of these steps.\nThe last part of this notebook shows an example in QGIS for making maps with data-defined styling, for both categorical and numeric data.\nIf you are interested in creating specific types of maps, check out the following notebooks:\n\nChoropleth maps\nBivariate choropleth maps\nProportional symbol maps\nMapping point density (dot maps, grid maps, heat maps)\nCategorical random dot maps\nFlow maps\n\n\n\n\n\n\nAlmost all maps can be thought of as either\n\nReference maps, which provide base geographic information (e.g.¬†streets, land forms, water, etc.) often for locating places or for navigation. Google Maps is probably the most well-used example.\nThematic maps, also called geographic or spatial data visualizations, emphasize patterns or trends tied to a specific theme (e.g., population density, election results, air pollution, etc.).\n\nReference maps are often used as base layers for thematic maps.\nFor example, if we‚Äôve collected a dataset about the location of public washrooms in a city, the first thing we might want to do is simply see where they are located by overlaying them onto a base map in GIS. This would be an example of a simple thematic map.\nHere are a few examples of thematic maps that we‚Äôve created, each with different urban data, objectives, and audience in mind.\n\n\n\nHeatmap of Bike Share trips in Toronto in 06/2024\n\n\n\n\n\nIsochrone map of access to outdoor skating rinks in Toronto\n\n\n\n\n\nBi-variate map of public transit accessibility and recent immigrant settlement patterns\n\n\n\n\n\nStatic maps are fixed, pre-generated images that display geographic information in a single, unchangeable format. They are ideal for simple visualization, printing, or when interactivity isn‚Äôt needed‚Äîsuch as in posters, reports, or static web images.\nStatic maps can vary in size from just a few dozen pixels on a screen or a small figure in a report, to graphics in a slide deck, to large maps printed as posters or dozens of maps as part of a series, like in an atlas. The three maps shown above are examples of static maps.\nInteractive maps are digital and allow users to engage with the content by zooming, panning, clicking for details, or filtering data. They are best for exploration, real-time applications, user-driven analysis, or providing increased engagement via animation in a data-story. Here are a few examples that we‚Äôve developed at the School of Cities‚Äù\n\nUrban Activity Atlas\nHeat Vulnerability in Toronto\nKnowledge of Languages in the Greater Toronto & Hamilton Area\n\nInteractive maps can provide flexibility and deeper engagement with spatial data. However, interactive maps typically are much more work to create than a static map as they include all the visual design thinking in a static map, plus additional thinking about how users interact with them and how the maps respond to user inputs, as well as increased technical development time.\nOur general recommendation is to always start with creating static maps, and then move on to creating an interactive version only if a static version is insufficient at conveying the story in your data that you want to convey.\nIf designed well, static maps can be super effective at highlighting key trends and stories in spatial data in a quick and easy to read way.\n\n\n\n\nHow do we decide what were are going to include on our map? Sometimes this may seem obvious, the key dataset(s) that we want to visualize should be there, but particularly with our reference layers, this can often require a lot of little subtle choices.\nThis is because maps always abstractions of reality. It is impossible to show everything in the world on a piece of paper or a screen - nor would we want to, as we usually want to focus our reader‚Äôs attention on specific data point(s), trend(s), or story.\nMaking maps and spatial data visualizations therefore often involve various decisions on selection and generalization of spatial data to reduce visual clutter to focus on specific data points.\n\n\nCartographic selection is about picking the most important things to draw on a map so it‚Äôs not too crowded. Imagine drawing a treasure map ‚Äî you‚Äôd likely want to show big landmarks like mountains or forests, but leave out tiny rocks or every individual tree.\nIn practice, this can include deciding about whether or not to include a dataset at all when creating a map (e.g.¬†in GIS or Python or other map-making software), or if we do include the dataset, if we want to filter it to only show certain features.\nFor example, you may want to include a dataset of public transit lines as a reference layer for a map of your city. However, you may not want to include every single transit route. An example of cartographic selection would be to only show major transit lines by pre-filtering the data by mode (e.g.¬†only show metro/subway) or frequency (e.g.¬†only show routes where a bus or train comes every 10 minutes or less). Now of course if the goal of your map and research is specifically about public transit, you may want to include all the routes.\nThe process of selecting some, but not all of the data that you have available, reduces clutter on your maps and can make them easier to read.\nWhat data to include depends on your objectives, audience, and story.\n\n\n\nSimilar to selection is cartographic generalization, this is when mapmakers simplify real-world details to make maps clearer and less cluttered, especially at smaller scales.\nFor example, a coastline with tons of tiny twists and inlets might get smoothed out‚Äîkeeping the overall shape but removing unnecessary complexity. This helps the map stay readable without losing its key features. It‚Äôs like sketching a quick but accurate version of a photo instead of drawing every single pixel.\n\n\n\nShoreline of eastern Canada at two different levels of simplification\n\n\nAnother example would be that if you had a dataset of sports and recreation facilities in a city with data on the type of activity each is predominately used by (e.g.¬†baseball, football, tennis, etc.). We can choose to have a different colour or symbol for each of these types, or generalize these to have them all look the same (e.g.¬†the same shade of green).\n\n\n\n\nOnce we‚Äôve decided what we want to include on our map, we have to decide how we want to style each layer. In some mapping tools, like QGIS, style options are often called Symbology\nThese are the common graphic styling options available for vector data layers. If you‚Äôve worked with other design software, especially vector graphic software, many of these will be familiar.\nPoints:\n\nSymbol type (e.g.¬†circle, square, etc.)\nSize\nFill colour\nStroke colour\nStroke width\nOpacity\n\nLines:\n\nWidth\nColour\nStroke style (e.g.¬†solid, dashed, etc.)\nOpacity\n\nPolygons:\n\nFill colour\nFill pattern (solid, hatching, etc.)\nStroke colour\nStroke width\nStroke style (e.g.¬†solid, dashed, etc.)\nOpacity\n\nNote that there are other options as well, the above are just the most commonly used.\n\n\nSingle-rule styling is when we want all features in a dataset to look the same, regardless of how they may different in terms of their attributes\nIn the example in our notebook on introducing spatial data and GIS, we created a simple map of Toronto where we had two vector datasets 1) ward administrative boundaries (polygons) and 2) public libraries (points).\nEach of these layers is styled via a single rule. For example, all public libraries are shown as blue squares, even though some libraries may be larger than others or have longer opening hours.\n\n\n\nScreenshot of QGIS with single-rule based styling for two layers\n\n\n\n\n\nA power using tools like QGIS and Python or others like them, is that we can visualize spatial data based on a set of rules relating to their associated attribute data. For example ‚Ä¶\n\nColour census tract data based on median income\nSize business point data based on their number of employees\nShade different land-cover (wetlands, forest, farmland) with different textures for each category\nStyle street network data based on quality and safety of cycling infrastructure\nMany more!\n\nMany of these options we use visual variables like size, hue, saturation, orientation, etc. that we showed in our data visualization notebook, but apply these directly to spatial data on a map.\nLet‚Äôs look at a couple examples in QGIS! (If you haven‚Äôt worked with QGIS before, check out our intro to spatial data and GIS notebook)\n\n\n\nFor the first example, let‚Äôs try to create a map of public washrooms in Toronto in QGIS, using some categorical based styling - where different categories or groups\n\nWashroom Facilities (a point dataset of public washroom facilities in Toronto)\nCentrelines (a line dataset representing a mix of features like streets, railways, power lines, rivers, etc. in Toronto)\nGreen space (a polygon dataset of green spaces and public parks in Toronto)\nLake Ontario (a polygon of Lake Ontario)\n\n(All of this data is from the City of Toronto, except for the Lake Ontario polygon, which is from OpenStreetMap)\nLoad the data into QGIS and re-order the layers such that the washroom locations are at the top. For our map, the washrooms are a the key data we want to show (i.e.¬†the foreground), while the others are visual reference (i.e.¬†the background).\nAfter loading in the data, right click on each of the layers and then click Open Attribute Table to view the what data is linked to each feature.\nPretty much every column of any spatial dataset can be styled categorically, but many wouldn‚Äôt be interesting, or if there are many different categories, lead to a lot of visual clutter.\nSo let‚Äôs pick something simple to start with, we‚Äôll colour the washrooms by their type, which only has two options \"Washroom Building\" or \"Portable Toilet\"\nTo style a layer based on a category, right-click the layer, go to Properties, then Symbology, and then at the top there will be a dropdown where one of the options is Categorical. The below dropdown is the Value, which is the column in your data that you want to use to define the style of your data. For this example, lets pick type. Then click Classify. You should each possible category display with a default symbol next to it.\n\n\n\nScreenshot of categorical styling in QGIS\n\n\nYou can change the default colours and symbols of each category by double clicking on it.\nOnce you hit OK at the bottom-right, your map should update!\n\n\n\nMap of public washrooms in Toronto coloured by their type\n\n\nThis was just the beginning! You can play with categorical styling for other layers as well. Here‚Äôs a couple ideas to improve the the reference layers\nStyle to colour green spaces based on their AREA_CLASS, to give different green spaces types different shades of green\nStyle the centreline data by FEATURE_CODE_DESC, to show major roads as thicker lines\nTip! with categorical styling you can also very quickly filter a dataset. Notice in how once a layer is being categorized, there are check marks next to each symbol, this allows you to easily hide or show different categories if you want.\nFor example, if you only wanted to show parks and cemeteries in the green space layer, you can simply uncheck all the other layers.\n\n\n\nScreenshot of selectable categorical styling in QGIS\n\n\n\n\n\nWe often have ordinal or numeric data that we want to use to define our styling. For example, showing larger numbers in a column with a darker colour and smaller numbers with a lighter colour.\nLet‚Äôs do a quick example, with data indicating camera locations for automated enforcement of speeding in Toronto.\nHow we decide to group (i.e.¬†classify or bin) or data, as well as what colours we pick can, can\nFor further ideas, ‚Ä¶.\n\n\n\n\nMaps are often the product of multiple layers, each with their own defined styling. Thinking about the ordering of the layers and their styling relative to each other goes a long way in making an effective map.\nWell designed maps, especially thematic maps, often have a visual hierarchy of background and foreground - where the background provides geographic reference (e.g.¬†streets, bodies of water), while the foreground are the key data that we want to show.\nThese are two examples of maps relating to the 2023 wildfires in Yellowknife where we tried to design them to a have strong visual hierarchy of the key data (e.g.¬†fire area, evacuation routes) relative to their geographic reference backgrounds. Both these maps are quite simple from data analytics standpoint, but were part of a story highlighting the scale and impact of the fires.\n\n\n\nMap of the 2023 Yellowknife wildfires\n\n\n\n\n\nScale and location of evacuation resulting from the 2023 Yellowknife wildfires",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#overview",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#overview",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Maps and spatial data visualizations can either created be at the research stage where we are trying to uncover patterns in the data, or as polished communication visuals where we want to convey patterns in the data to a our intended audience.\nCartography is often defined as the science and art of creating maps. It often involves decisions about ‚Ä¶\n\nselection of data to include on the map\ngeneralization of data to reduce complexity and clutter\nhow we want to style and symbolize our data\nhow to order and layer data\nwhat other map layout elements to include (e.g.¬†title, legend, north arrow), as well as how to design these to make a map easy to understand\n\nThis page will cover the basics of what goes into making maps and spatial data visualizations, following each of these steps.\nThe last part of this notebook shows an example in QGIS for making maps with data-defined styling, for both categorical and numeric data.\nIf you are interested in creating specific types of maps, check out the following notebooks:\n\nChoropleth maps\nBivariate choropleth maps\nProportional symbol maps\nMapping point density (dot maps, grid maps, heat maps)\nCategorical random dot maps\nFlow maps",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#types-of-maps",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#types-of-maps",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Almost all maps can be thought of as either\n\nReference maps, which provide base geographic information (e.g.¬†streets, land forms, water, etc.) often for locating places or for navigation. Google Maps is probably the most well-used example.\nThematic maps, also called geographic or spatial data visualizations, emphasize patterns or trends tied to a specific theme (e.g., population density, election results, air pollution, etc.).\n\nReference maps are often used as base layers for thematic maps.\nFor example, if we‚Äôve collected a dataset about the location of public washrooms in a city, the first thing we might want to do is simply see where they are located by overlaying them onto a base map in GIS. This would be an example of a simple thematic map.\nHere are a few examples of thematic maps that we‚Äôve created, each with different urban data, objectives, and audience in mind.\n\n\n\nHeatmap of Bike Share trips in Toronto in 06/2024\n\n\n\n\n\nIsochrone map of access to outdoor skating rinks in Toronto\n\n\n\n\n\nBi-variate map of public transit accessibility and recent immigrant settlement patterns\n\n\n\n\n\nStatic maps are fixed, pre-generated images that display geographic information in a single, unchangeable format. They are ideal for simple visualization, printing, or when interactivity isn‚Äôt needed‚Äîsuch as in posters, reports, or static web images.\nStatic maps can vary in size from just a few dozen pixels on a screen or a small figure in a report, to graphics in a slide deck, to large maps printed as posters or dozens of maps as part of a series, like in an atlas. The three maps shown above are examples of static maps.\nInteractive maps are digital and allow users to engage with the content by zooming, panning, clicking for details, or filtering data. They are best for exploration, real-time applications, user-driven analysis, or providing increased engagement via animation in a data-story. Here are a few examples that we‚Äôve developed at the School of Cities‚Äù\n\nUrban Activity Atlas\nHeat Vulnerability in Toronto\nKnowledge of Languages in the Greater Toronto & Hamilton Area\n\nInteractive maps can provide flexibility and deeper engagement with spatial data. However, interactive maps typically are much more work to create than a static map as they include all the visual design thinking in a static map, plus additional thinking about how users interact with them and how the maps respond to user inputs, as well as increased technical development time.\nOur general recommendation is to always start with creating static maps, and then move on to creating an interactive version only if a static version is insufficient at conveying the story in your data that you want to convey.\nIf designed well, static maps can be super effective at highlighting key trends and stories in spatial data in a quick and easy to read way.",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#whats-on-the-map",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#whats-on-the-map",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "How do we decide what were are going to include on our map? Sometimes this may seem obvious, the key dataset(s) that we want to visualize should be there, but particularly with our reference layers, this can often require a lot of little subtle choices.\nThis is because maps always abstractions of reality. It is impossible to show everything in the world on a piece of paper or a screen - nor would we want to, as we usually want to focus our reader‚Äôs attention on specific data point(s), trend(s), or story.\nMaking maps and spatial data visualizations therefore often involve various decisions on selection and generalization of spatial data to reduce visual clutter to focus on specific data points.\n\n\nCartographic selection is about picking the most important things to draw on a map so it‚Äôs not too crowded. Imagine drawing a treasure map ‚Äî you‚Äôd likely want to show big landmarks like mountains or forests, but leave out tiny rocks or every individual tree.\nIn practice, this can include deciding about whether or not to include a dataset at all when creating a map (e.g.¬†in GIS or Python or other map-making software), or if we do include the dataset, if we want to filter it to only show certain features.\nFor example, you may want to include a dataset of public transit lines as a reference layer for a map of your city. However, you may not want to include every single transit route. An example of cartographic selection would be to only show major transit lines by pre-filtering the data by mode (e.g.¬†only show metro/subway) or frequency (e.g.¬†only show routes where a bus or train comes every 10 minutes or less). Now of course if the goal of your map and research is specifically about public transit, you may want to include all the routes.\nThe process of selecting some, but not all of the data that you have available, reduces clutter on your maps and can make them easier to read.\nWhat data to include depends on your objectives, audience, and story.\n\n\n\nSimilar to selection is cartographic generalization, this is when mapmakers simplify real-world details to make maps clearer and less cluttered, especially at smaller scales.\nFor example, a coastline with tons of tiny twists and inlets might get smoothed out‚Äîkeeping the overall shape but removing unnecessary complexity. This helps the map stay readable without losing its key features. It‚Äôs like sketching a quick but accurate version of a photo instead of drawing every single pixel.\n\n\n\nShoreline of eastern Canada at two different levels of simplification\n\n\nAnother example would be that if you had a dataset of sports and recreation facilities in a city with data on the type of activity each is predominately used by (e.g.¬†baseball, football, tennis, etc.). We can choose to have a different colour or symbol for each of these types, or generalize these to have them all look the same (e.g.¬†the same shade of green).",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbols-and-layer-styling",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbols-and-layer-styling",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Once we‚Äôve decided what we want to include on our map, we have to decide how we want to style each layer. In some mapping tools, like QGIS, style options are often called Symbology\nThese are the common graphic styling options available for vector data layers. If you‚Äôve worked with other design software, especially vector graphic software, many of these will be familiar.\nPoints:\n\nSymbol type (e.g.¬†circle, square, etc.)\nSize\nFill colour\nStroke colour\nStroke width\nOpacity\n\nLines:\n\nWidth\nColour\nStroke style (e.g.¬†solid, dashed, etc.)\nOpacity\n\nPolygons:\n\nFill colour\nFill pattern (solid, hatching, etc.)\nStroke colour\nStroke width\nStroke style (e.g.¬†solid, dashed, etc.)\nOpacity\n\nNote that there are other options as well, the above are just the most commonly used.\n\n\nSingle-rule styling is when we want all features in a dataset to look the same, regardless of how they may different in terms of their attributes\nIn the example in our notebook on introducing spatial data and GIS, we created a simple map of Toronto where we had two vector datasets 1) ward administrative boundaries (polygons) and 2) public libraries (points).\nEach of these layers is styled via a single rule. For example, all public libraries are shown as blue squares, even though some libraries may be larger than others or have longer opening hours.\n\n\n\nScreenshot of QGIS with single-rule based styling for two layers\n\n\n\n\n\nA power using tools like QGIS and Python or others like them, is that we can visualize spatial data based on a set of rules relating to their associated attribute data. For example ‚Ä¶\n\nColour census tract data based on median income\nSize business point data based on their number of employees\nShade different land-cover (wetlands, forest, farmland) with different textures for each category\nStyle street network data based on quality and safety of cycling infrastructure\nMany more!\n\nMany of these options we use visual variables like size, hue, saturation, orientation, etc. that we showed in our data visualization notebook, but apply these directly to spatial data on a map.\nLet‚Äôs look at a couple examples in QGIS! (If you haven‚Äôt worked with QGIS before, check out our intro to spatial data and GIS notebook)\n\n\n\nFor the first example, let‚Äôs try to create a map of public washrooms in Toronto in QGIS, using some categorical based styling - where different categories or groups\n\nWashroom Facilities (a point dataset of public washroom facilities in Toronto)\nCentrelines (a line dataset representing a mix of features like streets, railways, power lines, rivers, etc. in Toronto)\nGreen space (a polygon dataset of green spaces and public parks in Toronto)\nLake Ontario (a polygon of Lake Ontario)\n\n(All of this data is from the City of Toronto, except for the Lake Ontario polygon, which is from OpenStreetMap)\nLoad the data into QGIS and re-order the layers such that the washroom locations are at the top. For our map, the washrooms are a the key data we want to show (i.e.¬†the foreground), while the others are visual reference (i.e.¬†the background).\nAfter loading in the data, right click on each of the layers and then click Open Attribute Table to view the what data is linked to each feature.\nPretty much every column of any spatial dataset can be styled categorically, but many wouldn‚Äôt be interesting, or if there are many different categories, lead to a lot of visual clutter.\nSo let‚Äôs pick something simple to start with, we‚Äôll colour the washrooms by their type, which only has two options \"Washroom Building\" or \"Portable Toilet\"\nTo style a layer based on a category, right-click the layer, go to Properties, then Symbology, and then at the top there will be a dropdown where one of the options is Categorical. The below dropdown is the Value, which is the column in your data that you want to use to define the style of your data. For this example, lets pick type. Then click Classify. You should each possible category display with a default symbol next to it.\n\n\n\nScreenshot of categorical styling in QGIS\n\n\nYou can change the default colours and symbols of each category by double clicking on it.\nOnce you hit OK at the bottom-right, your map should update!\n\n\n\nMap of public washrooms in Toronto coloured by their type\n\n\nThis was just the beginning! You can play with categorical styling for other layers as well. Here‚Äôs a couple ideas to improve the the reference layers\nStyle to colour green spaces based on their AREA_CLASS, to give different green spaces types different shades of green\nStyle the centreline data by FEATURE_CODE_DESC, to show major roads as thicker lines\nTip! with categorical styling you can also very quickly filter a dataset. Notice in how once a layer is being categorized, there are check marks next to each symbol, this allows you to easily hide or show different categories if you want.\nFor example, if you only wanted to show parks and cemeteries in the green space layer, you can simply uncheck all the other layers.\n\n\n\nScreenshot of selectable categorical styling in QGIS\n\n\n\n\n\nWe often have ordinal or numeric data that we want to use to define our styling. For example, showing larger numbers in a column with a darker colour and smaller numbers with a lighter colour.\nLet‚Äôs do a quick example, with data indicating camera locations for automated enforcement of speeding in Toronto.\nHow we decide to group (i.e.¬†classify or bin) or data, as well as what colours we pick can, can\nFor further ideas, ‚Ä¶.",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#layers-and-hierarchy",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#layers-and-hierarchy",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Maps are often the product of multiple layers, each with their own defined styling. Thinking about the ordering of the layers and their styling relative to each other goes a long way in making an effective map.\nWell designed maps, especially thematic maps, often have a visual hierarchy of background and foreground - where the background provides geographic reference (e.g.¬†streets, bodies of water), while the foreground are the key data that we want to show.\nThese are two examples of maps relating to the 2023 wildfires in Yellowknife where we tried to design them to a have strong visual hierarchy of the key data (e.g.¬†fire area, evacuation routes) relative to their geographic reference backgrounds. Both these maps are quite simple from data analytics standpoint, but were part of a story highlighting the scale and impact of the fires.\n\n\n\nMap of the 2023 Yellowknife wildfires\n\n\n\n\n\nScale and location of evacuation resulting from the 2023 Yellowknife wildfires",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Home page",
    "section": "",
    "text": "Home page"
  },
  {
    "objectID": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html",
    "href": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html",
    "title": "Bivariate Choropleth Maps",
    "section": "",
    "text": "Jeff Allen\nApril, 2023\nBivariate choropleth maps are pretty two-bular. They use colour to represent the values of two different data variables on the same map. Check out this map of neighbourhood material deprivation (a combined metric of lower income, education, employment rates, etc.) and quality of cycling infrastructure in Winnipeg.\nOf course, we can always just map these two variables individually. However, overlaying them onto a single map can be incredibly useful for highlighting areas of correlation (or lack thereof) at different scales.\nFor example, in the bivariate map above, we can quickly identify areas with greater material deprivation but a lack of cycling infrastructure (in pink). These neighborhoods arguably should be at the top of the list for new investments in cycling infrastructure. The dark purple areas are those with high material deprivation but good cycling infrastructure, while the green areas are wealthier areas with good cycling infrastructure.\nThese types of maps can aid effective spatial storytelling and communication of findings, highlighting specific local areas of need, and be a useful exploratory analysis step before more sophisticated spatial modeling.\nIn this tutorial, we‚Äôre going to cover how to create bivariate choropleth maps like these using Python, mostly using geoPandas, with some final touch-ups and legend design in Inkscape. The example data will be on urban health-related data in Canada (replicating the map above), but the methods and code can be applied to anywhere with two quantitative variables linked to the same set of geometries.\nAs a short side note on history, it is surprising that despite centuries of thematic maps showing multivariate data, bivariate choropleths are a recent creation. They gained popularity in the 1970s via maps created by the U.S. Bureau of the Census (DOI). Here‚Äôs one of their maps. I believe these were the first published bivariate maps, but if you are reading this and know of earlier bivariate maps, please let me know. I‚Äôd be super interested to see them.",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate Choropleth Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#prerequisites",
    "href": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#prerequisites",
    "title": "Bivariate Choropleth Maps",
    "section": "Prerequisites",
    "text": "Prerequisites\nPrior knowledge of Python, including pandas and geopandas, as well as Inkscape or similar graphic design software, would be helpful for the following tutorial.\nClick here to download this article as a Jupyter Notebook alongside the datasets required. In the download, there is also a standalone Python script, if you want to run the steps all-at-once or integrate with anything else you have cooking.\nYou can also run the notebook Binder\nIf you are running the notebook and/or script locally (generally recommended), you will need to use the following libraries. You‚Äôll have to install them via pip or conda if you do not have them installed already.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\nimport mapclassify\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate Choropleth Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#loading-data",
    "href": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#loading-data",
    "title": "Bivariate Choropleth Maps",
    "section": "Loading Data",
    "text": "Loading Data\nWe‚Äôre going to replicate the map of Winnipeg shown at the top of this page. I‚Äôve pre-filtered the source datasets for Winnipeg, and they are included in the download link above. The datasets we‚Äôll be using are: - Census Dissemination Areas (DA) Polygons - Canadian Marginalization Index (CAN-Marg) - Canadian Bikeway Comfort and Safety Classification system (Can-BICS)\nWe‚Äôre also going to add three additional layers solely for cartographic purposes (i.e.¬†as reference layers on the final map) - Census Subdivision (CSD) Polygon - i.e.¬†a boundary polygon for Winnipeg - Major Streets from OpenStreetMap - Rivers from OpenStreetMap\nTo get started, let‚Äôs read the first three data layers and merge them into a single geoDataFrame, joining by the Dissemination Area unique id, dauid. We‚Äôll also load the three carotgraphic reference layers.\n\ngdf = gpd.read_file(\"data/dissemination-area-winnipeg-2016.geojson\")\ndfm = pd.read_csv(\"data/can-marg-manitoba-2016.csv\")\ndfb = pd.read_csv(\"data/can-bics-winnipeg.csv\")\n\ngdf = gdf.merge(dfb, how='left', on='dauid').merge(dfm, how='left', on='dauid')\n\ncsd = gpd.read_file(\"data/csd-winnipeg-2016.geojson\")\nosm_streets = gpd.read_file(\"data/streets-osm-winnipeg.geojson\")\nosm_rivers = gpd.read_file(\"data/river-osm-winnipeg.geojson\")\n\nLet‚Äôs pick two variables to map, one from each of the tabular datasets: 1) Material resources from CAN-Marg - an indicator of individual and community access to and attainment of basic material needs (including housing, income, education, employment). The higher the value, the fewer the resources (i.e.¬†greater deprivation). 2) Can-BICS continuous metric - a weighted sum of the quality of bike infrastructure within a buffer of each DA. The higher the value, the better the infrastructure.\nHere‚Äôs a subset of what we want to map, the two variables noted above, plus the geometry data required to plot the data.\n\ngdf[[\"dauid\",\"material_resources_DA16\",\"CBICS_cont\",\"geometry\"]].head(8)\n\n\n\n\n\n\n\n\ndauid\nmaterial_resources_DA16\nCBICS_cont\ngeometry\n\n\n\n\n0\n46110001\n-0.349423\n0.000000\nMULTIPOLYGON (((-97.14934 49.99388, -97.14105 ...\n\n\n1\n46110002\n-0.747839\n0.000000\nMULTIPOLYGON (((-97.09195 49.96864, -97.09705 ...\n\n\n2\n46110003\n-0.321103\n0.000000\nMULTIPOLYGON (((-97.13213 49.95467, -97.13289 ...\n\n\n3\n46110004\n-0.958393\n0.000000\nMULTIPOLYGON (((-97.13575 49.95428, -97.13695 ...\n\n\n4\n46110005\n0.420112\n0.026394\nMULTIPOLYGON (((-97.13289 49.95334, -97.13418 ...\n\n\n5\n46110006\n-0.065753\n0.026394\nMULTIPOLYGON (((-97.12985 49.94970, -97.13065 ...\n\n\n6\n46110007\n-0.277738\n0.000000\nMULTIPOLYGON (((-97.12224 49.95150, -97.12321 ...\n\n\n7\n46110008\n-0.329884\n0.020913\nMULTIPOLYGON (((-97.12579 49.94767, -97.12714 ...",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate Choropleth Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#univariate-choropleth-maps",
    "href": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#univariate-choropleth-maps",
    "title": "Bivariate Choropleth Maps",
    "section": "Univariate Choropleth Maps",
    "text": "Univariate Choropleth Maps\nGeoPandas has a built-in plot function that makes it very easy to create univariate choropleth maps for any quantitative variable. In the code below, we use quantiles (equal number of DAs in each coloured group) to create choropleths using geopandas and matplotlib.\nIf you‚Äôre interested in learning more about creating univariate choropleth maps in Python, including how to classify data, check out the Geographic Data Science textbook‚Äôs chapter on the topic. You can also refer to the geopandas.GeoDataFrame.plot documentation for more information.\nCreating similar maps in QGIS is also straightforward. Simply load the data, right-click on the layer, go to properties, then symbology, and then select Graduated. From here, you‚Äôll have access to a wide range of options for colours and classification schemes to create informative univariate choropleth maps.\n\nfig, ax = plt.subplots(ncols=2, figsize=(7,6))\n\ngdf.plot(\n    column = \"material_resources_DA16\", \n    cmap = 'YlOrRd', \n    k = 5,\n    scheme = \"Quantiles\", \n    legend = True,\n    ax=ax[0],\n    legend_kwds = {\n        \"loc\": \"lower left\",\n        \"fontsize\": 7,\n        \"title\": \"Lack of Material\\nResources\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    }\n).set_axis_off();\n\ngdf.plot(\n    column = \"CBICS_cont\", \n    cmap = 'YlGnBu', \n    scheme = \"Quantiles\", \n    legend = True,\n    ax=ax[1],\n    legend_kwds = {\n        \"loc\": \"lower left\",\n        \"fontsize\": 7,\n        \"title\": \"Can-BICS\\nContinuous\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    }\n).set_axis_off();\n\nplt.tight_layout()",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate Choropleth Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#bivariate-classifications",
    "href": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#bivariate-classifications",
    "title": "Bivariate Choropleth Maps",
    "section": "Bivariate Classifications",
    "text": "Bivariate Classifications\nThe first step towards creating a bivariate map is to classify our data.\nI‚Äôll be focusing 3x3 classifications using quantiles, but the same principles can be applied to any classification type.\nA bivariate classification has two dimensions, let‚Äôs call those X and Y. The legend can be visualized as a 2-dimensional chart. For this example, I‚Äôm classifying each dimension into three categories, terming them 0 (low), 1, and 2 (high). We then concatenate these two categories together to create a joint classification. For example, a high value in the X dimension and a low value in the Y dimension would be classified as 2-0.\nWe can then colour based on this joint classification.\n\nBut how do we do this in Python and geopandas? Behind the scenes of the univariate choropleth maps in the previous section is a Python library called mapclassify.\nHere‚Äôs an example classifying the Can-BICS variable into k = 3 quantiles (changing this to k = 5 gives us the same classification in the map above)\n\nmapclassify.Quantiles(gdf[\"CBICS_cont\"], k = 3)\n\nQuantiles\n\n   Interval      Count\n----------------------\n[ 0.00,  1.15] |   373\n( 1.15,  2.25] |   372\n( 2.25, 10.79] |   373\n\n\nWe can use the same function to generate bivariate classifications. Below we create 3x3 square classification using quantiles. A k = 3 for each variable X (Can-BICS cycling infrastructure quality) and Y (Can-MARG lack of material resources), and then combine the two together.\n\ngdf['x_group'] = gdf[['CBICS_cont']].apply(mapclassify.Quantiles.make(rolling=True, k = 3))\ngdf['y_group'] = gdf[['material_resources_DA16']].apply(mapclassify.Quantiles.make(rolling=True, k = 3))\ngdf['xy_group'] = gdf['x_group'].astype(str) + \"-\" + gdf['y_group'].astype(str)\n\nHere‚Äôs a random sample of the result that we can use to spot check a few results\n\ngdf[[\"dauid\",\"material_resources_DA16\",\"CBICS_cont\",'x_group',\"y_group\",\"xy_group\"]].sample(5)\n\n\n\n\n\n\n\n\ndauid\nmaterial_resources_DA16\nCBICS_cont\nx_group\ny_group\nxy_group\n\n\n\n\n509\n46110546\n-0.570719\n3.677031\n2\n0\n2-0\n\n\n107\n46110108\n-0.258522\n0.226581\n0\n1\n0-1\n\n\n684\n46110741\n-0.149318\n2.971448\n2\n1\n2-1\n\n\n978\n46111074\n-0.949989\n1.141521\n0\n0\n0-0\n\n\n785\n46110855\n-0.488178\n1.340795\n1\n1\n1-1\n\n\n\n\n\n\n\nNow we are ready to make a bivariate map!\nAt this stage, we can also save the classified data to then view it in QGIS or use for web-mapping: gdf.to_file(\"data/winnipeg-bivariate.geojson\", driver='GeoJSON')",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate Choropleth Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#bivariate-colours",
    "href": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#bivariate-colours",
    "title": "Bivariate Choropleth Maps",
    "section": "Bivariate Colours",
    "text": "Bivariate Colours\nNow that we‚Äôve classified our data, we can assign colours and make a map!\nHere are a few examples for colouring that we can choose from:\n\nLet‚Äôs try to make a simple map using gpd.plot() based on our classified data and the first of these colour schemes.\nThe data we have are categorical. To plot categorical data with custom colours, we first have to provide a dictionary to map each category to each colour.\n\ncolor_mapping = {\n    \"0-2\": \"#f73593\", \n    \"0-1\": \"#f78fb6\",\n    \"0-0\": \"#f7fcf5\", \n    \"1-2\": \"#a53593\",\n    \"1-1\": \"#a58fb6\", \n    \"1-0\": \"#a5e8cd\", \n    \"2-2\": \"#403593\",\n    \"2-1\": \"#408fa7\",\n    \"2-0\": \"#40dba7\" \n}\n\nWe can then feed this into our plot. We‚Äôre also going to add the reference layers to make the map a bit more intuitive; the Winnipeg boundary, streets, and rivers.\n\nfig, ax = plt.subplots(figsize=(7,7))\n\n# Winnipeg border\ncsd.plot(\n    edgecolor = \"#c2c2c2\",\n    linewidth = 4.2,\n    ax = ax\n);\n\n# bivariate data\ngdf.plot(\n    column = \"xy_group\",\n    categorical = True,\n    edgecolor = \"white\",\n    linewidth = 0.2,\n    ax = ax,\n    color=gdf[\"xy_group\"].map(color_mapping),\n).set_axis_off();\n\n# Winnipeg rivers\nosm_rivers.plot(\n    color = \"#c2c2c2\",\n    linewidth = 0,\n    ax = ax\n);\n\n# Winnipeg streets\nosm_streets.plot(\n    color = \"#5e5e5e\",\n    linewidth = 0.25,\n    ax = ax\n);\n\n# custom legend\nlegend_elements = []\nfor key, value in color_mapping.items():\n    legend_elements.append(Patch(facecolor=value, edgecolor='gray', label=\"\"))\nax.legend(\n    handles=legend_elements, \n    loc='lower right', \n    fontsize= 12, \n    ncol=3, \n    handletextpad=0.1, \n    labelspacing = 0.1, \n    columnspacing = 0.1\n)\nax.text(0.55, 0.1, 'Material\\nDeprivation', transform=ax.transAxes, fontsize=10, verticalalignment='top')\nax.text(0.75, 0.01, 'Quality of Cycling\\nInfrastructure', transform=ax.transAxes, fontsize=10, verticalalignment='top')\n\n\n\n\n\n\n\n\nLooks great! One wrinkle, however, is that we had to create a custom legend because simply settinglegend = True didn‚Äôt work initially. It appears that there is a known issue with plotting categorical data with custom colours. We can try to continue to tinker with this map using matplotlib, but personally, I prefer to design more nuanced layout items, such as legends, using graphic design software like Inkscape, particularly if the goal is to create visually appealing ‚Äústatic‚Äù maps for an article or report.\n\nfig.savefig('images/winnipeg-bivariate-map-python-export.png')",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate Choropleth Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#layout-design-in-inkscape",
    "href": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#layout-design-in-inkscape",
    "title": "Bivariate Choropleth Maps",
    "section": "Layout Design in Inkscape",
    "text": "Layout Design in Inkscape\nThe line of code above exports the figure as an .svg (Scalable Vector Graphic). We can open it up in Inkscape (or similar graphic design software) and add layout items such as a title, legend, scale bar, north arrow, and other information.\nThe page size is set to 7x7 inches based on the plt.subplots(figsize=(7,7)) included at the top of the code used to render the map.\nGenerally, my workflow is to create at least three layers in the following order: - layout (title, legend, scale bar, north arrow, and other info) - map (in this case, the original exported .svg) - background (often just a single rectangle with a solid fill colour)\nThese can be expanded into more layers (or sub-layers) if desired.\nFrom a cartographic design side, it‚Äôs important to aim for balance. This is a bit of a subjective notion pertaining to the visual weight of the elements of an image. For example, in the map below, I tried to balance the layout by putting one item in each corner. I purposely used the bottom-left corner for the legend since it takes up the most space and fits nicely within the square-ish cutout of Winnipeg‚Äôs border.\nI also used a grid to align some of the items such as the map frame, title, north arrow, and scale bar. Overall, creating these layout items didn‚Äôt take too long, around 20 minutes or so.\nHere‚Äôs a screenshot of Inkscape showing all the items in the layout layer that I created.\n\nAnd here‚Äôs the final map! We can export this to any resolution that we want. This is 300dpi for 7‚Äùx7‚Äù (2100x2100 px), a nice size for fitting into a report.",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate Choropleth Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#final-thoughts",
    "href": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#final-thoughts",
    "title": "Bivariate Choropleth Maps",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThe classification method (e.g., quantiles, equal breaks, etc.), the number of classes, and of course, the colours can easily be tinkered with.\nA noted drawback with bivariate maps is that they are often not initially intuitive and sometimes require a good amount of looking back-and-forth with the legend, especially compared to univariate choropleth maps. Even with a 3x3 legend like the map above, it can take a bit of time to read the map and understand and parse out specific values (especially for the middle values). If you‚Äôre more interested in understanding each variable on its own, rather than a direct comparison, it‚Äôs probably best to stick with two side-by-side univariate choropleth maps. If you want to allow for comparison but keep things easier to read, a 2x2 legend might be better, but remember that different aggregation can lead to varying results!",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate Choropleth Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#additional-resources",
    "href": "notebooks/urban-data-visualization/bivariate-choropleth-maps/bivariate-choropleth-maps.html#additional-resources",
    "title": "Bivariate Choropleth Maps",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nWikipedia page on multivariate maps\nColor Statistical Mapping by the U.S. Bureau of the Census | (what I think might be the first published bivariate maps)\nBivariate maps in QGIS | by Joshua Stevens\nBivariate maps in R | by Timo Grossenbacher",
    "crumbs": [
      "Urban Data Visualization",
      "Bivariate Choropleth Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html",
    "title": "Data visualization",
    "section": "",
    "text": "Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools help people understand patterns, trends, and outliers in data. At its core, data visualization translates abstract numbers into something visible and intuitive, helping our audiences better understand what the data is telling us.\nWe live in a world increasingly shaped by data ‚Äì from climate change and public health to business performance and social media behavior ‚Äì however, raw numbers alone are difficult to interpret. Visualizations help bridge that gap by making data more accessible, understandable, and actionable. It‚Äôs not just about making things look good, it‚Äôs about making data make sense.\n‚ÄúThe simple graph has brought more information to the data analyst‚Äôs mind than any other device. It specializes in providing indications of unexpected phenomena.‚Äù ‚Äì John Tukey\n\n\nWe visualize data to help us understand it better. When we turn numbers into pictures like charts or graphs, it becomes easier to see patterns, trends, or problems. Broadly speaking, visualization can serve two major goals:\n\nExploration ‚Äì Using charts to help make sense of the data. It helps you find interesting things or answer questions\nCommunication ‚Äì Using visuals to help other people understand what you found in the data\n\nThese goals are not mutually exclusive. The best data work is iterative. We often begin by visualizing data to explore patterns, test ideas, and uncover insights. Once we have made sense of the data ourselves, we can use visualizations to communicate key findings, tell a compelling story, or make a case for action. Whether the goal is understanding, persuasion, or advocacy, good data visualization helps bridge the gap between raw information and meaningful insight.\nVisualizing data is ultimately about communication and striking the right balance between analysis, design, and narrative. To share data in a clear and meaningful way, it helps to think through this data storytelling framework:\n\n\n\nFramework for data storytelling\n\n\n\nAudience & Objectives: Who is this for? What do they need to know or do with the information?\nData Analysis: What does the data tell us? What patterns or relationships matter?\nInformation Design: How should this data be visually structured for clarity?\nWriting & Narrative: What story does the data tell? What‚Äôs your argument or takeaway?\n\nThis framework helps situate visualization as part of a broader storytelling or decision-making process. Whether you are creating a data story with multiple visuals or a single chart for a report, it is important to keep all of these things in mind. The purpose behind your visualization determines everything from the chart type you use to the level of detail, annotation, and tone you adopt.\n\n\n\nBefore we share our findings with others, we often need to make sense of the data ourselves. This is where exploratory data analysis comes in. This is the process of visually investigating datasets to uncover trends, spot anomalies, test hypotheses, and surface insights that might otherwise be hidden in raw numbers or tables.\nVisualization during this stage can reveal unexpected patterns and outliers, correlations and clusters, distribution shapes, and data quality issues.\nIt also helps uncover relationships that summary statistics alone might obscure. The exploratory process is typically iterative and often messy, with charts not needing to be beautiful, but instead, informative. Quick plots, heatmaps, and scatterplots are all useful tools in this phase, enabling a deeper understanding of the data before any formal analysis or presentation.\nEven datasets with identical statistical properties can convey vastly different stories when visualized. A perfect example of this is Anscombe‚Äôs Quartet (Figure 2), a set of four datasets that have nearly identical means, variances, and correlation coefficients, yet their graphical representations reveal dramatic differences. This illustrates the importance of visualizing data, rather than solely relying on summary statistics, as the true patterns and relationships may be hidden within the numbers themselves.\n\n\n\nAnsecombe‚Äôs Quartet, for datasets with almost identical summary statistics, but widely different patterns when charted\n\n\n\n\n\nOnce we‚Äôve explored the data and identified key findings, the next step is to share these insights with others. Data visualization becomes a powerful tool for communication, helping us inform, engage, and persuade different audiences. In this context, visualization is not just about showing data, it‚Äôs about shaping understanding and sometimes even inspiring action.\nData stories combine charts and text to walk the audience through a narrative arc. These stories often include:\n\nContext ‚Äì What‚Äôs the issue or question?\nData ‚Äì What patterns or evidence are we seeing?\nInsight ‚Äì Why does it matter?\nAction ‚Äì What should happen next?\n\nGood data visualizations and stories often zoom in and out to show both the big picture and key details while guiding the audience with clear visual hierarchy and annotations.\nIn advocacy contexts, data visualizations are used to support arguments, influence policy, and raise awareness. The visual design here should support the clarity and urgency of the message while still being transparent and truthful.\n\n\nNot all data visualizations are created for the same audience or goal. Some are meant to be read in detail (like charts in a policy report), while others are intended to create an emotional impact at a glance (like a billboard or social media graphic).\nFor example, a line chart depicting global temperatures over time provides nuance and precision, making it an excellent choice for scientists and analysts.\n\n\n\nGlobal Temperatures Over Time (Source: NASA/GISS)\n\n\nAlternatively, warming stripes, a minimalist visualization uses a simple color gradient to represent temperature changes. This approach communicates the same pattern but in a more striking and emotionally impactful way, making it highly effective for public engagement and advocacy.\n\n\n\nWarming strips\n\n\n\n\n\n\nA well-designed visualization isn‚Äôt just about making data look good ‚Äì it‚Äôs about making it understandable, accurate, and accessible. This often requires a base understanding of how we encode data visually, how the human brain processes that information, and how to make charts function well across different audiences and mediums.\n\n\nAt the heart of every chart or graph is visual encoding: the transformation of data into visual elements (Figure 5). Each variable in your dataset must be ‚Äúmapped‚Äù to a visual channel, often called a visual variable. The effectiveness of these visual encodings depends on what type of data you‚Äôre working with.\n\n\n\nBertin‚Äôs Visual Variables via Axis Maps\n\n\n\n\n\nEffective data visualization leverages our brain‚Äôs innate ability to rapidly process certain visual elements. This phenomenon, known as preattentive processing, allows viewers to quickly and effortlessly identify certain patterns, contrasts, and structures without conscious effort.\nThis allows us to instantly spot outliers, notice trends and clusters, and recognize visual hierarchies.\nKey preattentive features include color, form, orientation, and size. For example, we can quickly distinguish a red circle among blue circles (color) and identify a square amidst a group of circles (form). These features are processed so swiftly and effortlessly that they can influence our perception and understanding of visual information without conscious effort.\n\n\n\nExample of preattentive processing. Source: www.csc2.ncsu.edu/faculty/healey/PP/\n\n\nEven when designing a table for a slide deck or a report, you can use the theory of preattentive processing to pick a visual variable to focus your readers attention on specific data points that are important to your story.\n\n\n\nExample of preattentive processing to highlight rows in a table (Source: CNN)\n\n\nVia preattentive processing, we can guide viewers‚Äô attention efficiently, enhance comprehension, and reduce cognitive load.\n\n\n\n**Perceptual Rankings* help explain why some encodings are easier to interpret than others. Not all visual variables are equally effective.\nYou may have noticed that in the scatter-plot example in the previous section, that it‚Äôs generally easier to pick out a red circle among a group of blue circles rather than a blue square among a group of blue circles.\nResearch has established a hierarchy of visual channels based on their accuracy and ease of interpretation:\n\n\n\nPerceptual Rankings (Source: Visualization Analysis and Design by Tamara Munzner)\n\n\nUnderstanding these perceptual principles ensures that visualizations communicate information clearly and intuitively, aligning with how viewers naturally process visual stimuli.\nFor example, based on this research, position on a common scale is easier to interpret than size or area for quantitative data. Comparing volumes is also relatively difficult. This leads to two recommendations‚Ä¶\n\nAvoid pie charts (especially for more 3 categories) since they can be less effective than alternatives (e.g.¬†bar charts)\nAvoid 3D charts in most cases because they tend to over-complicate, be difficult to read, and add extra visual clutter compared to 2D alternatives\n\n\n\n\nYour data visualizations should be readable by everyone, which means thinking beyond aesthetics and into the realm of inclusive design.\nUse color vision deficiency-safe palettes, such as those available on ColorBrewer, and avoid relying solely on color to distinguish elements.\nIncorporate texture, thickness, shape, or labels where possible. Additionally, ensure sufficient contrast between the foreground and background, as light grey text on white or red-on-green combinations are problematic for readability. The greater the contrast between the foreground and background, the easier it is to read. For text, keep in mind that font size plays a significant role: smaller fonts require even higher contrast between the text color and background to maintain readability.\n\n\n\n\nStrong visualizations depend not just on the data and encodings, but on thoughtful framing. The supporting elements of a chart provide clarity and context.\nThinking about and directly designing each of the different components of a chart, rather than just using the defaults of a software (e.g.¬†what a chart in Excel or a Python library will give you initially), will go a long way in making effective graphics.\nMake sure every visual component earns its place. If it doesn‚Äôt clarify, it probably distracts.\n\n\n\nChart components (Source: data.europa.eu/apps/data-visualisation-guide)\n\n\n\n\n\nA strong visualization can make complex data accessible; however, without care, the message can get lost. Great visualizations balance clarity, precision, and aesthetics. This section offers practical tips and guiding principles to elevate your data visualizations.\nNote that these are general recommendations and rules of thumb, not rules that you must follow 100% of the time! data visualization is a combination of technical, design, and artistic skills, and there are often exceptions to the rules :)\n\n\nGuide the viewer‚Äôs eye by creating a hierarchy between background and foreground. Your key message should be the focus and highlighted while everything else (axes, grid-lines, background elements) should support and not compete with it.\n\nBold or highlight key data points.\nUse size, contrast, and color to signal importance.\nDe-emphasize secondary elements like gridlines, minor tick marks, or axis labels\n\nThe line chart in the image above has a strong visual hiearchy between the key data points it wants to show (e.g.¬†the lines for Sweden, the E.U., and Ireland) relative to the rest of the chart components.\nWhen guiding your readers through a story with a series of visualizations, sometimes it is useful to follow a Data Visualization Sandwich metaphor\n\nThis is very similar to journalistic styles of writing, ‚Äúdon‚Äôt burry the lead‚Äù and ‚Äúbottom line up-front‚Äù\nLet‚Äôs look at an example, this is a map of the United States from an article by the Guardian on how the USA is facing above average rises in temperatures. This map acts as the Patty ‚Äì it draws the attention of the reader and introduces the topic of the article.\n\nThe article then takes a deeper look at specific counties that have experienced significant temperature increases via a table\n\nFinally, this visual, which could be considered a patty or bun, helps illustrate the temperatures across U.S. states. It also includes a ‚Äútopping‚Äù ‚Äì an annotation over California that highlights how the entire population has experienced a temperature increase since 1895.\n\n\n\n\nCoined by Edward Tufte, the data-ink ratio refers to the proportion of visual elements that represent actual data, rather than decoration, relative to all ‚Äòink‚Äô on the chart.\nIn other words, this is about reducing clutter and focusing graphical elements of a chart on the data.\n\n\n\nLeft: Low data-ink ratio / Right: High data-ink ratio\n\n\nHere are a few recommendations for reducing clutter and increasing data-ink ratios:\n\nRemove non-essential ‚Äúchart-junk‚Äù (3D effects, background shading, unnecessary gridlines, borders).\nUse direct labels instead of relying on legends.\nChoose simple chart types unless complexity is truly needed.\nUse subtle formatting of reference information like gridlines to keep the audience focused on the data itself.\nAvoid overuse of colors, labels, and gridlines.\nDon‚Äôt always need to visualize every possible variable, focus on the story you‚Äôre trying to tell.\nGroup or collapse less important data to simplify interpretation.\nWhitespace is your friend\n\nHere‚Äôs an example of COVID-19 deaths that appeared in The Guardian that has limited visual clutter and strong hierarchy.\n\n\n\n\nEffective visualization isn‚Äôt one-size-fits-all. Always consider where and how your work will be seen. Tailor your design choices to the medium and the audience:\n\nPrint vs digital: Pick fonts, line weights, and colors that are clear at the specific resolution and paper size that you are designing for. What works on a low-resolution screen may not translate well to paper.\nPresentations vs.¬†reports: Slides call for bold, minimal visuals with large text and fewer details. Reports allow for more complexity and written explanation.\nSocial media: Prioritize clarity at small sizes. Expect compression and low resolution and keep text minimal and legible.\nAudience expertise: Technical audiences may appreciate complex charts and granular data. Broader or non-expert audiences often benefit from simpler visuals, clear labeling, and guiding annotations.\n\nIf you design a chart to fit within a specific medium (e.g.¬†mobile view on a screen), it‚Äôs recommended you redesign the chart if you want to have it in a different context (e.g.¬†printed in a report). This is to prevent fonts being resized so they are no longer legible, graphics losing resolution and becoming fuzzy, or distorted if they are resized.\n\n\n\nIf you‚Äôre creating visualizations within an organization or for a specific campaign, it‚Äôs important to follow established brand guidelines.\nThis includes using approved typefaces, colors, and logos, and aligning your visuals with the tone and messaging of the broader content.\nMaintaining visual consistency across all charts, graphics, and reports reinforces brand identity, leads to more cohesive products, and builds recognition with your audience.\n\n\n\nGreat charts don‚Äôt always speak for themselves. Use clear titles, subtitles, and axis labels. Add callouts or annotations to highlight patterns or anomalies. You can guide the reader to your takeaway rather than leaving it to interpretation.\n\n\n\nExample of how simple text labels and titles can help tell an effective visual story (Source: Wall Street Journal 2015)\n\n\n\n\n\nMatch the visualization to the type of data and the message you‚Äôre trying to convey. Here‚Äôs a quick overview of very common charts for different types of data\n\nChoosing the right chart type is essential to making your data clear, compelling, and truthful. While there‚Äôs no single ‚Äúcorrect‚Äù choice for every scenario, understanding your data and communication goals will help you select a format that highlights the story you want to tell. Whether you‚Äôre comparing categories, showing trends over time, exploring distributions, or revealing relationships, each chart type has strengths and limitations.\nFor a deeper dive into chart selection, visit From Data to Viz, a comprehensive, visual guide that helps you choose the most appropriate chart based on your data structure and communication goals.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#why-visualize-data",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#why-visualize-data",
    "title": "Data visualization",
    "section": "",
    "text": "We visualize data to help us understand it better. When we turn numbers into pictures like charts or graphs, it becomes easier to see patterns, trends, or problems. Broadly speaking, visualization can serve two major goals:\n\nExploration ‚Äì Using charts to help make sense of the data. It helps you find interesting things or answer questions\nCommunication ‚Äì Using visuals to help other people understand what you found in the data\n\nThese goals are not mutually exclusive. The best data work is iterative. We often begin by visualizing data to explore patterns, test ideas, and uncover insights. Once we have made sense of the data ourselves, we can use visualizations to communicate key findings, tell a compelling story, or make a case for action. Whether the goal is understanding, persuasion, or advocacy, good data visualization helps bridge the gap between raw information and meaningful insight.\nVisualizing data is ultimately about communication and striking the right balance between analysis, design, and narrative. To share data in a clear and meaningful way, it helps to think through this data storytelling framework:\n\n\n\nFramework for data storytelling\n\n\n\nAudience & Objectives: Who is this for? What do they need to know or do with the information?\nData Analysis: What does the data tell us? What patterns or relationships matter?\nInformation Design: How should this data be visually structured for clarity?\nWriting & Narrative: What story does the data tell? What‚Äôs your argument or takeaway?\n\nThis framework helps situate visualization as part of a broader storytelling or decision-making process. Whether you are creating a data story with multiple visuals or a single chart for a report, it is important to keep all of these things in mind. The purpose behind your visualization determines everything from the chart type you use to the level of detail, annotation, and tone you adopt.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-exploratory-analysis",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-exploratory-analysis",
    "title": "Data visualization",
    "section": "",
    "text": "Before we share our findings with others, we often need to make sense of the data ourselves. This is where exploratory data analysis comes in. This is the process of visually investigating datasets to uncover trends, spot anomalies, test hypotheses, and surface insights that might otherwise be hidden in raw numbers or tables.\nVisualization during this stage can reveal unexpected patterns and outliers, correlations and clusters, distribution shapes, and data quality issues.\nIt also helps uncover relationships that summary statistics alone might obscure. The exploratory process is typically iterative and often messy, with charts not needing to be beautiful, but instead, informative. Quick plots, heatmaps, and scatterplots are all useful tools in this phase, enabling a deeper understanding of the data before any formal analysis or presentation.\nEven datasets with identical statistical properties can convey vastly different stories when visualized. A perfect example of this is Anscombe‚Äôs Quartet (Figure 2), a set of four datasets that have nearly identical means, variances, and correlation coefficients, yet their graphical representations reveal dramatic differences. This illustrates the importance of visualizing data, rather than solely relying on summary statistics, as the true patterns and relationships may be hidden within the numbers themselves.\n\n\n\nAnsecombe‚Äôs Quartet, for datasets with almost identical summary statistics, but widely different patterns when charted",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-communication-and-storytelling",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-communication-and-storytelling",
    "title": "Data visualization",
    "section": "",
    "text": "Once we‚Äôve explored the data and identified key findings, the next step is to share these insights with others. Data visualization becomes a powerful tool for communication, helping us inform, engage, and persuade different audiences. In this context, visualization is not just about showing data, it‚Äôs about shaping understanding and sometimes even inspiring action.\nData stories combine charts and text to walk the audience through a narrative arc. These stories often include:\n\nContext ‚Äì What‚Äôs the issue or question?\nData ‚Äì What patterns or evidence are we seeing?\nInsight ‚Äì Why does it matter?\nAction ‚Äì What should happen next?\n\nGood data visualizations and stories often zoom in and out to show both the big picture and key details while guiding the audience with clear visual hierarchy and annotations.\nIn advocacy contexts, data visualizations are used to support arguments, influence policy, and raise awareness. The visual design here should support the clarity and urgency of the message while still being transparent and truthful.\n\n\nNot all data visualizations are created for the same audience or goal. Some are meant to be read in detail (like charts in a policy report), while others are intended to create an emotional impact at a glance (like a billboard or social media graphic).\nFor example, a line chart depicting global temperatures over time provides nuance and precision, making it an excellent choice for scientists and analysts.\n\n\n\nGlobal Temperatures Over Time (Source: NASA/GISS)\n\n\nAlternatively, warming stripes, a minimalist visualization uses a simple color gradient to represent temperature changes. This approach communicates the same pattern but in a more striking and emotionally impactful way, making it highly effective for public engagement and advocacy.\n\n\n\nWarming strips",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-theory",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-theory",
    "title": "Data visualization",
    "section": "",
    "text": "A well-designed visualization isn‚Äôt just about making data look good ‚Äì it‚Äôs about making it understandable, accurate, and accessible. This often requires a base understanding of how we encode data visually, how the human brain processes that information, and how to make charts function well across different audiences and mediums.\n\n\nAt the heart of every chart or graph is visual encoding: the transformation of data into visual elements (Figure 5). Each variable in your dataset must be ‚Äúmapped‚Äù to a visual channel, often called a visual variable. The effectiveness of these visual encodings depends on what type of data you‚Äôre working with.\n\n\n\nBertin‚Äôs Visual Variables via Axis Maps\n\n\n\n\n\nEffective data visualization leverages our brain‚Äôs innate ability to rapidly process certain visual elements. This phenomenon, known as preattentive processing, allows viewers to quickly and effortlessly identify certain patterns, contrasts, and structures without conscious effort.\nThis allows us to instantly spot outliers, notice trends and clusters, and recognize visual hierarchies.\nKey preattentive features include color, form, orientation, and size. For example, we can quickly distinguish a red circle among blue circles (color) and identify a square amidst a group of circles (form). These features are processed so swiftly and effortlessly that they can influence our perception and understanding of visual information without conscious effort.\n\n\n\nExample of preattentive processing. Source: www.csc2.ncsu.edu/faculty/healey/PP/\n\n\nEven when designing a table for a slide deck or a report, you can use the theory of preattentive processing to pick a visual variable to focus your readers attention on specific data points that are important to your story.\n\n\n\nExample of preattentive processing to highlight rows in a table (Source: CNN)\n\n\nVia preattentive processing, we can guide viewers‚Äô attention efficiently, enhance comprehension, and reduce cognitive load.\n\n\n\n**Perceptual Rankings* help explain why some encodings are easier to interpret than others. Not all visual variables are equally effective.\nYou may have noticed that in the scatter-plot example in the previous section, that it‚Äôs generally easier to pick out a red circle among a group of blue circles rather than a blue square among a group of blue circles.\nResearch has established a hierarchy of visual channels based on their accuracy and ease of interpretation:\n\n\n\nPerceptual Rankings (Source: Visualization Analysis and Design by Tamara Munzner)\n\n\nUnderstanding these perceptual principles ensures that visualizations communicate information clearly and intuitively, aligning with how viewers naturally process visual stimuli.\nFor example, based on this research, position on a common scale is easier to interpret than size or area for quantitative data. Comparing volumes is also relatively difficult. This leads to two recommendations‚Ä¶\n\nAvoid pie charts (especially for more 3 categories) since they can be less effective than alternatives (e.g.¬†bar charts)\nAvoid 3D charts in most cases because they tend to over-complicate, be difficult to read, and add extra visual clutter compared to 2D alternatives\n\n\n\n\nYour data visualizations should be readable by everyone, which means thinking beyond aesthetics and into the realm of inclusive design.\nUse color vision deficiency-safe palettes, such as those available on ColorBrewer, and avoid relying solely on color to distinguish elements.\nIncorporate texture, thickness, shape, or labels where possible. Additionally, ensure sufficient contrast between the foreground and background, as light grey text on white or red-on-green combinations are problematic for readability. The greater the contrast between the foreground and background, the easier it is to read. For text, keep in mind that font size plays a significant role: smaller fonts require even higher contrast between the text color and background to maintain readability.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#chart-components",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#chart-components",
    "title": "Data visualization",
    "section": "",
    "text": "Strong visualizations depend not just on the data and encodings, but on thoughtful framing. The supporting elements of a chart provide clarity and context.\nThinking about and directly designing each of the different components of a chart, rather than just using the defaults of a software (e.g.¬†what a chart in Excel or a Python library will give you initially), will go a long way in making effective graphics.\nMake sure every visual component earns its place. If it doesn‚Äôt clarify, it probably distracts.\n\n\n\nChart components (Source: data.europa.eu/apps/data-visualisation-guide)",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#practical-tips-for-effective-data-visualization",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#practical-tips-for-effective-data-visualization",
    "title": "Data visualization",
    "section": "",
    "text": "A strong visualization can make complex data accessible; however, without care, the message can get lost. Great visualizations balance clarity, precision, and aesthetics. This section offers practical tips and guiding principles to elevate your data visualizations.\nNote that these are general recommendations and rules of thumb, not rules that you must follow 100% of the time! data visualization is a combination of technical, design, and artistic skills, and there are often exceptions to the rules :)\n\n\nGuide the viewer‚Äôs eye by creating a hierarchy between background and foreground. Your key message should be the focus and highlighted while everything else (axes, grid-lines, background elements) should support and not compete with it.\n\nBold or highlight key data points.\nUse size, contrast, and color to signal importance.\nDe-emphasize secondary elements like gridlines, minor tick marks, or axis labels\n\nThe line chart in the image above has a strong visual hiearchy between the key data points it wants to show (e.g.¬†the lines for Sweden, the E.U., and Ireland) relative to the rest of the chart components.\nWhen guiding your readers through a story with a series of visualizations, sometimes it is useful to follow a Data Visualization Sandwich metaphor\n\nThis is very similar to journalistic styles of writing, ‚Äúdon‚Äôt burry the lead‚Äù and ‚Äúbottom line up-front‚Äù\nLet‚Äôs look at an example, this is a map of the United States from an article by the Guardian on how the USA is facing above average rises in temperatures. This map acts as the Patty ‚Äì it draws the attention of the reader and introduces the topic of the article.\n\nThe article then takes a deeper look at specific counties that have experienced significant temperature increases via a table\n\nFinally, this visual, which could be considered a patty or bun, helps illustrate the temperatures across U.S. states. It also includes a ‚Äútopping‚Äù ‚Äì an annotation over California that highlights how the entire population has experienced a temperature increase since 1895.\n\n\n\n\nCoined by Edward Tufte, the data-ink ratio refers to the proportion of visual elements that represent actual data, rather than decoration, relative to all ‚Äòink‚Äô on the chart.\nIn other words, this is about reducing clutter and focusing graphical elements of a chart on the data.\n\n\n\nLeft: Low data-ink ratio / Right: High data-ink ratio\n\n\nHere are a few recommendations for reducing clutter and increasing data-ink ratios:\n\nRemove non-essential ‚Äúchart-junk‚Äù (3D effects, background shading, unnecessary gridlines, borders).\nUse direct labels instead of relying on legends.\nChoose simple chart types unless complexity is truly needed.\nUse subtle formatting of reference information like gridlines to keep the audience focused on the data itself.\nAvoid overuse of colors, labels, and gridlines.\nDon‚Äôt always need to visualize every possible variable, focus on the story you‚Äôre trying to tell.\nGroup or collapse less important data to simplify interpretation.\nWhitespace is your friend\n\nHere‚Äôs an example of COVID-19 deaths that appeared in The Guardian that has limited visual clutter and strong hierarchy.\n\n\n\n\nEffective visualization isn‚Äôt one-size-fits-all. Always consider where and how your work will be seen. Tailor your design choices to the medium and the audience:\n\nPrint vs digital: Pick fonts, line weights, and colors that are clear at the specific resolution and paper size that you are designing for. What works on a low-resolution screen may not translate well to paper.\nPresentations vs.¬†reports: Slides call for bold, minimal visuals with large text and fewer details. Reports allow for more complexity and written explanation.\nSocial media: Prioritize clarity at small sizes. Expect compression and low resolution and keep text minimal and legible.\nAudience expertise: Technical audiences may appreciate complex charts and granular data. Broader or non-expert audiences often benefit from simpler visuals, clear labeling, and guiding annotations.\n\nIf you design a chart to fit within a specific medium (e.g.¬†mobile view on a screen), it‚Äôs recommended you redesign the chart if you want to have it in a different context (e.g.¬†printed in a report). This is to prevent fonts being resized so they are no longer legible, graphics losing resolution and becoming fuzzy, or distorted if they are resized.\n\n\n\nIf you‚Äôre creating visualizations within an organization or for a specific campaign, it‚Äôs important to follow established brand guidelines.\nThis includes using approved typefaces, colors, and logos, and aligning your visuals with the tone and messaging of the broader content.\nMaintaining visual consistency across all charts, graphics, and reports reinforces brand identity, leads to more cohesive products, and builds recognition with your audience.\n\n\n\nGreat charts don‚Äôt always speak for themselves. Use clear titles, subtitles, and axis labels. Add callouts or annotations to highlight patterns or anomalies. You can guide the reader to your takeaway rather than leaving it to interpretation.\n\n\n\nExample of how simple text labels and titles can help tell an effective visual story (Source: Wall Street Journal 2015)\n\n\n\n\n\nMatch the visualization to the type of data and the message you‚Äôre trying to convey. Here‚Äôs a quick overview of very common charts for different types of data\n\nChoosing the right chart type is essential to making your data clear, compelling, and truthful. While there‚Äôs no single ‚Äúcorrect‚Äù choice for every scenario, understanding your data and communication goals will help you select a format that highlights the story you want to tell. Whether you‚Äôre comparing categories, showing trends over time, exploring distributions, or revealing relationships, each chart type has strengths and limitations.\nFor a deeper dive into chart selection, visit From Data to Viz, a comprehensive, visual guide that helps you choose the most appropriate chart based on your data structure and communication goals.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/flow-maps/flow-maps.html",
    "href": "notebooks/urban-data-visualization/flow-maps/flow-maps.html",
    "title": "Origin-Destination Flow Maps",
    "section": "",
    "text": "Flow maps are one such way that we can visualize connections and travel between different sets of locations.\nIn the example below, we are mapping travel to hospitals in Calgary. Each line represents a single trip from someone‚Äôs home neighbourhood to a hospital, visualized with a very faint transparency, but when overlaid on top of each other, they highlight overall patterns of visits to these five hospitals.\nCharts like these are sometimes called hub-and-spoke maps or spider diagrams. In the tutorial that follows, we will learn about the data structures behind these maps and how to visualize them, specifically replicating this example.",
    "crumbs": [
      "Urban Data Visualization",
      "Origin-Destination Flow Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/flow-maps/flow-maps.html#prerequisites",
    "href": "notebooks/urban-data-visualization/flow-maps/flow-maps.html#prerequisites",
    "title": "Origin-Destination Flow Maps",
    "section": "Prerequisites",
    "text": "Prerequisites\nPrior knowledge of pandas, geopanda, QGIS, and Inkscape (or similar graphic design software) would be helpful for the following tutorial.\nClick here to download this article as a Jupyter Notebook alongside the datasets required. In the download, there is also a standalone Python script, if you want to run the steps all-at-once or integrate with anything else you have brewing.\nIf you are running the notebook and/or script locally (generally recommended), you will need to use the following libraries. You‚Äôll have to install them (e.g.¬†via pip or conda) if you do not have them installed already.\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import LineString\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Urban Data Visualization",
      "Origin-Destination Flow Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/flow-maps/flow-maps.html#origin-destination-matrices",
    "href": "notebooks/urban-data-visualization/flow-maps/flow-maps.html#origin-destination-matrices",
    "title": "Origin-Destination Flow Maps",
    "section": "Origin-Destination Matrices",
    "text": "Origin-Destination Matrices\nOrigin-Destination matrices encode many people travel from one set of locations (e.g.¬†neighbourhoods) to another set of locations (e.g.¬†hospitals). These are sometimes called flow or trip tables.\nIn urban geography applications, this data often represents people travelling in the city, to or from specific types of places. But it can also be used to represent the transport or travel of other phenomena e.g.¬†products, money, wildlife, etc.\nThis type of data can be based on real observations e.g.¬†from administrative data that includes visiting patients‚Äô addresses or a survey that asks about daily travel. The data can also be simulated, i.e.¬†estimated, based on a model that predicts trips between locations.\nThe toy data that we are going to look at below is the latter, ersatz data of trips to hospitals in Calgary. Let‚Äôs load it and take a look. The data have 3 columns; the first is the unique ID of a Census Dissemination Area (DA), the second is the ID of a hospital, and the third column is the number of visits from each DA to each hospital.\n\nod = pd.read_csv(\"data/od-flows.csv\")\nod.tail(5)\n\n\n\n\n\n\n\n\ndauid\nhospital_id\ntrips\n\n\n\n\n8370\n48062794\n0\n19.0\n\n\n8371\n48062794\n1\n41.0\n\n\n8372\n48062794\n2\n4.0\n\n\n8373\n48062794\n3\n0.0\n\n\n8374\n48062794\n4\n16.0\n\n\n\n\n\n\n\nWhen working with data like this, it is sometimes the case that the data are available in a wide format, where the rows pertain to the origin (e.g.¬†Dissemination Area) and the columns pertain to the destination (e.g.¬†hospitals), instead of the long format shown above.\nIn pandas, it‚Äôs super easy to pivot from long to wide.\n\nod_wide = od.pivot_table(index='dauid', columns='hospital_id', values='trips')\nod_wide.tail(5)\n\n\n\n\n\n\n\nhospital_id\n0\n1\n2\n3\n4\n\n\ndauid\n\n\n\n\n\n\n\n\n\n48062790\n39.0\n49.0\n7.0\n0.0\n31.0\n\n\n48062791\n37.0\n3.0\n11.0\n0.0\n15.0\n\n\n48062792\n25.0\n2.0\n9.0\n0.0\n11.0\n\n\n48062793\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n48062794\n19.0\n41.0\n4.0\n0.0\n16.0\n\n\n\n\n\n\n\nAnd here‚Äôs how to melt from wide to long if you need to.\n\nod_wide.reset_index().melt(\n    id_vars='dauid', \n    value_vars=[0,1,2,3,4], \n    var_name=\"hospital_id\", \n    value_name=\"population\"\n).head(5)\n\n\n\n\n\n\n\n\ndauid\nhospital_id\npopulation\n\n\n\n\n0\n48060056\n0\n20.0\n\n\n1\n48060057\n0\n16.0\n\n\n2\n48060058\n0\n20.0\n\n\n3\n48060059\n0\n14.0\n\n\n4\n48060060\n0\n24.0\n\n\n\n\n\n\n\nWe also have location data for the centroids of Dissemination Areas (DAs) and the hospitals. Let‚Äôs load these and quickly plot them to take a look:\n\nda = gpd.read_file(\"data/da-2021-centroids.geojson\")\nda_poly = gpd.read_file(\"data/da-2021-polygons.geojson\")\nhp = gpd.read_file(\"data/hospital-locations.geojson\")\n\nfig, ax = plt.subplots(figsize=(6,6))\nda_poly.plot(\n    ax = ax,\n    edgecolor = 'LightGray',\n    linewidth = 0.5,\n    color = \"White\"\n)\nda.plot(\n    ax = ax,\n    color = 'LightBlue',\n    markersize = 4\n)\nhp.plot(\n    ax = ax,\n    color = 'Black',\n    markersize = 22\n).set_axis_off()\n\n\n\n\n\n\n\n\nFor a flow map, we want to connect the two sets of points, and then style the lines based a weight connecting them, in this case number of trips between them.\nFor this, we are essentially creating a straight line geometry for every row in od. We can do this by first joining in the coordinates to the od flow matrix\n\nda['dauid'] = da['name'].astype('int64')\nodm = od.merge(\n    da, \n    how='left', \n    on = \"dauid\"\n).merge(\n    hp, \n    how='left', \n    left_on='hospital_id',\n    right_on='id'\n)\n\nThen we use some shapely magic, specifically the LineString function to combine two point geometries into a line geometry. The lambda function (a small single expression function) applies this to each row in the GeoDataFrame.\n\nodm = gpd.GeoDataFrame(\n    {\n        'dauid': odm['dauid'],\n        'hospital_id': odm['hospital_id'],\n        'trips': odm['trips'],\n        'geometry': odm.apply(\n            lambda x: LineString([x['geometry_x'], x['geometry_y']]), axis=1\n        )\n    }\n).set_crs(epsg=\"4326\")\n\n# saving to file\nodm.to_file(\"data/od-flow-lines.geojson\", driver=\"GeoJSON\")\n\nLet‚Äôs plot the result!\n\nfig, ax = plt.subplots(figsize=(6,6))\nodm.plot(\n    ax = ax,\n    linewidth = 0.25,\n    color = 'Black'\n).set_axis_off()\n\n\n\n\n\n\n\n\nThat‚Äôs a bit of a mess, but it looks like the lines are all there. There are a few simple ways to distinguish lines with more trips, and dim those with fewer. The first is to have the line width be a function of the number of trips\n\nfig, ax = plt.subplots(figsize=(6,6))\nodm.plot(\n    ax = ax,\n    linewidth = odm[\"trips\"] / 1000,\n    color = 'Black'\n).set_axis_off()\n\n\n\n\n\n\n\n\nWe can also plot based on the function of the opacity of the line (i.e.¬†less transparency if the connection has more trips). In the plot function, alpha controls the opacity, and ranges from 0 to 1. To normalize the trips column, we divide by its max value.\n\nfig, ax = plt.subplots(figsize=(6,6))\nodm.plot(\n    ax = ax,\n    linewidth = 1,\n    alpha = odm['trips'] / odm['trips'].max(),\n    color = 'Black'\n).set_axis_off()\n\n\n\n\n\n\n\n\nLet‚Äôs try to better distinguish flows to each hospital, i.e.¬†to better show what the catchment area is of visitors to each. We can do this the help of a little colour! Below we combine categorical colouring with dynamic opacity styling used above.\n\n# specifying the colour and opacity of each line\n\ncolour_mapping = {\n    0: \"#DC4633\",  \n    1: \"#8DBF2E\",\n    2: \"#F1C500\",\n    3: \"#00A189\",\n    4: \"#6FC7EA\",\n}\nodm[\"colour\"] = odm[\"hospital_id\"].map(colour_mapping)\n\nodm[\"opacity\"] = odm[\"trips\"] / 250\nodm['opacity'] = odm['opacity'].clip(upper=1)\n                   \n# plotting them all on a single plot!\n    \nfig, ax = plt.subplots(figsize=(7,7), facecolor='black')\n\nda_poly.plot(\n    ax = ax,\n    edgecolor = '#484848',\n    linewidth = 2,\n    color = \"Black\"\n)\nda_poly.plot(\n    ax = ax,\n    edgecolor = '#2B2B2B',\n    linewidth = 0.5,\n    color = \"Black\"\n)\n\nfor hospital in [0,1,2,3,4]:\n    odp = odm[odm[\"hospital_id\"] == hospital]\n    odp.plot(\n        ax=ax, \n        linewidth=1, \n        alpha=odp[\"opacity\"], \n        color=odp['colour'], \n        zorder = 1\n    )\n    \nfor hospital in [0,1,2,3,4]:\n    odp = odm[odm[\"hospital_id\"] == hospital]\n    odp.plot(\n        ax=ax, \n        linewidth=2, \n        alpha=odp[\"opacity\"] / 3, \n        color=odp['colour'], \n        zorder = 2\n    )\n    \nhp.plot(\n    ax = ax,\n    color = 'White',\n    markersize = 40,\n    zorder = 3\n)\n\nax.set_title(\n    'Travel to Hospitals in Calgary', \n    fontsize=10,\n    loc = \"left\"\n).set_color('LightGray')\n\nax.set_axis_off()\n\nfig.savefig('images/calgary-hospital-map-all-python-export.png')\n\n\n\n\n\n\n\n\nThe above map views nicely, but it can be a bit difficult to parse out travel to any specific hospital. One solution to this are small multiples, where we create 5 smaller plots, each focusing on a specific hospital.\n\nfig, ax = plt.subplots(ncols=5, nrows=1, figsize=(15,5), facecolor='black')\n\nfor hospital in [0,1,2,3,4]:\n    \n    # data layers\n    \n    odp = odm[odm[\"hospital_id\"] == hospital]\n    da_poly.plot(\n        ax = ax[hospital],\n        edgecolor = '#484848',\n        linewidth = 2,\n        color = \"Black\"\n    )\n    da_poly.plot(\n        ax = ax[hospital],\n        edgecolor = '#2B2B2B',\n        linewidth = 0.5,\n        color = \"Black\"\n    )\n    odp.plot(\n        ax=ax[hospital], \n        linewidth=1, \n        alpha=odp[\"opacity\"], \n        color=odp['colour'], \n        zorder = 1\n    )\n    hp[hp[\"id\"] == hospital].plot(\n        ax = ax[hospital],\n        color = 'White',\n        markersize = 10,\n        zorder = 3\n    ).set_axis_off()\n    \n    # titles\n    \n    ax[hospital].set_title(\n        hp.loc[hospital, 'facility_name'], \n        fontsize=8,\n        loc='center'\n    ).set_color('White')\n\n# overall title and save to file\n    \nfig.suptitle(\n    'Travel to Hospitals in Calgary', \n    fontsize=13\n).set_color('White')\n\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])\nfig.subplots_adjust(top=0.9, bottom=0.1)\n\nfig.savefig('images/calgary-hospital-map-multipes-python-export.png')\n\n\n\n\n\n\n\n\nThat‚Äôs it for now. Thanks!",
    "crumbs": [
      "Urban Data Visualization",
      "Origin-Destination Flow Maps"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html",
    "title": "Programming with Python and computational notebooks",
    "section": "",
    "text": "Coding, or computer programming, is the act of giving a computer written instructions for a task or set of tasks. According to Wikipedia, it ‚Äúinvolves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages.‚Äù\nProgramming languages, just like Python and R and JavaScript can be thought of similarly to languages that people speak and write, like Spanish or Arabic ‚Äì each language has its own set of rules and different syntax, but (most) ideas can be translated from one language to another. Instead of using language to communicate with other people, you write code to give directions to a computer. In data analysis, these directions could be to summarize data in a table, create a chart or a map, or generate a statistical model, for example.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-programming",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-programming",
    "title": "Programming with Python and computational notebooks",
    "section": "Why learn programming?",
    "text": "Why learn programming?\nLearning programming gives you more flexibility and control when working with urban and spatial data. It helps you handle larger datasets, repeat analyses efficiently, and create custom maps or tools that aren‚Äôt possible point-and-click software like Excel and GIS. It‚Äôs a practical skill that can really open up what you‚Äôre able to do in this field.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-python",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-python",
    "title": "Programming with Python and computational notebooks",
    "section": "Why learn Python?",
    "text": "Why learn Python?\nPython is widely used programming language, especially in urban and spatial analysis because it‚Äôs easy to learn relative to other languages and has a strong ecosystem of libraries for working with data, maps, and models. Plus, it‚Äôs open source and has a huge community, which means lots of learning resources and support.\nThis notebook introduces the Python programming language and Jupyter notebooks. It will cover:\n\nA brief overview of programming, Python, integrated development environments (IDEs), and computational notebooks\nHow to run a Python (.py) script in the terminal\nHow to run code using Jupyter Notebook (.ipynb) in Visual Studio Code\nThe basics of coding in Python, including:\n\nVariables\nSimple math\nLists and dictionaries\nIf statements\nFor and while loops\nFunctions\n\n\nWhile we‚Äôll mostly be looking at Python, the same concepts are applicable across many other languages and software.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#how-does-python-work",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#how-does-python-work",
    "title": "Programming with Python and computational notebooks",
    "section": "How does Python work?",
    "text": "How does Python work?\nWhen you write something in English or another language, you need to choose an environment to write in. For example, if you‚Äôre writing an email, you might use Gmail or Outlook. If you‚Äôre writing a report, you might use Google Docs or Microsoft Word. Or maybe you prefer writing on parchment with a quill pen!\nPython is a programming language that lets you write instructions in a plain text file, often called a script, which the Python interpreter on your computer reads and runs line by line. You don‚Äôt need to compile your code like in some other languages‚Äîjust write it, save it as a .py file, and run it directly.\nFor example, I have a file on my computer called my-script.py, a very simple script which simply prints ‚ÄúHello, world!‚Äù\nprint(\"Hello, world!\")\nWhen running this code in your command line should result in something like the following.\nyour-computer-name:~$ python my-script.py \nHello, world!\nThe command line is often referred to as the shell, terminal, console, prompt or various other names.\nIf you are new to this, check out this tutorial for more in-depth instructions on command line Python",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#tools-for-writing-code",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#tools-for-writing-code",
    "title": "Programming with Python and computational notebooks",
    "section": "Tools for writing code",
    "text": "Tools for writing code\nWhen coding, you also have to choose an environment to write in. This is typically called a ‚Äúcode editor‚Äù or ‚Äúintegrated development environment‚Äù (IDE). The simplest editor would simply be the notepad or text editor on your computer. But there are many IDEs to choose from that make coding much easier, Some of the most popular ones for Python are Visual Studio Code (‚ÄúVS Code‚Äù) and PyCharm.\nA main benefit for using a code editor is that it highlights different parts of your code in different colours or styles making it much easier to read.\n\n\n\nScreenshot of part of a Python script in VS Code",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#computational-notebooks",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#computational-notebooks",
    "title": "Programming with Python and computational notebooks",
    "section": "Computational notebooks",
    "text": "Computational notebooks\nThere are many different ways to execute code, or tell the computer to perform the instructions you‚Äôve written. If you write your code in a file that‚Äôs saved with the .py file extension, you can run it all at once, and the computer will follow all of your instructions, one line at a time. This works well for scripts that are complete and do not need to be run in separate chunks.\nIf you are exploring, analyzing, or visualizing data, it is sometimes easier to work in a computational notebook. Computational notebooks are coding environments that not only allow you to write code, but also let you write explanations and show the outputs of your analysis, very similar to pre-digital formats.\n\n\n\nNotebook by Galileo\n\n\nJupyter Notebooks (named after Galileo‚Äôs work!) are often used for data analysis in Python. When using a Jupyter Notebook, you can run code chunk-by-chunk and see the output right below each chunk. For example, you can write a chunk of code that manipulates a dataframe and then look at the first few rows of the dataframe right below the code. The page you are reading now was written in a Jupyter notebook! :)\n\n\n\nExample of a Jupyter notebook that is plotting a map\n\n\nYou can run Jupyter Notebooks in VS Code via the Jupyter Notebook ‚Äúextension‚Äù. You can also work with notebooks via Jupyter Lab (a web-based environment for Jupyter Notebooks) or Sublime Text.\nTo get started in VS Code, you can follow these instructions. To create a notebook, you just need to create a file with the .ipynb extension in whichever directory you want to save it in. To write and run code in the notebook, you will use ‚Äúcells‚Äù that contain chunks of information.\nWhen you run an individual cell, you are telling the computer to follow the instructions you‚Äôve provided in only that cell, ignoring any other cells that are in the notebook. Be careful of the order in which you ‚Äúexecute‚Äù the cells. For example, if you run cell B before running cell A, it doesn‚Äôt matter if cell B is located below cell A ‚Äì the computer will still follow the instructions in B first and A second.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#python-basics",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#python-basics",
    "title": "Programming with Python and computational notebooks",
    "section": "Python basics",
    "text": "Python basics\nNow that you have Jupyter Notebook set up, let‚Äôs code! Below, we‚Äôll cover some of the basic building blocks of Python.\nOpen up a fresh .ipynb file and you can get started building bits of code for each of the topics below\n\nVariables\nA variable is like a labeled entity that stores information (numbers, text, etc.). You can create a variable, give it a name, and assign it a value.\nNote that in the below code, some of the lines are written with a # at the beginning - these are comments. Putting a # in front of a line of code tells the computer not to execute it. You should use comments often to explain what your code is doing, either for someone else who might need to understand it or for your future self.\n\n# Assign a value to a variable\nname = \"Alice\"\nage = 30\n\nIn the above cell, we created two variables, one called name and another called age. The name variable is a string because it is a sequence of characters. The computer knows this is a string because we enclosed the text, Alice, in quotes. Single or double quotes both work here.\nThe age variable is an integer because it is a numeric value without decimals. You can see the data type with type([name of object]) like below:\n\ntype(age)\n\nint\n\n\nIf we print the variable, it will show us the variable‚Äôs value:\n\nprint(age)\n\n30\n\n\nWe can also re-assign variables, which will change their value. Now when we print the value of age, it will show 31 instead of 30:\n\n# Re-assign `age` variable with a new value\nage = 31\nprint(age)\n\n31\n\n\nRemember that the computer interprets your code in the order you run the cells, not in the order of the cells in the notebook. For example, if you ran the above cell that assigns a value of 31 to the age variable before running the cell that assigns the value of 30 to age, the computer would store the value of 31.\n\n\nSimple math\nPython can do simple math, like a calculator:\n\n4 + 3\n\n7\n\n\n\n10/3\n\n3.3333333333333335\n\n\nYou can also use the math module to access more advanced functions, like taking the square root. To use this module, you have to import it first:\n\nimport math # import module\nmath.sqrt(25)\n\n5.0\n\n\n\n\nLists\nA list is a collection of elements which can be accessed by their position. Python uses something called zero-based indexing, which means the first element of a sequence has an index (position) of 0 instead of 1.\nIn the below example, fruits is a variable whose type is a list.\n\n# Assign list to variable called 'fruits'\nfruits = [\"apple\", \"banana\", \"cherry\"]\ntype(fruits)\n\nlist\n\n\n\n# Access first item in list\nprint(fruits[0])\n\napple\n\n\n\n# Access second item in list\nprint(fruits[1])\n\nbanana\n\n\nItems can be appended to lists:\n\n# Add \"orange\" to the list\nfruits.append(\"orange\")\nprint(fruits)\n\n['apple', 'banana', 'cherry', 'orange']\n\n\nWe can check the length of the new list to see how many elements it has:\n\nlen(fruits)\n\n4\n\n\nLearn more about lists here.\n\n\nDictionaries\nA dictionary is a type of object that stores information in pairs: each ‚Äúentry‚Äù in the dictionary has both a key and a value. In the example below, person is a dictionary that contains characteristics ‚Äì specifically the name and age ‚Äì of a person.\n\nperson = {\"name\": \"Alice\", \"age\": 30}\n\nWe can access the value associated with the name of the person:\n\nprint(person[\"name\"])\n\nAlice\n\n\nWe can also add a new key-value pair to the dictionary that represents, in this case, the person‚Äôs job:\n\nperson[\"job\"] = \"Engineer\"\nprint(person)\n\n{'name': 'Alice', 'age': 30, 'job': 'Engineer'}\n\n\nLearn more about dictionaries here.\n\n\nIf statements\nIf statements let your code make decisions. You check a condition (e.g., whether age &gt;= 18), and run different code depending on whether it‚Äôs true or false.\n\nage = 18\n\nif age &gt;= 18:\n    print(\"You're an adult!\")\nelse:\n    print(\"You're a minor.\")\n\nYou're an adult!\n\n\nLearn more about if statements here.\n\n\nFor loops\nA for loop repeats code for each item in a list or range. For example:\n\nfor fruit in [\"apple\", \"banana\", \"cherry\"]:\n    print(fruit)\n\napple\nbanana\ncherry\n\n\nLearn more about for loops here.\n\n\nWhile loops\nA while loop repeats code as long as a condition is true. In the below example, we start with 0 and keep adding 1 until we get to 3, after which we stop counting:\n\ncount = 0\nwhile count &lt;= 3:\n    print(\"Counting:\", count)\n    count += 1\n\nCounting: 0\nCounting: 1\nCounting: 2\nCounting: 3\n\n\nLearn more about while loops here.\n\n\nFunctions\nA function is a reusable block of code that performs a task. You ‚Äúdefine‚Äù it (write the code that performs the task) once and ‚Äúcall‚Äù it (run that pre-defined code) whenever you want. In the below example, we define the function greet so that when it is called, it prints ‚ÄúHello, [name]!‚Äù where name is an argument (also known as a parameter) that‚Äôs passed into the function.\n\n# Define the function\ndef greet(name):\n    print(f\"Hello, {name}!\")\n\n# Call the function\ngreet(\"Alice\")\n\nHello, Alice!\n\n\nNot all functions need arguments. For example:\n\n# Define the function\ndef howdy():\n    print(\"Howdy!\")\n\n# Call the function\nhowdy()\n\nHowdy!\n\n\nSome functions have more than one argument. For example:\n\n# Define the function\ndef add_numbers(a, b):\n    c = a + b\n    print(c)\n\n# Call the function\nadd_numbers(8, 7)\n\n15\n\n\nWhile all of the example functions listed above result in something being printed out, most functions do more than that. For example, a single function can filter a dataset based on a set of values, manipulate the resulting dataset, and create a plot.\nLearn more about functions here.\n\n\nLibraries\nLibraries are collections of related functions that do things like analyze data, draw charts, or work with maps.\nMany libraries like math for a variety of mathematical operations, os for interacting with files on your computer, and random for generating random numbers, typically come automatically installed with Python.\nThere are many popular freely available libraries available‚Äîlike pandas for working with data tables, matplotlib for plotting, and geopandas for spatial data‚Äîthat can help you do a lot with just a few lines of code.\nTo include functions that are part of a library, we use import at the top of our script. For example, lets import a function for a random number generator, and use it create our own d20 dice rolling function.\n\nimport random\n\n# Function to roll a 20-sided dice\ndef roll_d20():\n    return random.randint(1, 20)\n\n# Simulate rolling the dice\nroll = roll_d20()\nprint(roll)\n\n14\n\n\nWhen using external libraries like pandas, we need to install the library before we can import and then use it. In Jupyter Notebook, we can do this by running !pip install [name of library]. If you are running Python in your command line, you can simply run pip install [name of library]\nIf you don‚Äôt already have pip installed on your computer, follow these instructions. If you‚Äôre still having trouble, try Googling your specific questions or asking a chatbot for step-by-step instructions!",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html",
    "href": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html",
    "title": "Statistical Foundations",
    "section": "",
    "text": "So far, we‚Äôve learned how to handle data - but we also need to know how to understand it and analyze it in convincing ways. Statistics is the core of this - it can reveal what is typical or an outlier, what relationships exist between different variables, and whether assumptions we have are likely or not.\nIn order to do this, we are going to use the Python libraries numpy and scipy. Both of these libraries offer greater mathematical precision and access to a wide variety of statistical methods and tests, which we can apply to a data set loaded in pandas. For the sake of this tutorial, we also use matplotlib to show some intuitive visualizations - but you don‚Äôt need to worry about tinkering with these yet.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nFor this tutorial, we are going to use election and census data that was adapted for a project published by the School of Cities looking at immigrant vote patterns in the GTA. The data we are using is adapted from the 2025 Ontario election and the 2021 Canadian census. The data frame has the following columns: - riding_name: the name of the riding - num_pop_tot: the number of people in the riding - num_imm_tot: the number of immigrants in the riding - num_imm_new: the number of new immigrants in the riding - avg_hou_inc: the average household income in the riding - num_vm_tot: the number of visible minority individuals in the riding - cons_pct: the vote percent for the Progressive Conservative party - lib_pct: the vote percent for the Liberal Party - ndp_pct: the vote percent for the New Democratic Party (NDP) - oth_pct: the vote percent for other parties\n\ndf = pd.read_csv('./data/ont-ed_stats_2025.csv')\ndf.head()\n\n\n\n\n\n\n\n\nriding_name\nnum_pop_tot\nnum_imm_tot\nnum_imm_new\navg_hou_inc\nnum_vm_tot\ncons_pct\nlib_pct\nndp_pct\noth_pct\n\n\n\n\n0\nBrampton Centre\n102309.0\n48275.0\n14985.0\n103369.0\n68880.0\n51.85\n33.92\n8.77\n5.45\n\n\n1\nScarborough‚ÄîRouge Park\n102256.0\n52637.0\n8294.0\n120745.0\n77090.0\n52.98\n34.75\n10.38\n1.89\n\n\n2\nScarborough North\n94688.0\n60423.0\n11513.0\n103151.0\n87647.0\n52.98\n34.75\n10.38\n1.89\n\n\n3\nAjax\n115392.0\n47716.0\n8721.0\n137570.0\n74759.0\n44.16\n44.96\n7.01\n3.87\n\n\n4\nBeaches‚ÄîEast York\n106811.0\n33588.0\n9031.0\n129975.0\n39034.0\n21.38\n51.17\n22.94\n4.51",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical Foundations"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#descriptive-statistics-the-shape-of-the-data",
    "href": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#descriptive-statistics-the-shape-of-the-data",
    "title": "Statistical Foundations",
    "section": "Descriptive statistics: the shape of the data",
    "text": "Descriptive statistics: the shape of the data\n\nCentral tendency\nWhat is ‚Äútypical‚Äù in a set of data? This is at the heart of the notion of the central tendency in statistics. There are three common ways to measure this, with a accompanying functions. - Mean (np.mean()): the average of all values - Median (np.median()): the ‚Äúmiddle‚Äù value if you order them, ie. at the 50th percentile - Mode (stats.mode()): the most common value\nLet‚Äôs compute some of these central tendency statistics for the NDP, and compare them to the distribution of vote shares in all ridings.\n\nparty_var = 'ndp_pct'\n\nmean = np.mean(df[party_var])\nmedian = np.median(df[party_var])\nmode = stats.mode(df[party_var])\n\nprint(f\"Mean: {mean:.2f}\")\nprint(f\"Median: {median:.2f}\")\nprint(f\"Mode: {mode.mode} (appears {mode.count} times)\")\n\nMean: 14.11\nMedian: 6.88\nMode: 3.95 (appears 2 times)\n\n\n\nplt.figure(figsize=(8, 5))\nplt.hist(df[party_var], bins=20, color='orange', edgecolor='black')\n\nplt.axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:.2f}%')\nplt.axvline(median, color='blue', linestyle='--', linewidth=2, label=f'Median: {median:.2f}%')\nplt.axvline(mode.mode, color='green', linestyle='--', linewidth=2, label=f'Mode: {mode.mode:.2f}%')\n\nplt.title('Distribution of Vote Percentages by Riding')\nplt.xlabel('Vote Percentage (%)')\nplt.ylabel('Number of Ridings')\nplt.grid(axis='y', alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotice the difference between the mean and the median. While the mean accounts for all of the data directly, it is also sensitive to outliers and can be pulled in one direction or another.\nTry setting the variable party_var to different parties to see how the matter of mean vs.¬†median plays out for different distributions.\n\n\nDispersion\nSimilar to how we ask what is typical in a set of data, it‚Äôs also important to ask how that data varies. This is what we refer to when we talk about the dispersion of a distribution, and there are four common measures (with functions): - Standard deviation (np.std()): How spread out data points are from the mean, in the same units as the original data. - Variance (np.var()): The average squared deviation from the mean, representing how wildly individual values differ from the average or overall unevenness. - Range (np.max() - np.min()): The difference between the maximum and minimum values, showing the total spread of the dataset. - Interquartile range (np.percentile(..., [25, 75])): The range of the middle 50% of data (Q3‚ÄìQ1), reducing sensitivity to outliers.\nLet‚Äôs take a look at how the number of immigrants across different ridings varies, and compare it with the mean and median.\n\ncol_var = 'num_imm_tot'\n\nmean = np.mean(df[col_var])\nmedian = np.median(df[col_var])\nstd_dev = np.std(df[col_var], ddof=1)\nvariance = np.var(df[col_var])\ndata_range = np.max(df[col_var]) - np.min(df[col_var])\nq1, q3 = np.percentile(df[col_var], [25, 75])\niqr = q3 - q1\n\nprint(f\"Mean: {mean:.2f}\")\nprint(f\"Median: {median:.2f}\")\nprint('===')\nprint(f\"Std Dev: {std_dev:.2f}\")\nprint(f\"Variance: {variance:.2f}\")\nprint(f\"Range: {data_range:.2f}\")\nprint(f\"IQR: {iqr:.2f}\")\n\nMean: 52330.33\nMedian: 54255.00\n===\nStd Dev: 13630.44\nVariance: 182348372.93\nRange: 62964.00\nIQR: 18211.50\n\n\n\nplt.figure(figsize=(10, 6))\nax = plt.gca()\n\n# Histogram\nn, bins, patches = plt.hist(df[col_var], bins=20, color='#2ca02c', edgecolor='black', alpha=0.7)\n\n# Central tendency and dispersion\nplt.axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:,.0f}')\nplt.axvline(median, color='blue', linestyle='--', linewidth=2, label=f'Median: {median:,.0f}')\n\nplt.axvspan(mean - std_dev, mean + std_dev, color='red', alpha=0.1, label=f'¬±1 Std Dev: {std_dev:,.0f}')\nplt.axvspan(q1, q3, color='blue', alpha=0.1, label=f'IQR: {iqr:,.0f}')\n\n# Annotations\nplt.title('Distribution of Immigrant Population by Riding', pad=20, fontsize=14)\nplt.xlabel('Number of Immigrants', fontsize=12)\nplt.ylabel('Number of Ridings', fontsize=12)\nplt.grid(axis='y', alpha=0.2)\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTake a look at how different measures of dispersion capture different parts of the data. Once again, set col_var to a different variable or two and examine the different measures of dispersion and how they play out.",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical Foundations"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#bivariate-analysis-finding-similarities-in-the-data",
    "href": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#bivariate-analysis-finding-similarities-in-the-data",
    "title": "Statistical Foundations",
    "section": "Bivariate analysis: finding similarities in the data",
    "text": "Bivariate analysis: finding similarities in the data\n\nCorrelation\nCorrelation measures how closely two variables move together, ranging from -1 (as X increases, Y decreases), to +1 (as X increases, Y increases). A value near 0 indicates no linear association. While it is acceptable to use the word ‚Äòrelationship‚Äô here instead of ‚Äòassociation‚Äô, it‚Äôs worth noting that a strong correlation is not enough to show causation ‚Äì namely, if we had a correlation value of 1, we can‚Äôt definitively say that Y increases because X increases, only that they increase together.\nWe‚Äôll be looking at a very common measure called Pearson correlation. It measures how closely two variables follow a straight-line relationship, ideal for normally distributed data with linear trends (e.g., height vs.¬†weight). While we won‚Äôt look at it today, it‚Äôs worth independently looking into ‚ÄúSpearman Rank Correlation‚Äù as well - which can capture nonlinear trends.\nWe‚Äôll look at the example of vote share for the Progressive Conservatives and the number of immigrants in a riding below. In the project that this data was drawn from, we used correlation to show how immigrant voters in the GTA are shifting conservative over time.\n\ncensus_var = 'num_imm_tot'\nparty_var = 'cons_pct'\n\nr, p_val = stats.pearsonr(df[census_var], df[party_var])\nprint(f\"Pearson corr: {r:.2f} (p-value: {p_val:.3f})\")\n\nPearson corr: 0.40 (p-value: 0.003)\n\n\nHere we can see a correlation between the two variables we have chosen. Try setting census_var and party_var to different column values and see if there is a correlation between those as well, or not!",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical Foundations"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#linear-regression-relationships-between-variables",
    "href": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#linear-regression-relationships-between-variables",
    "title": "Statistical Foundations",
    "section": "Linear regression: relationships between variables",
    "text": "Linear regression: relationships between variables\n\nSimple regression\nLinear Regression identifies the straight-line relationship between two variables, allowing you to predict outcomes (e.g., voting percentages) based on another factor (e.g., immigrant population). It calculates a ‚Äúbest-fit‚Äù line that minimizes the distance between all data points and the line itself, summarized by the equation y = slope  x + intercept*.\n\ncensus_var = 'num_imm_tot'\nparty_var = 'cons_pct'\n\nx = df[census_var].values  # Independent variable\ny = df[party_var].values     # Dependent variable\n\n# Fit regression\nslope, intercept, r_value, p_value, _ = stats.linregress(x, y)\nr_squared = r_value ** 2\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.scatter(x, y, color='blue', alpha=0.5, label='Actual Data')\nplt.plot(x, intercept + slope * x, 'r-', label=f'Regression Line: $R¬≤$={r_squared:.2f}\\nslope={slope:.5f}')\nplt.xlabel('Immigrant Population')\nplt.ylabel('Conservative Vote %')\nplt.title('Predicting Votes from Demographics', pad=15)\nplt.legend()\nplt.grid(alpha=0.2)\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Model Equation: {party_var} = {intercept:.2f} + {slope:.5f} * {census_var}\")\nprint(f\"R¬≤ = {r_squared:.3f} (Explains {r_squared*100:.1f}% of variance)\")\nprint(f\"p-value = {p_value:.4f} {'(Significant)' if p_value &lt; 0.05 else '(Not significant)'}\")\n\nModel Equation: cons_pct = 23.99 + 0.00039 * num_imm_tot\nR¬≤ = 0.161 (Explains 16.1% of variance)\np-value = 0.0026 (Significant)\n\n\nNot all regression lines are made equal, and R¬≤ measures how well the regression line fits the data, ranging from 0 (no fit) to 1 (perfect fit). It answers: ‚ÄúWhat percentage of variation in voting patterns can be explained by immigrant population?‚Äù. In the above example, we can say that 16.1% of the differences in conservative votes across ridings are predictable from immigrant numbers‚Äîthe rest is due to other factors.\nNow try the following two exercises to examine outcomes: - Change the x-variable to avg_hou_inc. Does wealth predict conservative votes better than immigrant population? - Add ndp_pct as y-variable. Is the relationship positive or negative?\n\n\nOther kinds of regression\nWhile linear regression is ideal for modeling straight-line relationships, real-world data often requires more flexible approaches. Below are key alternatives with use cases relevant to political/demographic data, along with their Python implementations: - Multiple Linear Regression: Modeling how multiple demographic factors jointly influence voting patterns (e.g., predicting Conservative vote share using both immigrant population and average income). - Polynomial Regression: Modeling curved relationships (e.g., voter turnout vs.¬†age, where middle-aged groups vote more than very young or elderly). - Logistic Regression: Predicting binary outcomes (e.g., whether a riding will vote Conservative (1) or not (0) based on income thresholds). - Ridge/Lasso Regression: Handling multicollinearity (e.g., when immigrant population and visible minority numbers are strongly correlated).\nIn case you want to do these more complex kinds of regression, the typical go to library is scikit-learn.",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical Foundations"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#hypothesis-testing-making-conclusions",
    "href": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#hypothesis-testing-making-conclusions",
    "title": "Statistical Foundations",
    "section": "Hypothesis testing: making conclusions",
    "text": "Hypothesis testing: making conclusions\nHypothesis testing evaluates whether observed patterns in data are statistically significant or likely due to random chance. A t-test compares the means of two groups (e.g., Conservative vote share in high- vs.¬†low-income ridings) to determine if their difference is meaningful.\nThe t-statistic measures the size of the difference relative to the variability in the data‚Äîthink of it like a ‚Äúsignal-to-noise ratio‚Äù where if the group difference (signal) stands out from natural variation (noise). Larger absolute values (e.g., |t| &gt; 2) suggest stronger evidence against no difference (a strong ‚Äúsignal‚Äù), and the sign indicates direction (e.g., positive = Group A &gt; Group B).\nThe p-value then calculates how likely we‚Äôd see this t-statistic if no true difference existed. If p &lt; 0.05, we reject the null hypothesis.\nThere‚Äôs two different t-tests we‚Äôll illustrate today. - One-sample: Compares data to a fixed number - Two-sample: Compares two datasets to each other.\n\nOne-Sample t-test\nOur data set only includes ridings within the Greater Toronto Area (GTA) - but more than half of the ridings exist in the rest of Ontario. In the 2025 election, the Conservatives won 43% of the vote - does the average vote share for Conservatives in the GTA vary from Ontario in full?\n\nparty_var = 'cons_pct'\ntotal_party_vote = 43\n\nsample_data = df[party_var]\nt_stat, p_value = stats.ttest_1samp(sample_data, popmean=total_party_vote)\n\nprint(f\"t-statistic: {t_stat:.2f}, p-value: {p_value:.4f}\")\nprint(\"Significantly different!\" if p_value &lt; 0.05 else \"No significant difference.\")\n\nt-statistic: 0.76, p-value: 0.4497\nNo significant difference.\n\n\n\nplt.figure(figsize=(8, 4))\nplt.hist(sample_data, bins=15, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(total_party_vote, color='red', linestyle='--', label=f'Vote share in full election ({total_party_vote}%)')\nplt.xlabel('Vote %')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTwo-Sample t-test\nWe‚Äôve talked a lot about immigrant ridings as a whole, but we can also examine different subsets of them. For example: Do high-immigrant ridings vote differently than low-immigrant ridings?\n\ncensus_var = 'num_imm_tot'\nparty_var = 'cons_pct'\n\n# Split data into two groups (median split)\nmedian_data = df[census_var].median()\nhigh_data = df[df[census_var] &gt; median_data][party_var]\nlow_data = df[df[census_var] &lt;= median_data][party_var]\n\n# Independent t-test\nt_stat, p_value = stats.ttest_ind(high_data, low_data)\n\nprint(f\"t-statistic: {t_stat:.2f}, p-value: {p_value:.4f}\")\nprint(\"Significantly different!\" if p_value &lt; 0.05 else \"No significant difference.\")\n\nt-statistic: 2.52, p-value: 0.0149\nSignificantly different!\n\n\n\nplt.figure(figsize=(8, 4))\nplt.hist(high_data, bins=15, alpha=0.5, label=f'High-{census_var} ridings')\nplt.hist(low_data, bins=15, alpha=0.5, label=f'Low-{census_var} ridings')\nplt.xlabel('Vote %')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFinally, substitute out the census and party variables for different versions and run this analysis. Assess whether there are significant differences in the new hypothesis that you are testing.",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical Foundations"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "A lot of urban datasets are directly linked to specific places, e.g.¬†addresses, streets, neighbourhoods, political or administrative boundaries, etc.\nData that include place-based information are often called spatial, geographic, or geospatial data Geographic Information Systems (GIS) are tools and software for analyzing, processing, and visualizing spatial data.\nThis page will cover the basics of spatial data and how we can view and interact with this data in the software QGIS (you will need to download QGIS to work on the hands-on part of this tutorial).\n\n\nA spatial dataset is a combination of‚Ä¶\n\nattribute data (the what)\nlocation data and spatial dimensions (the where)\n\nSpatial data, this combination of attribute and location data, can be organized and represented in a number of different formats.\nFor example, a city can be represented on a map via a single point with a label (e.g.¬†based on latitude and longitude coordinates). Or a city can be represented as a polygon, based on on it‚Äôs administrative boundary\nImportantly, there are always uncertainty about the level of accuracy, precision, and resolution of spatial data. Spatial data are abstractions of reality, and thus have some loss of information when used for visualization and analysis. Any analysis can only be as good as the available data.\nThe two most common forms of spatial data are vector data and raster data.\n\n\n\nExamples of spatial data\n\n\n\n\nVector data uses geographic coordinates, or a series of coordinates, to create points, lines, and polygons representing real-world features.\ne.g.¬†in the map below (a screenshot of OpenStreetMap) lines are used to represent roads and rail, points for retail, polygons for parks and buildings, etc.\n\n\n\nScreenshot of OpenStreetMap\n\n\nSpatial data can be stored as columns in traditional table-based formats (e.g.¬†.csv), but there are also a wide range of data formats specific to spatial data. These are some of the most common:\n\nGeoJSON .geojson\nGeoPacakge .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nHere is an example of a single point location of a city stored in a .geojson file. .geojson is a standardized data schema .json format for spatial data.\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [-80.992, 46.490]\n            },\n            \"properties\": {\n                \"city_name\": \"Sudbury\"\n            }\n        }\n    ]\n}\n\n\n\nRaster data represents space as a continuous grid with equal cell sizes. Each cell contains a value pertaining to the type of feature it represents. These values can be quantitative (e.g.¬†elevation) or categorical (e.g.¬†type of land use). Common examples of raster data include Digital Elevation Models (DEMs), satellite imagery, and scanned images like historical maps.\ne.g.¬†the map below shows a DEM for Toronto at two different scales.\n\n\n\nDEM of Toronto\n\n\n\n\n\n\nWe use Geographic Information Systems (GIS) to analyze, manipulate, and visualize spatial data.\nWhy is GIS useful?\n\nexplore spatial patterns and relationships in data\ncreate maps for publications\ngenerate new data, either ‚Äúby hand‚Äù or via spatial relationships from other data (e.g.¬†through spatial queries)\nperform spatial analysis (i.e.¬†statistical methods applied to spatial data)\n\nGIS is often thought of as more than just a tool or piece of software. It can refer to all aspects of managing and analyzing digital spatially referenced data.\nThe power of GIS software and tools is the ability to work with data stored in different layers (e.g.¬†a layer for roads, another for buildings, and so on) in conjunction with each other. These layers can be visualized and analyzed relative to each other based on their spatial relationships.\n\n\n\nSimple AI generated schematic of layers\n\n\nGIS software usually links to data stored elsewhere on a computer, rather than in a project file. If the source location of the data (i.e.¬†which folder it‚Äôs in) changes, then this will have to be updated in the GIS project. If data are edited in GIS, it will update the data in its source location.\nThe open-source QGIS and the proprietary ESRI ArcGIS are the two most used desktop GIS software. ESRI‚Äôs suite of tools are often used by larger corporate and government organizations while QGIS is more used by small consultants and freelancers, non-profits, and academia. Many spatial data processing, visualization, and analyses steps also have equivalents in Python, R, SQL and other programming languages via specific libraries\nWe‚Äôll be working with QGIS and Python, because they free and work across multiple operating systems, and are commonly used across different areas of work and research. But pretty much everything we‚Äôll show can be accomplished across different software options, the buttons and steps will just be a bit different.\nLearning the core analytical and visualization concepts and ideas are much more important than exactly where to click or what specific functions to run.\n\n\n\nA CRS is a framework for describing the geographic position of location-based data. It tells your GIS software where your data are located on the Earth. Without a CRS, your data would just be a bunch of numbers without an ability to place them on a map and relate them to other data.\nThere are thousands of CRSs. Each CRS has specific attributes about its position and one main set of spatial units (e.g.¬†a CRS will measure location in degrees longitude/latitude, metres, miles, etc.).\nProbably the most common CRS is WGS84, which uses longitude/latitude to link data to the Earth‚Äôs surface.\nWhen doing spatial analyses and processing where we are comparing two or more datasets to each other, it is important that they are in the same CRS.\n\n\nCRS are often paired with map projections. This is sometimes called a Projected CRS.\nThe earth is a 3D globe, but most maps are represented on 2D surfaces (e.g.¬†on a screen, piece of paper). Map projections are mathematical model to flatten the earth to create a 2D view.\nEvery projection distorts shape, area, distance, or direction in some way or another. Different map projections distort the Earth in different ways. Here are some examples! For more check out this projection transition tool\n\n\n\nQGIS screenshot without data\n\n\nFor doing analysis and an urban scale, it is important that we choose a map projection that conserves area and distances in both X and Y directions (i.e.¬†space is not being distorted in one particular direction more than others)\nFor example, these are two aerial images of Toronto, the left uses a local Mercator projection which does not distort data at a local scale, while the right is a rectangular projection where degrees latitude are equal shown at the same scale as degrees longitude.\nA general recommendation for urban-scale analyses is to use a Universal Transverse Mercator (UTM) projected CRS, which conserves area, distance, and direction along bands of the earth. This is a good tool for finding which UTM zone your region is in.\n\n\n\n\nLet‚Äôs use QGIS to quickly view a few spatial datasets. When you open up QGIS, it should look something like this:\n\n\n\nQGIS screenshot without data\n\n\nThe Browser panel is used for finding and loading data on your computer or from external sources, each line and symbol is for a different data source. The Layer panel will list all the datasets that are loaded into your project. The big white space will populate with data once it is loaded. The buttons at the top include a variety of options for loading data, saving your project, exploring your map, and various common processing and analytical tools.\n\n\nLet‚Äôs begin by adding in a basemap. Basemaps are often used to provide geographic reference for other data that we load into QGIS.\nWe‚Äôll add a basemap of OpenStreetMap, a free crowd-sourced map of the world, available as XYZ raster tiles. To do this, go to Layer - Data Source Manager or simply click on the Data Source Manager button highlighted below. When you open the Data Source Manager you have options for a wide variety of data types to add to your map.\n\n\n\nData source manager location in QGIS\n\n\n\n\n\nAdding an XYZ tile\n\n\nHere is the URL to paste into QGIS for adding an OpenStreetMap basemap: https://tile.openstreetmap.org/{z}/{x}/{y}.png.\nWhen you click ‚ÄòAdd‚Äô it should be added to the map. You can use the navigation buttons or just scroll on your mouse to zoom to your city.\n\n\n\nQGIS with OpenStreetMap\n\n\nLet‚Äôs now add some vector data to the map. We‚Äôve pre-downloaded two datasets from the City of Toronto‚Äôs Open Data Portal - City Wards (polygons) - Library Locations (points)\nThese can be added via the Data Source Manager - Vector, or simply via drag-and-drop from the folder of where they are located on your computer.\n\n\n\nQGIS with Toronto data\n\n\n\n\n\nThe data added to the project will be listed in the Layers panel. The order of the layers determine the order of which they stack on top of each other on the map. In the example above, the libraries are listed at the top of the other two layers, and it thus appears on top of the other two layers on the map.\nTo change the layer order, you can drag individual layers to be above or below any other layer in the Layers panel.\nMost vector data has associated attribute data, e.g.¬†the number of a ward, the name of a library, etc. To view this data, right-click on a layer and then click on Open Attribute Table. In GIS, column names are often called ‚ÄúFields‚Äù\n\n\n\nWhen you load vector data like this into QGIS, the layers have default styling (e.g.¬†colours, sizes, line-widths, etc.).\nThere are tonnes of options in QGIS to change these initial styles. To do so, right-click on a layer, go to Properties, and then Symbology.\nThere are lots of options in here to change the colours, size, and symbols.\nFor example, in the screenshot below, the sizes of the libraries and the line-widths of the wards are increased a bit, the wards are given a transparent fill, and the library symbols are converted to squares.\n\n\n\nQGIS with Toronto data\n\n\nTip! you get extra options if you click on the sub-component of the symbol. e.g.¬†in this example I clicked on Simple Fill to find additional styling options like pattern-based fill styles.\n\n\n\nQGIS symbology example\n\n\nWe‚Äôll explore data-driven styling, where colours or other style options are based on data values, in a future tutorial.\n\n\n\nYou can change the project CRS in QGIS by going to Project - Properties - CRS.\n\n\n\nYou can export and create copies of any of your data layers. This may be useful if you want to change the file format, the CRS, subset the attribute table, or simply just make a copy of the data.\nTo do this, go to Export - Save Features As, and you can adjust the Format, CRS, and other options as needed.\n\n\n\nYou can save a project in QGIS by going to Project and then Save or Save As.\nIt will get saved to a .qgis file. These files save styling, filters, layer orders, map scale and location, and layout settings.\nHowever, .qgis files do not save datasets directly in the file, only paths and links to where the data is stored, either on your computer or from a URL like the OpenStreetMap basemap. Saving only links to data is common in other tools and software.\nTherefore, if you share your project, you‚Äôll need to also share any local datasets it is linking to.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#spatial-data",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#spatial-data",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "A spatial dataset is a combination of‚Ä¶\n\nattribute data (the what)\nlocation data and spatial dimensions (the where)\n\nSpatial data, this combination of attribute and location data, can be organized and represented in a number of different formats.\nFor example, a city can be represented on a map via a single point with a label (e.g.¬†based on latitude and longitude coordinates). Or a city can be represented as a polygon, based on on it‚Äôs administrative boundary\nImportantly, there are always uncertainty about the level of accuracy, precision, and resolution of spatial data. Spatial data are abstractions of reality, and thus have some loss of information when used for visualization and analysis. Any analysis can only be as good as the available data.\nThe two most common forms of spatial data are vector data and raster data.\n\n\n\nExamples of spatial data\n\n\n\n\nVector data uses geographic coordinates, or a series of coordinates, to create points, lines, and polygons representing real-world features.\ne.g.¬†in the map below (a screenshot of OpenStreetMap) lines are used to represent roads and rail, points for retail, polygons for parks and buildings, etc.\n\n\n\nScreenshot of OpenStreetMap\n\n\nSpatial data can be stored as columns in traditional table-based formats (e.g.¬†.csv), but there are also a wide range of data formats specific to spatial data. These are some of the most common:\n\nGeoJSON .geojson\nGeoPacakge .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nHere is an example of a single point location of a city stored in a .geojson file. .geojson is a standardized data schema .json format for spatial data.\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [-80.992, 46.490]\n            },\n            \"properties\": {\n                \"city_name\": \"Sudbury\"\n            }\n        }\n    ]\n}\n\n\n\nRaster data represents space as a continuous grid with equal cell sizes. Each cell contains a value pertaining to the type of feature it represents. These values can be quantitative (e.g.¬†elevation) or categorical (e.g.¬†type of land use). Common examples of raster data include Digital Elevation Models (DEMs), satellite imagery, and scanned images like historical maps.\ne.g.¬†the map below shows a DEM for Toronto at two different scales.\n\n\n\nDEM of Toronto",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#geographic-information-systems-gis",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#geographic-information-systems-gis",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "We use Geographic Information Systems (GIS) to analyze, manipulate, and visualize spatial data.\nWhy is GIS useful?\n\nexplore spatial patterns and relationships in data\ncreate maps for publications\ngenerate new data, either ‚Äúby hand‚Äù or via spatial relationships from other data (e.g.¬†through spatial queries)\nperform spatial analysis (i.e.¬†statistical methods applied to spatial data)\n\nGIS is often thought of as more than just a tool or piece of software. It can refer to all aspects of managing and analyzing digital spatially referenced data.\nThe power of GIS software and tools is the ability to work with data stored in different layers (e.g.¬†a layer for roads, another for buildings, and so on) in conjunction with each other. These layers can be visualized and analyzed relative to each other based on their spatial relationships.\n\n\n\nSimple AI generated schematic of layers\n\n\nGIS software usually links to data stored elsewhere on a computer, rather than in a project file. If the source location of the data (i.e.¬†which folder it‚Äôs in) changes, then this will have to be updated in the GIS project. If data are edited in GIS, it will update the data in its source location.\nThe open-source QGIS and the proprietary ESRI ArcGIS are the two most used desktop GIS software. ESRI‚Äôs suite of tools are often used by larger corporate and government organizations while QGIS is more used by small consultants and freelancers, non-profits, and academia. Many spatial data processing, visualization, and analyses steps also have equivalents in Python, R, SQL and other programming languages via specific libraries\nWe‚Äôll be working with QGIS and Python, because they free and work across multiple operating systems, and are commonly used across different areas of work and research. But pretty much everything we‚Äôll show can be accomplished across different software options, the buttons and steps will just be a bit different.\nLearning the core analytical and visualization concepts and ideas are much more important than exactly where to click or what specific functions to run.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#coordinate-reference-systems-crs-and-map-projections",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#coordinate-reference-systems-crs-and-map-projections",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "A CRS is a framework for describing the geographic position of location-based data. It tells your GIS software where your data are located on the Earth. Without a CRS, your data would just be a bunch of numbers without an ability to place them on a map and relate them to other data.\nThere are thousands of CRSs. Each CRS has specific attributes about its position and one main set of spatial units (e.g.¬†a CRS will measure location in degrees longitude/latitude, metres, miles, etc.).\nProbably the most common CRS is WGS84, which uses longitude/latitude to link data to the Earth‚Äôs surface.\nWhen doing spatial analyses and processing where we are comparing two or more datasets to each other, it is important that they are in the same CRS.\n\n\nCRS are often paired with map projections. This is sometimes called a Projected CRS.\nThe earth is a 3D globe, but most maps are represented on 2D surfaces (e.g.¬†on a screen, piece of paper). Map projections are mathematical model to flatten the earth to create a 2D view.\nEvery projection distorts shape, area, distance, or direction in some way or another. Different map projections distort the Earth in different ways. Here are some examples! For more check out this projection transition tool\n\n\n\nQGIS screenshot without data\n\n\nFor doing analysis and an urban scale, it is important that we choose a map projection that conserves area and distances in both X and Y directions (i.e.¬†space is not being distorted in one particular direction more than others)\nFor example, these are two aerial images of Toronto, the left uses a local Mercator projection which does not distort data at a local scale, while the right is a rectangular projection where degrees latitude are equal shown at the same scale as degrees longitude.\nA general recommendation for urban-scale analyses is to use a Universal Transverse Mercator (UTM) projected CRS, which conserves area, distance, and direction along bands of the earth. This is a good tool for finding which UTM zone your region is in.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#working-with-spatial-data-in-qgis",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#working-with-spatial-data-in-qgis",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "Let‚Äôs use QGIS to quickly view a few spatial datasets. When you open up QGIS, it should look something like this:\n\n\n\nQGIS screenshot without data\n\n\nThe Browser panel is used for finding and loading data on your computer or from external sources, each line and symbol is for a different data source. The Layer panel will list all the datasets that are loaded into your project. The big white space will populate with data once it is loaded. The buttons at the top include a variety of options for loading data, saving your project, exploring your map, and various common processing and analytical tools.\n\n\nLet‚Äôs begin by adding in a basemap. Basemaps are often used to provide geographic reference for other data that we load into QGIS.\nWe‚Äôll add a basemap of OpenStreetMap, a free crowd-sourced map of the world, available as XYZ raster tiles. To do this, go to Layer - Data Source Manager or simply click on the Data Source Manager button highlighted below. When you open the Data Source Manager you have options for a wide variety of data types to add to your map.\n\n\n\nData source manager location in QGIS\n\n\n\n\n\nAdding an XYZ tile\n\n\nHere is the URL to paste into QGIS for adding an OpenStreetMap basemap: https://tile.openstreetmap.org/{z}/{x}/{y}.png.\nWhen you click ‚ÄòAdd‚Äô it should be added to the map. You can use the navigation buttons or just scroll on your mouse to zoom to your city.\n\n\n\nQGIS with OpenStreetMap\n\n\nLet‚Äôs now add some vector data to the map. We‚Äôve pre-downloaded two datasets from the City of Toronto‚Äôs Open Data Portal - City Wards (polygons) - Library Locations (points)\nThese can be added via the Data Source Manager - Vector, or simply via drag-and-drop from the folder of where they are located on your computer.\n\n\n\nQGIS with Toronto data\n\n\n\n\n\nThe data added to the project will be listed in the Layers panel. The order of the layers determine the order of which they stack on top of each other on the map. In the example above, the libraries are listed at the top of the other two layers, and it thus appears on top of the other two layers on the map.\nTo change the layer order, you can drag individual layers to be above or below any other layer in the Layers panel.\nMost vector data has associated attribute data, e.g.¬†the number of a ward, the name of a library, etc. To view this data, right-click on a layer and then click on Open Attribute Table. In GIS, column names are often called ‚ÄúFields‚Äù\n\n\n\nWhen you load vector data like this into QGIS, the layers have default styling (e.g.¬†colours, sizes, line-widths, etc.).\nThere are tonnes of options in QGIS to change these initial styles. To do so, right-click on a layer, go to Properties, and then Symbology.\nThere are lots of options in here to change the colours, size, and symbols.\nFor example, in the screenshot below, the sizes of the libraries and the line-widths of the wards are increased a bit, the wards are given a transparent fill, and the library symbols are converted to squares.\n\n\n\nQGIS with Toronto data\n\n\nTip! you get extra options if you click on the sub-component of the symbol. e.g.¬†in this example I clicked on Simple Fill to find additional styling options like pattern-based fill styles.\n\n\n\nQGIS symbology example\n\n\nWe‚Äôll explore data-driven styling, where colours or other style options are based on data values, in a future tutorial.\n\n\n\nYou can change the project CRS in QGIS by going to Project - Properties - CRS.\n\n\n\nYou can export and create copies of any of your data layers. This may be useful if you want to change the file format, the CRS, subset the attribute table, or simply just make a copy of the data.\nTo do this, go to Export - Save Features As, and you can adjust the Format, CRS, and other options as needed.\n\n\n\nYou can save a project in QGIS by going to Project and then Save or Save As.\nIt will get saved to a .qgis file. These files save styling, filters, layer orders, map scale and location, and layout settings.\nHowever, .qgis files do not save datasets directly in the file, only paths and links to where the data is stored, either on your computer or from a URL like the OpenStreetMap basemap. Saving only links to data is common in other tools and software.\nTherefore, if you share your project, you‚Äôll need to also share any local datasets it is linking to.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/index.html",
    "href": "notebooks/urban-data-analytics/index.html",
    "title": "Urban data analysis",
    "section": "",
    "text": "Urban data analysis",
    "crumbs": [
      "Urban Data Analytics"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html",
    "href": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html",
    "title": "Introduction to Geopandas",
    "section": "",
    "text": "GeoPandas is a library for working with spatial (typically geographic) data. It extends the functionality of pandas to support spatial data types and operations, making it easier to analyze, visualize, and manipulate spatial data. Many of the tasks that are typically done within a GIS can be done via geopandas.\n\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#loading-exploring-and-geometric-data-types",
    "href": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#loading-exploring-and-geometric-data-types",
    "title": "Introduction to Geopandas",
    "section": "Loading, Exploring, and Geometric Data Types",
    "text": "Loading, Exploring, and Geometric Data Types\nGeospatial data represents real-world features using three primary geometric types: - Points: Single (x,y) coordinates for discrete locations like transit stops or landmarks. - Lines: Connected sequences of points forming paths, such as roads or rivers. - Polygons: Closed shapes defining areas like census tracts or property boundaries.\n\ntransit_stops = gpd.read_file(\"data/ttc_stops.geojson\")\ntransit_routes = gpd.read_file(\"data/ttc_routes.geojson\")\ncensus_tracts = gpd.read_file(\"data/toronto_census_tract_2021.shp\")\ncensus_data = pd.read_csv(\"data/census_tract_data_sample.csv\")\n\nIn geopandas, we typically load data with a more agnostic read_file() function. For this workshop, we‚Äôre going to use four sources of data: - transit_stops: each of the stops for the TTC - transit_routes: each of the lines for the TTC - census_tracts: The geometry of Canadian census tracts within Toronto - census_data: the data of Canadian census tracts within Toronto\nLet‚Äôs take a look at the first layer, the transit stops. We have two columns with text, and a third with geometry data.\n\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nNAME\ngeometry\n\n\n\n\n0\nAvenue\nEglinton Crosstown LRT\nPOINT (-79.40851 43.70460)\n\n\n1\nForest Hill\nEglinton Crosstown LRT\nPOINT (-79.42556 43.70102)\n\n\n2\nLeaside\nEglinton Crosstown LRT\nPOINT (-79.37715 43.71105)\n\n\n3\nSloane\nEglinton Crosstown LRT\nPOINT (-79.31352 43.72597)\n\n\n4\nBirchmount\nEglinton Crosstown LRT\nPOINT (-79.27791 43.73006)\n\n\n...\n...\n...\n...\n\n\n101\nVaughan Metropolitan Centre\nToronto-York Spadina Subway Extension\nPOINT (-79.52727 43.79351)\n\n\n102\nSheppard-Yonge\nSheppard Subway\nPOINT (-79.41092 43.76151)\n\n\n103\nSpadina\nBloor-Danforth Subway\nPOINT (-79.40397 43.66728)\n\n\n104\nSt. George\nBloor-Danforth Subway\nPOINT (-79.39930 43.66827)\n\n\n105\nBloor-Yonge\nBloor-Danforth Subway\nPOINT (-79.38572 43.67100)\n\n\n\n\n106 rows √ó 3 columns\n\n\n\nThe data can be manipulated like a regular pandas data frame. For example, if we want to filter out all stations on the ‚ÄúEglinton Crosstown LRT‚Äù line, since it hasn‚Äôt been completed yet (at the time of writing), we can do so as follows:\n\ntransit_stops = transit_stops.loc[transit_stops[\"NAME\"] != \"Eglinton Crosstown LRT\"]\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nNAME\ngeometry\n\n\n\n\n25\nKipling\nBloor-Danforth Subway\nPOINT (-79.53628 43.63694)\n\n\n26\nIslington\nBloor-Danforth Subway\nPOINT (-79.52459 43.64532)\n\n\n27\nRoyal York\nBloor-Danforth Subway\nPOINT (-79.51129 43.64811)\n\n\n28\nOld Mill\nBloor-Danforth Subway\nPOINT (-79.49509 43.65007)\n\n\n29\nJane\nBloor-Danforth Subway\nPOINT (-79.48446 43.64979)\n\n\n...\n...\n...\n...\n\n\n101\nVaughan Metropolitan Centre\nToronto-York Spadina Subway Extension\nPOINT (-79.52727 43.79351)\n\n\n102\nSheppard-Yonge\nSheppard Subway\nPOINT (-79.41092 43.76151)\n\n\n103\nSpadina\nBloor-Danforth Subway\nPOINT (-79.40397 43.66728)\n\n\n104\nSt. George\nBloor-Danforth Subway\nPOINT (-79.39930 43.66827)\n\n\n105\nBloor-Yonge\nBloor-Danforth Subway\nPOINT (-79.38572 43.67100)\n\n\n\n\n81 rows √ó 3 columns\n\n\n\nWe can do the same for the transit lines data. Here the data are coded as a MULTILINESTRING, essentially a combination of lines that combine into one object. There are also MULTIPOLYGON geometry types\n\ntransit_routes = transit_routes.loc[transit_routes[\"NAME\"] != \"Eglinton Crosstown LRT\"]\ntransit_routes\n\n\n\n\n\n\n\n\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nExisting\nSubway\nSheppard Subway\nMULTILINESTRING ((-79.41092 43.76151, -79.4096...\n\n\n1\nExisting\nSubway\nYonge-University-Spadina Subway\nMULTILINESTRING ((-79.46247 43.75043, -79.4621...\n\n\n2\nExisting\nSubway\nSpadina Subway Extension\nMULTILINESTRING ((-79.52727 43.79351, -79.5261...\n\n\n4\nExisting\nSubway\nScarborough RT\nMULTILINESTRING ((-79.26453 43.73226, -79.2632...\n\n\n5\nExisting\nSubway\nBloor Subway\nMULTILINESTRING ((-79.26453 43.73226, -79.2669...\n\n\n\n\n\n\n\nBefore we go on, it‚Äôs important to have an idea of the metadata of geometric files that we work with. There‚Äôs two key parts to this. - CRS (Coordinate Reference System): The crs attribute defines a geodataset‚Äôs spatial ‚Äúcoordinate system‚Äù (e.g., latitude/longitude, meters-based projections). We can use it to ensure layers align‚Äîfor example, combining Toronto census tracts (EPSG:3347) with a Web Mercator basemap (EPSG:3857). - Total Bounds: The total_bounds attribute returns the min/max coordinates (xmin, ymin, xmax, ymax) of your data‚Äôs extent. It‚Äôs useful for setting map zoom levels or clipping other datasets to the same area‚Äîlike focusing a transit map on Toronto‚Äôs downtown core.\n\ntransit_stops.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ntransit_stops.total_bounds\n\narray([-79.53627668,  43.63693527, -79.25160004,  43.79350523])"
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#basic-static-plots",
    "href": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#basic-static-plots",
    "title": "Introduction to Geopandas",
    "section": "Basic Static Plots",
    "text": "Basic Static Plots\nWe explore geometry simply by plotting using .plot(). We can do this for any row, or the entire GeoDataFrame\n\ntransit_stops.plot()\n\n\n\n\n\n\n\n\nThis is the default plot, but we can tweak the colours, add multiple layers, and change some of the layout options using matplotlib, probably the most commonly used map. Here‚Äôs a very simple schematic of rapid transit in Toronto (circa 2021)\n\nfig, ax = plt.subplots(ncols = 1, figsize=(3, 3))\n\ntransit_stops.plot(\n    color=\"Black\",\n    markersize = 6,\n    ax = ax\n)\n\ntransit_routes.plot(\n    linewidth = 1,\n    color=\"Black\",\n    ax = ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nLet‚Äôs take a look at the census tract data. Here we have polygon geometries.\n\ncensus_tracts\n\n\n\n\n\n\n\n\nid\nctuid\ndguid\nctname\nlandarea\npruid\ngeometry\n\n\n\n\n0\n487\n5350128.04\n2021S05075350128.04\n0128.04\n0.1620\n35\nPOLYGON ((629437.750 4839364.950, 629247.561 4...\n\n\n1\n502\n5350363.06\n2021S05075350363.06\n0363.06\n0.8210\n35\nPOLYGON ((640741.738 4848050.419, 640723.345 4...\n\n\n2\n506\n5350363.07\n2021S05075350363.07\n0363.07\n2.2422\n35\nPOLYGON ((642782.718 4849973.938, 642781.180 4...\n\n\n3\n508\n5350378.23\n2021S05075350378.23\n0378.23\n1.5314\n35\nPOLYGON ((639248.900 4849901.332, 639248.900 4...\n\n\n4\n509\n5350378.24\n2021S05075350378.24\n0378.24\n2.5129\n35\nPOLYGON ((639952.255 4850407.204, 639952.255 4...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n580\n5861\n5350210.04\n2021S05075350210.04\n0210.04\n0.4751\n35\nMULTIPOLYGON (((623047.314 4831182.748, 623047...\n\n\n581\n5862\n5350062.03\n2021S05075350062.03\n0062.03\n0.4638\n35\nPOLYGON ((629776.795 4835352.843, 629766.377 4...\n\n\n582\n5863\n5350062.04\n2021S05075350062.04\n0062.04\n0.1215\n35\nPOLYGON ((630319.668 4835517.832, 630149.660 4...\n\n\n583\n5864\n5350017.01\n2021S05075350017.01\n0017.01\n0.8026\n35\nPOLYGON ((633075.947 4834744.346, 633089.159 4...\n\n\n584\n5865\n5350017.02\n2021S05075350017.02\n0017.02\n0.5681\n35\nPOLYGON ((631852.697 4833570.180, 631843.913 4...\n\n\n\n\n585 rows √ó 7 columns\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\ncensus_tracts.plot(\n    edgecolor=\"Black\",\n    color=\"White\",\n    linewidth=0.5,\n    ax = ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nLet‚Äôs try to make a choropleth map. We‚Äôll have to join in the tabular data in census_data\n\ncensus_tracts_data = census_tracts.merge(census_data, how='left', on='ctuid')\n\n\ncensus_tracts_data[\"population density\"] = census_tracts_data[\"population_2021\"] / census_tracts_data[\"landarea\"]\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\ncensus_tracts_data.to_crs(4326).plot(\n    column = \"population density\",\n    edgecolor=\"White\",\n    cmap = 'YlOrRd', \n    k = 7,\n    scheme = \"Quantiles\", \n    linewidth=0.5,\n    legend = True,\n    zorder=1,\n    legend_kwds = {\n        \"loc\": \"lower right\",\n        \"fontsize\": 7,\n        \"title\": \"Population Density\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    },\n    ax=ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nWe can choose a different variable and make a map of median income. Notice that we‚Äôve also switched the color parameter cmap; just as in matplotlib generally, we can style the map as we please.\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\ncensus_tracts_data.plot(\n    column = \"median_aftertax_hhld_income_2020\",\n    edgecolor=\"Black\",\n    cmap = 'coolwarm_r', \n    k = 4,\n    scheme = \"Quantiles\", \n    linewidth=0.5,\n    legend = True,\n    legend_kwds = {\n        \"loc\": \"lower right\",\n        \"fontsize\": 7,\n        \"title\": \"Median Income\\n(after tax)\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    },\n    ax=ax\n).set_axis_off()"
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#interactive-exploration",
    "href": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#interactive-exploration",
    "title": "Introduction to Geopandas",
    "section": "Interactive Exploration",
    "text": "Interactive Exploration\nGeoPandas‚Äô explore() function generates an interactive Leaflet map (like Google Maps) from your geodata. We can use it to better understand the data we are working with and how it might be viewed from the user side on a web application (e.g., Svelte).\nYou‚Äôll need to install a couple libraries in order for this to work - matplotlib, folium, and mapclassify. This can be done in the environment that you‚Äôre working in with a command like pip install folium matplotlib mapclassify.\n\ntransit_stops.explore(\n    column='LOCATION_N',  # Popup labels\n    tiles=\"CartoDB Positron\", \n    marker_kwds={\"radius\": 5}\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#layering-multiple-datasets",
    "href": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#layering-multiple-datasets",
    "title": "Introduction to Geopandas",
    "section": "Layering Multiple Datasets",
    "text": "Layering Multiple Datasets\nLayering lets you combine different geographic datasets (like roads on top of neighborhoods) to reveal spatial relationships. In GeoPandas, each plot() call adds a new visual layer, with zorder controlling which appears on top. These plots are highly customizable, as we see below.\n\n# Initialize the canvas (all layers will share this axis)\nfig, ax = plt.subplots(figsize=(7, 7))  \n\n# Layer 1: Transit routes (black lines on top, zorder=3)\ntransit_routes.to_crs(4326).plot(\n    linewidth=1,        \n    color=\"Black\",      \n    ax=ax,              # Draw on our shared axis\n    zorder=3            # Top layer (overlaps others)\n)\n\n# Layer 2: Transit stops (black dots below routes, zorder=2)\ntransit_stops.to_crs(4326).plot(\n    color=\"Black\",      \n    markersize=16,      \n    ax=ax,              # Same shared axis\n    zorder=2            # Middle layer (above tracts)\n)\n\n# Layer 3: Census tracts (colored by density, zorder=1)\ncensus_tracts_data.to_crs(4326).plot(\n    column=\"population density\",  \n    edgecolor=\"White\",            \n    cmap='YlOrRd',                \n    k=7,                          \n    scheme=\"Quantiles\",           \n    linewidth=0.5,                \n    legend=True,                  \n    zorder=1,                     # Bottom layer\n    legend_kwds={\n        \"loc\": \"lower right\",     \n        \"fontsize\": 7,            \n        \"title\": \"Population Density\",  \n        \"alignment\": \"left\",      \n        \"reverse\": True           \n    },\n    ax=ax\n).set_axis_off()  # Hide distracting x/y axes"
  },
  {
    "objectID": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html",
    "href": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "This page covers 1) common metrics and indicators used in urban analysis, 2) spatial units and measures of aggregation often used for urban analysis, and 3) limitations and biases for working with urban datasets to keep in mind when working with urban data.\n\n\nThe table below lists examples of variables/metrics that are used in urban analyses, grouped by topic.\n\n\n\nTopic\nCommon metrics\n\n\n\n\nPeople / socioeconomics\n- Population density (e.g., population per square kilometer)- Median income- Race/ethnicity- Displacement risk- Social vulnerability index- Population change- Inequality metrics (e.g., Gini index - US & Canada)- Crime rate- Education level (e.g., % with bachelor‚Äôs degree, master‚Äôs degree)- Dissimilarity index of racial segregation- Voting patterns (e.g., in Toronto)\n\n\nHousing\n- % owners/renters- Median rent & home value- Vacancy rate- Cost burden- Core housing need- Number of units built or permits issued (e.g., Canadian ADU analysis; market-rate housing in the Bay Area)\n\n\nLand use\n- Walk score- Entropy index for land use mix- Change in land cover (or forest)- Zoning (e.g., zoning policy changes; residential zoning in Canadian cities)\n\n\nEconomics / employment\nJob density (e.g., Longitudinal Employer-Household Dynamics in the US; Canadian Employer-Employee Dynamics Database in Canada)- Commuting patterns- Venture capital investment by city- Sales by sector downtown- Downtown recovery post-pandemic (e.g., trends; Urban Activity Atlas)\n\n\nTransportation\n- Commute mode (e.g., % of people by census tract who commute via public transit vs car)- Public transit accessibility (e.g., ‚Äúwalkshed‚Äù around transit stations; % of population within given distance of rail station; population near individual transit stations)- E-bike trip distance- Bike share trips in Toronto- Traffic violations\n\n\nEnvironment\n- Emissions/air pollution)- Urban heat (e.g., heat exposure in Toronto)- Greenness)- Park access (e.g., distance to nearest park)- Flood maps (e.g., Toronto; FEMA; Canada)- Tree canopy coverage\n\n\nHealth\n- Life expectancy- Prevalence of chronic diseases (e.g., diabetes, asthma)\n\n\n\n\n\n\nUrban data is often linked to specific places. This is often called spatial or geographic data.\nWhen analyzing spatial data, our analysis is often at the level of specific spatial units or unit of aggregations. Sometimes our data is directly collected at these units, while sometimes it is useful to aggregate large datasets to these units to help analyses and visualizations. Below are spatial units and types of encoding that are often used for collecting and analyzing urban data.\nIn our notebook on spatial data and GIS, we go into details on how different spatial data is structured, and how we can begin to view, explore, and analyze different data in GIS.\n\n\nPolitical boundaries that delineate jurisdictions for different levels of government, from national down to local levels. Countries, provinces or states, counties, municipalities, electoral districts or city wards, and within cities, neighbourhood planning areas, are all examples of commonly used spatial units.\n\n\n\nFederal electoral district in Saskatoon\n\n\n\n\n\nNational censuses aggregate data to a variety of spatial units ranging in size, many are the same as administrative and political boundaries, as well as many smaller-geography boundaries that are super useful for urban- and neighbourhood-scale maps and analyses. Census tracts (usually in the range of 2,500 and 8,000 persons) and Dissemination Areas (400 to 700 persons) are two scales that are often used. Check out (Statistics Canada documentation) or see our notebook on Canadian census data for more information.\n\n\n\nCommon census boundaries in Toronto\n\n\n\n\n\nA grid is repetitive tesselation spread across the surface of a map. Grids are used in spatial analysis when existing boundaries are unavailable, unsuitable, or when evenly sized, uniform areas are required. Geohashes, which uniquely identify specific regions according to their latitude and longitude everywhere on Earth, are one type of commonly used grid. Grids do not have to 4-sided.\n\n\n\nGeohashes in Qu√©bec City (source)\n\n\nTriangular and hexagonal grids are often used for some studies. Hexagon‚Äôs are often recommended since they are the regular polygon with the most sides (i.e.¬†can closest represent a circle), that can tesselate without any gaps.\n\n\n\nScreenshot of a map showing density of activity in Glasgow\n\n\n\n\n\nWhile streets are often added to maps to provide geographic context, streets can also be their own unit of analysis. For example, traffic flow could be measured on street segments throughout a city. In the image below, streets are coloured by how many parking tickets that they have.\n\n\n\nParking tickets in Toronto (source)\n\n\n\n\n\nSome urban data is measured or collected at the address level. For example, address of businesses, non-profits, or community facilities. To map them and compare with other spatial data, addresses are often geocoded, where their names are converted into geographic coordinates (latitude and longitude). See our Spatial data and GIS tutorial for more.\n\n\n\nGeocoded addresses of businesses in Mississauga from the Canadian Urban Institutes Measuring Main Streets project\n\n\n\n\n\n\nWhen collecting and analyzing data, it is important to verify the quality of the dataset. Some data is incomplete or has missing values, which can bias the results, especially if data from certain categories is missing disproportionately. For example, if income data is missing more often for lower-income individuals, the results may overestimate average income and under-represent vulnerable populations.\nThese are a few important sources of bias or limitations when working with spatial data that are super important to be aware of when working with data linked to places.\n\nSelf-reporting bias which is when individuals report inaccurate information about themselves in a survey. This can be intentional (e.g., under-reporting income or over-reporting education) or unintentional (e.g., forgetting details). This can lead to biases in the final dataset and any subsequent analysis.\nEcological fallacy is the phenomenon of drawing conclusions about individuals based on the group they belong to. For example, one might infer that everyone in a census tract with an overall high median income is wealthy. Although the median income is high, there may be low income residents who live in the tract who are not close to the median.\n[Edge effects] in spatial analysis refer to the limitations or distortions that occur at the boundaries of a study area. They can bias results or reduce accuracy, especially when spatial patterns or processes extend beyond the area being analyzed.. For example, let‚Äôs say you were mapping access to healthy food in a city. Your map may show that one corner of your city does not have a grocery store, leading to a conclusion of it being a food dessert. But if you didn‚Äôt consider grocery stores just outside the edge or boundary of your city adjacent to this corner, this may not be the case.\nModifiable areal unit problem (MAUP) is another source of bias when working with spatial data. It is a form of statistical bias that results from the fact that changing the scale or shape of aggregation units leads to different results. Gerrymandering is a classic example of intentional MAUP to obtain specific voting outcomes. Check out the graphic below, we can see the results of how different spatial units are arranged would impact the overall results of an election. Overall, it is important to think critically when working with different spatial units to avoid misrepresenting data or cherry-picking results.\n\n\n\n\nGerrymandering example",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#common-metrics-and-indicators-for-urban-analyses",
    "href": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#common-metrics-and-indicators-for-urban-analyses",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "The table below lists examples of variables/metrics that are used in urban analyses, grouped by topic.\n\n\n\nTopic\nCommon metrics\n\n\n\n\nPeople / socioeconomics\n- Population density (e.g., population per square kilometer)- Median income- Race/ethnicity- Displacement risk- Social vulnerability index- Population change- Inequality metrics (e.g., Gini index - US & Canada)- Crime rate- Education level (e.g., % with bachelor‚Äôs degree, master‚Äôs degree)- Dissimilarity index of racial segregation- Voting patterns (e.g., in Toronto)\n\n\nHousing\n- % owners/renters- Median rent & home value- Vacancy rate- Cost burden- Core housing need- Number of units built or permits issued (e.g., Canadian ADU analysis; market-rate housing in the Bay Area)\n\n\nLand use\n- Walk score- Entropy index for land use mix- Change in land cover (or forest)- Zoning (e.g., zoning policy changes; residential zoning in Canadian cities)\n\n\nEconomics / employment\nJob density (e.g., Longitudinal Employer-Household Dynamics in the US; Canadian Employer-Employee Dynamics Database in Canada)- Commuting patterns- Venture capital investment by city- Sales by sector downtown- Downtown recovery post-pandemic (e.g., trends; Urban Activity Atlas)\n\n\nTransportation\n- Commute mode (e.g., % of people by census tract who commute via public transit vs car)- Public transit accessibility (e.g., ‚Äúwalkshed‚Äù around transit stations; % of population within given distance of rail station; population near individual transit stations)- E-bike trip distance- Bike share trips in Toronto- Traffic violations\n\n\nEnvironment\n- Emissions/air pollution)- Urban heat (e.g., heat exposure in Toronto)- Greenness)- Park access (e.g., distance to nearest park)- Flood maps (e.g., Toronto; FEMA; Canada)- Tree canopy coverage\n\n\nHealth\n- Life expectancy- Prevalence of chronic diseases (e.g., diabetes, asthma)",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#common-spatial-units-and-measures-of-aggregation",
    "href": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#common-spatial-units-and-measures-of-aggregation",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "Urban data is often linked to specific places. This is often called spatial or geographic data.\nWhen analyzing spatial data, our analysis is often at the level of specific spatial units or unit of aggregations. Sometimes our data is directly collected at these units, while sometimes it is useful to aggregate large datasets to these units to help analyses and visualizations. Below are spatial units and types of encoding that are often used for collecting and analyzing urban data.\nIn our notebook on spatial data and GIS, we go into details on how different spatial data is structured, and how we can begin to view, explore, and analyze different data in GIS.\n\n\nPolitical boundaries that delineate jurisdictions for different levels of government, from national down to local levels. Countries, provinces or states, counties, municipalities, electoral districts or city wards, and within cities, neighbourhood planning areas, are all examples of commonly used spatial units.\n\n\n\nFederal electoral district in Saskatoon\n\n\n\n\n\nNational censuses aggregate data to a variety of spatial units ranging in size, many are the same as administrative and political boundaries, as well as many smaller-geography boundaries that are super useful for urban- and neighbourhood-scale maps and analyses. Census tracts (usually in the range of 2,500 and 8,000 persons) and Dissemination Areas (400 to 700 persons) are two scales that are often used. Check out (Statistics Canada documentation) or see our notebook on Canadian census data for more information.\n\n\n\nCommon census boundaries in Toronto\n\n\n\n\n\nA grid is repetitive tesselation spread across the surface of a map. Grids are used in spatial analysis when existing boundaries are unavailable, unsuitable, or when evenly sized, uniform areas are required. Geohashes, which uniquely identify specific regions according to their latitude and longitude everywhere on Earth, are one type of commonly used grid. Grids do not have to 4-sided.\n\n\n\nGeohashes in Qu√©bec City (source)\n\n\nTriangular and hexagonal grids are often used for some studies. Hexagon‚Äôs are often recommended since they are the regular polygon with the most sides (i.e.¬†can closest represent a circle), that can tesselate without any gaps.\n\n\n\nScreenshot of a map showing density of activity in Glasgow\n\n\n\n\n\nWhile streets are often added to maps to provide geographic context, streets can also be their own unit of analysis. For example, traffic flow could be measured on street segments throughout a city. In the image below, streets are coloured by how many parking tickets that they have.\n\n\n\nParking tickets in Toronto (source)\n\n\n\n\n\nSome urban data is measured or collected at the address level. For example, address of businesses, non-profits, or community facilities. To map them and compare with other spatial data, addresses are often geocoded, where their names are converted into geographic coordinates (latitude and longitude). See our Spatial data and GIS tutorial for more.\n\n\n\nGeocoded addresses of businesses in Mississauga from the Canadian Urban Institutes Measuring Main Streets project",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#biases-and-limitations-of-spatial-data",
    "href": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#biases-and-limitations-of-spatial-data",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "When collecting and analyzing data, it is important to verify the quality of the dataset. Some data is incomplete or has missing values, which can bias the results, especially if data from certain categories is missing disproportionately. For example, if income data is missing more often for lower-income individuals, the results may overestimate average income and under-represent vulnerable populations.\nThese are a few important sources of bias or limitations when working with spatial data that are super important to be aware of when working with data linked to places.\n\nSelf-reporting bias which is when individuals report inaccurate information about themselves in a survey. This can be intentional (e.g., under-reporting income or over-reporting education) or unintentional (e.g., forgetting details). This can lead to biases in the final dataset and any subsequent analysis.\nEcological fallacy is the phenomenon of drawing conclusions about individuals based on the group they belong to. For example, one might infer that everyone in a census tract with an overall high median income is wealthy. Although the median income is high, there may be low income residents who live in the tract who are not close to the median.\n[Edge effects] in spatial analysis refer to the limitations or distortions that occur at the boundaries of a study area. They can bias results or reduce accuracy, especially when spatial patterns or processes extend beyond the area being analyzed.. For example, let‚Äôs say you were mapping access to healthy food in a city. Your map may show that one corner of your city does not have a grocery store, leading to a conclusion of it being a food dessert. But if you didn‚Äôt consider grocery stores just outside the edge or boundary of your city adjacent to this corner, this may not be the case.\nModifiable areal unit problem (MAUP) is another source of bias when working with spatial data. It is a form of statistical bias that results from the fact that changing the scale or shape of aggregation units leads to different results. Gerrymandering is a classic example of intentional MAUP to obtain specific voting outcomes. Check out the graphic below, we can see the results of how different spatial units are arranged would impact the overall results of an election. Overall, it is important to think critically when working with different spatial units to avoid misrepresenting data or cherry-picking results.\n\n\n\n\nGerrymandering example",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  }
]