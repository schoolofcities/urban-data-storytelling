[
  {
    "objectID": "outline-example.html",
    "href": "outline-example.html",
    "title": "<img src='https://raw.githubusercontent.com/schoolofcities/gta-immigration/refs/heads/main/src/assets/top-logo-full.svg'  style='height:auto;width:220px'></img><br>Urban Data Storytelling üìäüìàüèôÔ∏è",
    "section": "",
    "text": "Data storytelling (V1, V2)\nAssessment: - Describe your goals, stakeholders, narrative ideas, list of potential data, etc.\nMandatory readings: - intro to data storytelling (video) - intro to urban data (video and notebook) - measuring the city (notebook) - data ethics / literacy (videos) Optional readings (if they want a background in coding) - intro to python and jupyter (notebook)\nData analysis (V3)\nAssessment: - List each dataset you have downloaded/have access to, and briefly describe why it‚Äôs important for your work - Analyze and explore your data to provide at least 3 summaries that super important to your research and story. Describe what the data tells you. These can include, but are not limited to,summary statistics (means, percents, etc.), summary tables (e.g.¬†cross-tabs, pivot tables, etc.), correlation statistics or summary trend from a regression model, or other results from a statistical analysis\nMandatory readings: - data analysis and processing (notebook) - spatial data and gis (notebook) Optional readings (pick and choose depending on which relate to your project): - statistics fundementals (notebook) - intro to census data (notebook) - intro to openstreetmap data (notebook) - REV - spatial data in python (notebook) - spatial data processing (notebook)\nData visualization (V4, V5)\nAssessment: - Create at least 3 maps and/or charts of your data that help communicate patterns and help tell your story. Try to make at least 1 non-spatial chart and at least 1 map/spatial data visualization.\nMandatory readings: - data visualization (notebook) - maps and spatial data visualization (notebook) Optional readings (pick and choose depending on which relate to your project): - exploratory data viz in Python (notebook) - choropleth maps (notebook) - proportional symbol maps (notebook) - mapping density (notebook) (might add 1 or 2 more in this section depending on time)"
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html",
    "href": "notebooks/urban-data-analytics/data-ethics-and-literacy/data-ethics-and-literacy.html",
    "title": "Data ethics and literacy",
    "section": "",
    "text": "Data ethics and literacy\nInsert videos and any text?",
    "crumbs": [
      "Urban Data Analytics",
      "Data ethics and literacy"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "National censuses, like the Canadian census, are very common data sources analyzing demographic and socio-economic data pertaining to specific places.\nStatistics Canada conducts a national census of the population every five years, asking a range of demographic and socio-economic questions. The results paint a demographic portrait of the country at the time period the census was conducted.\nThe most recent census at the time of writing was in 2021. Lots of census data are publicly available for download, across the following topics:\n\nAge\nCommuting to work\nEducation\nEthnocultural and religious diversity\nFamilies, households, and marital status\nHousing\nImmigration, place of birth, and citizenship\nIncome\nIndigenous peoples\nLabour\nLanguage and language of work\nMobility and migration\nPopulation and dwelling counts\nTypes of dwellings\n\nMost data are pre-aggregated to a variety of geographic boundaries (e.g.¬†provinces, cities, neighbourhoods, blocks, etc.), which allow for finding a variety of demographic and socio-economic statistics for specific places as well as for making a range of maps.\nFor example, here‚Äôs a map of population density in the Greater Toronto Area (GTA), clearly showing where people are clustered throughout the region.\n\n\n\nMap of population density in Toronto\n\n\nThis notebook covers:\n\nan overview of Canadian Census data\nwhere to find census data on the Statistics Canada website\nhow to explore maps of census data using CensusMapper\nhow to download census data to use in your own projects\n\n\n\nThere are two parts to the census, the short-form survey and the long-form survey. The short-form survey asks a set of basic household and demographic questions (e.g.¬†address, age, marital status, etc.) and is sent to all households in Canada. The long-from survey is sent to 25% of households in Canada. It asks additional questions pertaining to a broader range of demographic, social, and economic topics (e.g.¬†religion, education, journey to work, etc.). Statistics Canada also augments collected census survey data by joining in data from other administrative sources, including income data collected by the Canadian Revenue Agency (CRA).\nCensus data are collected primarily on a household-by-household basis (one adult member in each household usually fills out the census on behalf of everyone in the household). Data of individual responses from the census are often called census ‚Äúmicro-data‚Äù. Because of personal identification concerns, this data is only accessible by accredited researchers. (A public use microdata file called the PUMF is available though. It is a random sample of the overall population, with several of the identifying variables removed, such as home addresses and postal code).\n\n\n\nSummaries (i.e.¬†aggregations) of census data to a range of geographic areas are publicly available to view online or download. These are super useful for understanding the demographics of a place. For example, the total population in a province, the number of people who speak Spanish in Toronto, or the average income in a specific neighbourhood.\nThe Census Profile tables on Statistics Canada‚Äôs website allow for searching for census data for specific variables and geographic areas. For example, here‚Äôs an output of ‚ÄúKnowledge of Official Languages‚Äù in Ontario.\n\n\n\nTable of knowledge of language in Ontario\n\n\nWhen working with census data, it‚Äôs often advisable to use the Census Dictionary, the main reference guidebook, to understand what different data variables and geographies in the census represent. For example, here‚Äôs the entry for Knowledge of official languages.\nCensus profile data is typically limited to single categories totals (e.g.¬†number of people who speak French) by gender, as shown in the table above. However, if you are interested cross-tabulations, summaries across multiple categories (e.g.¬†number of people who have knowledge of French who also speak French at work, e.g.¬†total number low-income residents who live in different types of housing), then there are a variety of Data Tables available for this purpose.\nIf neither the Census Profile or Data Tables fit your purpose, there is also a Public Use Microdata File (PUMF). This is a non-aggregated (i.e.¬†each row is a disaggregated individual-level response) dataset covering a sample of the Canadian population. This data can be queried and cross-tabulated across any number of categories included. For privacy reasons, the data only include larger geographic linkages (e.g.¬†provinces, large metro areas), and are only a sample of the overall census.\n\n\n\nThe are a number of geographic boundaries available with associated census data, ranging in scale from city blocks to the entire country. Below is an example of commonly used boundaries for urban-scale maps and analysis.\n\n\n\nCommon census boundaries in Toronto\n\n\nEach polygon on this map has associated publicly available summary census data. Joining this tabular data to these spatial boundaries allows for making a wide range of maps showing the distribution of demographics and socio-economic variables\nYou can bulk download census data for a number of geographic levels and formats from the Statistics Canada website. These downloads are essentially copies of the Census Profile data, but for all regions noted in each row.\nOne issue to be aware of is that census boundaries can change over time each time a census is conducted. Doing a longitudinal analysis of spatial census data often requires using a technique like areal interpolation, in which data are joined to a common set of spatial units prior to analyses.\n\n\n\nExample of census tract boundaries changing in Victoria\n\n\n\n\n\nCensusMapper is a website for exploring and downloading census data across Canada. When we first land on the website, it defaults to a map of Population Density in Vancouver and it shares a number of preset options for making maps.\nIf we want to search for a specific census variable, we can click Make a Map in the top right, and then select the year (e.g.¬†2021). Here we can search and explore all available data. Using the search icon at the top-left or by clicking inset Canada map can help us to navigate elsewhere in the country. Just through a few clicks, I was able to create this map of knowledge of Portuguese in Toronto. Try making your own map!\n\n\n\nPortuguese in Toronto on CensusMapper\n\n\nWe can also use CensusMapper to download census data for specified geographic boundaries. To do so, click on API on the top right. First select the census year. Variable Selection is used for searching for and selecting the variables (i.e.¬†attribute data such as knowledge of a particular language) to download. Region Selection is the geographic area that we want download data for (e.g.¬†for Toronto). In the Overview panel, we can view what we‚Äôve selected as well as pick the geographic aggregation level (e.g.¬†Census Tracts, Dissemination Areas, etc.). Once selected, we can then download the attribute table and/or geographic boundaries.\nCensusMapper is partly built on an R library for downloading census data called cancensus. If you work with R, it is definitely worth checking out!\n\n\n\nWhile CensusMapper (and other online tools like it) are great for exploring and downloading data, we often want to make more customized maps (e.g.¬†for a report, a paper, a website, etc.) or analyze census data in conjunction with other data sources (e.g.¬†comparing demographic data to the location of libraries, public transit, or grocery stores, etc.).\nTo do so, the general process is to first download census data directly from the Statistics Canada website above or from CensusMapper and then load it into whatever software or library that you are working with.\nLINK TO CHROPLETH TUTORIAL AND OTHERS?",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#overview-of-the-canadian-census",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#overview-of-the-canadian-census",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "There are two parts to the census, the short-form survey and the long-form survey. The short-form survey asks a set of basic household and demographic questions (e.g.¬†address, age, marital status, etc.) and is sent to all households in Canada. The long-from survey is sent to 25% of households in Canada. It asks additional questions pertaining to a broader range of demographic, social, and economic topics (e.g.¬†religion, education, journey to work, etc.). Statistics Canada also augments collected census survey data by joining in data from other administrative sources, including income data collected by the Canadian Revenue Agency (CRA).\nCensus data are collected primarily on a household-by-household basis (one adult member in each household usually fills out the census on behalf of everyone in the household). Data of individual responses from the census are often called census ‚Äúmicro-data‚Äù. Because of personal identification concerns, this data is only accessible by accredited researchers. (A public use microdata file called the PUMF is available though. It is a random sample of the overall population, with several of the identifying variables removed, such as home addresses and postal code).",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#finding-census-data",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#finding-census-data",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "Summaries (i.e.¬†aggregations) of census data to a range of geographic areas are publicly available to view online or download. These are super useful for understanding the demographics of a place. For example, the total population in a province, the number of people who speak Spanish in Toronto, or the average income in a specific neighbourhood.\nThe Census Profile tables on Statistics Canada‚Äôs website allow for searching for census data for specific variables and geographic areas. For example, here‚Äôs an output of ‚ÄúKnowledge of Official Languages‚Äù in Ontario.\n\n\n\nTable of knowledge of language in Ontario\n\n\nWhen working with census data, it‚Äôs often advisable to use the Census Dictionary, the main reference guidebook, to understand what different data variables and geographies in the census represent. For example, here‚Äôs the entry for Knowledge of official languages.\nCensus profile data is typically limited to single categories totals (e.g.¬†number of people who speak French) by gender, as shown in the table above. However, if you are interested cross-tabulations, summaries across multiple categories (e.g.¬†number of people who have knowledge of French who also speak French at work, e.g.¬†total number low-income residents who live in different types of housing), then there are a variety of Data Tables available for this purpose.\nIf neither the Census Profile or Data Tables fit your purpose, there is also a Public Use Microdata File (PUMF). This is a non-aggregated (i.e.¬†each row is a disaggregated individual-level response) dataset covering a sample of the Canadian population. This data can be queried and cross-tabulated across any number of categories included. For privacy reasons, the data only include larger geographic linkages (e.g.¬†provinces, large metro areas), and are only a sample of the overall census.",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#census-geography",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#census-geography",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "The are a number of geographic boundaries available with associated census data, ranging in scale from city blocks to the entire country. Below is an example of commonly used boundaries for urban-scale maps and analysis.\n\n\n\nCommon census boundaries in Toronto\n\n\nEach polygon on this map has associated publicly available summary census data. Joining this tabular data to these spatial boundaries allows for making a wide range of maps showing the distribution of demographics and socio-economic variables\nYou can bulk download census data for a number of geographic levels and formats from the Statistics Canada website. These downloads are essentially copies of the Census Profile data, but for all regions noted in each row.\nOne issue to be aware of is that census boundaries can change over time each time a census is conducted. Doing a longitudinal analysis of spatial census data often requires using a technique like areal interpolation, in which data are joined to a common set of spatial units prior to analyses.\n\n\n\nExample of census tract boundaries changing in Victoria",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#making-maps-with-censusmapper",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#making-maps-with-censusmapper",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "CensusMapper is a website for exploring and downloading census data across Canada. When we first land on the website, it defaults to a map of Population Density in Vancouver and it shares a number of preset options for making maps.\nIf we want to search for a specific census variable, we can click Make a Map in the top right, and then select the year (e.g.¬†2021). Here we can search and explore all available data. Using the search icon at the top-left or by clicking inset Canada map can help us to navigate elsewhere in the country. Just through a few clicks, I was able to create this map of knowledge of Portuguese in Toronto. Try making your own map!\n\n\n\nPortuguese in Toronto on CensusMapper\n\n\nWe can also use CensusMapper to download census data for specified geographic boundaries. To do so, click on API on the top right. First select the census year. Variable Selection is used for searching for and selecting the variables (i.e.¬†attribute data such as knowledge of a particular language) to download. Region Selection is the geographic area that we want download data for (e.g.¬†for Toronto). In the Overview panel, we can view what we‚Äôve selected as well as pick the geographic aggregation level (e.g.¬†Census Tracts, Dissemination Areas, etc.). Once selected, we can then download the attribute table and/or geographic boundaries.\nCensusMapper is partly built on an R library for downloading census data called cancensus. If you work with R, it is definitely worth checking out!",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#further-analysis-and-visualization-of-census-data",
    "href": "notebooks/urban-data-analytics/canadian-census-data/canadian-census-data.html#further-analysis-and-visualization-of-census-data",
    "title": "Overview of Canadian census data",
    "section": "",
    "text": "While CensusMapper (and other online tools like it) are great for exploring and downloading data, we often want to make more customized maps (e.g.¬†for a report, a paper, a website, etc.) or analyze census data in conjunction with other data sources (e.g.¬†comparing demographic data to the location of libraries, public transit, or grocery stores, etc.).\nTo do so, the general process is to first download census data directly from the Statistics Canada website above or from CensusMapper and then load it into whatever software or library that you are working with.\nLINK TO CHROPLETH TUTORIAL AND OTHERS?",
    "crumbs": [
      "Urban Data Analytics",
      "Overview of Canadian census data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html",
    "title": "Introduction to urban data",
    "section": "",
    "text": "[Insert video here]\n\n\nIn data analysis, understanding the type and origin of data is essential to choosing the right methods for analysis, limitations in the data, and interpreting results. The following categories outline common types of data sources that we often encounter. These sources vary in how and why they are collected.\n\n\n\n\n\n\n\n\nData source type\nDescription\nExamples\n\n\n\n\nDesigned/Survey\nData collected through surveys, experiments, or designed methods.\nCensus data (historical or current), research surveys, opinion polls\n\n\nAdministrative\nData from routine operations or official records. Often open when government-run.\nCity records (public housing locations, public transit data), tax records, healthcare data (historical and current), school enrollment data\n\n\nCrowdsourced\nData contributed by the public or community-driven platforms.\nOpenStreetMap (OSM), Wikipedia, social media data, 311 data, Google reviews\n\n\nEvent-Driven/Real-Time\nData generated from sensors, transactions, or interactions. Often continuous and time-sensitive.\nSatellite data, mobile phone GPS, IoT sensors, e-commerce transactions, website clicks\n\n\nDerived data\nData created by transforming or calculating metrics from existing data or simulations.\nIndices like low-income prevalence or social deprivation, environmental quality scores, simulated datasets for testing or model training\n\n\n\nSometimes data can be a combination, for example, both the United States Census Bureau and Statistics Canada collect a combination of survey data and administrative data.\n\n\n\n\nMap of average household income in Toronto using data from Statistics Canada (2020)\n\n\n\n\n\nData availability refers to how accessible data is and the conditions under which it can be used. Key categories include:\nPublic / Open data - Data that is freely accessible to anyone and can be reused without restrictions. Open data is often provided by governments, research institutions, and public organizations. It is typically non-sensitive and available in machine-readable formats. Examples include aggregated census data, municipal open data, and OpenStreetMap.\nRestricted data Data that is available but comes with limitations due to privacy, security, or legal concerns. This includes sensitive datasets such as health data, dis-aggregate census data, and government records with personal identifiers that are redacted or protected.\nProprietary data - Data that is owned by a specific entity (e.g., a corporation or private organization) and is not freely available. Access is typically granted through licenses, paid subscriptions, or agreements. For example, cell phone mobility data from Spectus can be used to measure post-pandemic downtown recovery trends, real estate data from Costar can be used to assess vacancy rates or rent prices, and consumer data from Data Axle can be used to study the impact of new housing on migration patterns.\n\n\n\n\nCrowdsourced OpenStreetMap data in Vancouver\n\n\n\n\n\nThe table below lists a handful of websites where you can find publicly available data about cities, the environment, land use, transportation, Indigenous communities, housing and homelessness. This is a non-exhaustive list; there are many other great data sources available. Note that municipalities‚Äô open data portals typically contain information on all or most of these topics.\n\n\n\nTopic\nData sources\n\n\n\n\nDemographic data\n- Canadian census data\n\n\nMunicipal data\n- Open data portals (e.g., Toronto, Montreal, & Vancouver)\n\n\nEnvironment\n- NASA‚Äôs Earth Science Data Systems (ESDS) Program- The Canadian Urban Environmental Health Research Consortium- Natural Resources Canada- Environment and Climate Change Canada\n\n\nLand use and built environment\n- OpenStreetMap- Land cover of Canada- Municipal-level zoning maps (e.g., in Toronto)\n\n\nTransportation\n- Metrolinx Open Data for the Greater Toronto Area- Canadian Urban Transit Association- Mobilizing Justice Hub- Transitland\n\n\nIndigenous communities\n- First Nations Data Centre- Native Land Digital\n\n\nHousing and homelessness\n- Housing - Statistics Canada- Housing data from Canada Mortgage and Housing Corporation\n\n\n\nWeb scraping, or extracting information from the internet, is another method for creating datasets. Since the data do not already exist and must be created, this can be more time-intensive than using existing datasets. However, packages like beautifulsoup or selenium in Python make this process easier.\n\n\n\nData format refers to how data is stored and structured. In practice, this is most relevant when loading and saving data. The data format you choose to use depends on the data‚Äôs size, structure, use, how it is being stored, and whether it is spatial (has a geometry column) or not.\nSome of the most common data formats for non-spatial data are:\n\nCSV (comma separated values) .csv\nExcel .xlsx\nJSON (JavaScript Object Notation) .json\nXML (Extensible Markup Language) .xml\n\nSome of the most common data formats for spatial data are (see Spatial data & GIS for more information):\n\nGeoJSON .geojson\nGeoPackage .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nWhile the file formats above suffice for relatively small or simple datasets, very large or complex datasets require more efficient storage via formats like Parquet (see instructions for Python). Relational databases are another commonly used data storage format for ‚Äúbig data‚Äù because they are more efficient, faster to query, more secure, and can be accessed by multiple users.\n\n\n\nIt‚Äôs important to make sure that each variable in your dataset is in the right format so the computer interprets it correctly. For example, if you load a .csv file with a column representing the population of a neighbourhood, you would want to make sure this variable is interpreted as a number and not a string of characters so you can easily use this column to calculate additional statistics (e.g.¬†sum of population in all neighbourhoods, population density in each neighbourhood).\nSee the table below for a list of common data types used in data analysis software.\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample\n\n\n\n\nInteger (int)\nWhole numbers without decimal points.\n5, -3, 42\n\n\nFloat\nNumbers with decimal points, representing real numbers.\n3.14, -0.001, 2.718\n\n\nString (str)\nA sequence of characters, used for textual data.\n\"Hello\", \"Data analysis\", \"123\"\n\n\nBoolean (bool)\nRepresents binary values: True or False.\nTrue, False\n\n\nList\nOrdered collection of items, can contain different data types.\n[1, 2, 3], [\"apple\", \"banana\"]\n\n\nTuple\nImmutable sequence of items, like lists but cannot be modified.\n(1, 2, 3), (\"apple\", \"banana\")\n\n\nDictionary (dict)\nCollection of key-value pairs, often used for mapping.\n{\"country\": \"Alice\", \"age\": 30}\n\n\nDateTime\nUsed for representing date and time.\n2025-04-09 14:32:00, 2021-01-01\n\n\nSet\nUnordered collection of unique items.\n{1, 2, 3}, {\"apple\", \"banana\"}\n\n\nNoneType\nRepresents the absence of a value, or null.\nNone\n\n\n\nDifferent software (e.g.¬†Excel, Python, R, QGIS, etc.) might have slightly different names for each of the above. For example, this page provides a nice summary of data types in Python.\n\n\n\nWhile data types specify what kind of data a variable can hold, levels of measurement describe how data is structured and the relationships between different values. They refer to how we can classify and interpret the data in terms of its inherent ordering, spacing, and possible mathematical operations.\n\nNominal: Categorical data with no inherent order (e.g., colors, countries, land-use types e.g.¬†Urban, Wetlands, Forest, etc.).\nOrdinal: Data with a meaningful order, but unknown or not always equal differences between values (e.g., movie ratings like Good,Okay, or Bad, or levels of education e.g.¬†High School, Bachelors, Masters ).\nInterval: Ordered data with equal intervals between values but no true zero (e.g., temperature in Celsius or Fahrenheit, datetime).\nRatio: Ordered data with equal intervals and a true zero point, allowing for meaningful ratios (e.g., length, area, income).\n\n\n\n\nThere are hundreds of software and tools for processing, analyzing, and visualizing data.\nWhen choosing which software to use to analyze or visualize data, one of the main considerations is whether the software is open source or proprietary. Open source software has its source code publicly available and can be modified by anyone on the internet. Proprietary software‚Äôs source code is not publicly available, is typically developed and updated by a closed group, and is licensed to users in exchange for payment. Read this link if you‚Äôre interested in learning more about the difference between the two.\nIn this course, we will focus on open source software because it is free and available to everyone. Below is a list of some of the main open source programming languages, software, and tools that we recommend using for data analysis and mapping.\n\n\n\nPurpose\nSoftware/Tools\n\n\n\n\nAnalyzing and visualizing data\nPython, R, SQL\n\n\nMaking pretty graphics\nInkscape, GIMP\n\n\nWeb development\nHTML, CSS, Javascript\n\n\nWeb-based maps and visualization\nD3, MapLibre, Protomaps\n\n\nHosting / project management\nGitHub\n\n\n\n\n\n\nWhile is no set of specific step-by-step instructions for data analysis ‚Äì each project involves unique data sources, variables, methodologies, and outputs ‚Äì but there is a general framework that we recommend following:\n\nDefine the problem or research question. What question are you trying to answer with data? Is data analysis the best way to answer that question? Who is the audience for your data analysis, and what do they want to know?\nCollect data. What kind of data do you need to answer your research question, and where can you find it? Does it exist? In what format?\nClean data. Make sure the data has appropriate variable names, does not have misspellings or other errors, and the variables are the correct data types. Get rid of any redundant or irrelevant data that you don‚Äôt need, and determine a method for dealing with any missing values.\nAnalyze data. Start by exploring the data to understand its structure and any statistical patterns. Then perform your analysis ‚Äì for example, are you trying to uncover trends, or measure the relationship among variables?\nVisualize data. Create plots, maps, or other visual representations that illustrate the structure, trends, or relationships present in your data.\nPresent data. Clearly communicate your results to your intended audience. This could involve writing a report, or creating a presentation or interactive dashboard. Whatever gets your message across!\n\nIt is important to iterate through some of these steps and make updates as needed. For example, if step 5 (visualize your data) reveals that you have an imbalanced dataset, you may need to go back to step 3 or 4 to address this. And of course once you hit steps 5. and 6., you might find something super interesting in your data that you‚Äôll want to collect more data and repeat the process all over again! :)\n\n\n\nLearning new software for data analysis or mapping can be confusing and frustrating. Luckily, there are a lot of great resources that can help!\nThe first place you should look when you‚Äôre confused about how to do something is the official documentation. For example, if you‚Äôre having trouble loading a CSV file in Python using the pandas package, take a look at the documentation for .read_csv on the pandas website. Or if you‚Äôre not sure how to create a spatial buffer in QGIS, check out the QGIS buffer operations page.\nIf you‚Äôre still stuck on a question, Google it! Chances are, someone else has dealt with a similar issue, and there is likely a community of people helping them out. For example, one of the most popular resources for coding is Stack Overflow, a website where programmers ask and answer questions. Responses with the most votes are shown at the top, making it easy to find helpful code snippets and explanations that you can adapt for your own needs. The website is so widely used that Stack Overflow posts will usually show up towards the top when you Google search coding questions.\nThere are also websites like W3Schools and GeeksforGeeks that offer online courses and tutorials covering everything from sorting a list in Python to building complicated statistical models. These websites show up often as results for relevant Google searches.\nAnd of course there are AI chatbots like ChatGPT. These tools can be extremely helpful for debugging code, writing code, or providing instructions for GIS. However, be careful and don‚Äôt trust them blindly, as they are often wrong, and sometimes make up packages or functions that don‚Äôt exist. Also, if you use chatbots for help, make sure you understand what they are telling you. Asking for guidance or hints about specific, discrete questions is much better than asking the chatbot to write an entire Python script for you. The more you rely on chatbots, the less you will learn, and the less you will be able to do on your own. Learning data analysis or coding in particular can feel like an uphill battle, but if you start with a solid foundation and thorough understanding of how it works, the better you will be able to prompt and efficiently use chatbots, and importantly you will have a stronger base to then tackle complicated problems in the future.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#where-does-data-come-from",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#where-does-data-come-from",
    "title": "Introduction to urban data",
    "section": "",
    "text": "In data analysis, understanding the type and origin of data is essential to choosing the right methods for analysis, limitations in the data, and interpreting results. The following categories outline common types of data sources that we often encounter. These sources vary in how and why they are collected.\n\n\n\n\n\n\n\n\nData source type\nDescription\nExamples\n\n\n\n\nDesigned/Survey\nData collected through surveys, experiments, or designed methods.\nCensus data (historical or current), research surveys, opinion polls\n\n\nAdministrative\nData from routine operations or official records. Often open when government-run.\nCity records (public housing locations, public transit data), tax records, healthcare data (historical and current), school enrollment data\n\n\nCrowdsourced\nData contributed by the public or community-driven platforms.\nOpenStreetMap (OSM), Wikipedia, social media data, 311 data, Google reviews\n\n\nEvent-Driven/Real-Time\nData generated from sensors, transactions, or interactions. Often continuous and time-sensitive.\nSatellite data, mobile phone GPS, IoT sensors, e-commerce transactions, website clicks\n\n\nDerived data\nData created by transforming or calculating metrics from existing data or simulations.\nIndices like low-income prevalence or social deprivation, environmental quality scores, simulated datasets for testing or model training\n\n\n\nSometimes data can be a combination, for example, both the United States Census Bureau and Statistics Canada collect a combination of survey data and administrative data.\n\n\n\n\nMap of average household income in Toronto using data from Statistics Canada (2020)",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-availability",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-availability",
    "title": "Introduction to urban data",
    "section": "",
    "text": "Data availability refers to how accessible data is and the conditions under which it can be used. Key categories include:\nPublic / Open data - Data that is freely accessible to anyone and can be reused without restrictions. Open data is often provided by governments, research institutions, and public organizations. It is typically non-sensitive and available in machine-readable formats. Examples include aggregated census data, municipal open data, and OpenStreetMap.\nRestricted data Data that is available but comes with limitations due to privacy, security, or legal concerns. This includes sensitive datasets such as health data, dis-aggregate census data, and government records with personal identifiers that are redacted or protected.\nProprietary data - Data that is owned by a specific entity (e.g., a corporation or private organization) and is not freely available. Access is typically granted through licenses, paid subscriptions, or agreements. For example, cell phone mobility data from Spectus can be used to measure post-pandemic downtown recovery trends, real estate data from Costar can be used to assess vacancy rates or rent prices, and consumer data from Data Axle can be used to study the impact of new housing on migration patterns.\n\n\n\n\nCrowdsourced OpenStreetMap data in Vancouver",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-sources-for-urban-analysis",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-sources-for-urban-analysis",
    "title": "Introduction to urban data",
    "section": "",
    "text": "The table below lists a handful of websites where you can find publicly available data about cities, the environment, land use, transportation, Indigenous communities, housing and homelessness. This is a non-exhaustive list; there are many other great data sources available. Note that municipalities‚Äô open data portals typically contain information on all or most of these topics.\n\n\n\nTopic\nData sources\n\n\n\n\nDemographic data\n- Canadian census data\n\n\nMunicipal data\n- Open data portals (e.g., Toronto, Montreal, & Vancouver)\n\n\nEnvironment\n- NASA‚Äôs Earth Science Data Systems (ESDS) Program- The Canadian Urban Environmental Health Research Consortium- Natural Resources Canada- Environment and Climate Change Canada\n\n\nLand use and built environment\n- OpenStreetMap- Land cover of Canada- Municipal-level zoning maps (e.g., in Toronto)\n\n\nTransportation\n- Metrolinx Open Data for the Greater Toronto Area- Canadian Urban Transit Association- Mobilizing Justice Hub- Transitland\n\n\nIndigenous communities\n- First Nations Data Centre- Native Land Digital\n\n\nHousing and homelessness\n- Housing - Statistics Canada- Housing data from Canada Mortgage and Housing Corporation\n\n\n\nWeb scraping, or extracting information from the internet, is another method for creating datasets. Since the data do not already exist and must be created, this can be more time-intensive than using existing datasets. However, packages like beautifulsoup or selenium in Python make this process easier.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-formats",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-formats",
    "title": "Introduction to urban data",
    "section": "",
    "text": "Data format refers to how data is stored and structured. In practice, this is most relevant when loading and saving data. The data format you choose to use depends on the data‚Äôs size, structure, use, how it is being stored, and whether it is spatial (has a geometry column) or not.\nSome of the most common data formats for non-spatial data are:\n\nCSV (comma separated values) .csv\nExcel .xlsx\nJSON (JavaScript Object Notation) .json\nXML (Extensible Markup Language) .xml\n\nSome of the most common data formats for spatial data are (see Spatial data & GIS for more information):\n\nGeoJSON .geojson\nGeoPackage .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nWhile the file formats above suffice for relatively small or simple datasets, very large or complex datasets require more efficient storage via formats like Parquet (see instructions for Python). Relational databases are another commonly used data storage format for ‚Äúbig data‚Äù because they are more efficient, faster to query, more secure, and can be accessed by multiple users.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-types",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-types",
    "title": "Introduction to urban data",
    "section": "",
    "text": "It‚Äôs important to make sure that each variable in your dataset is in the right format so the computer interprets it correctly. For example, if you load a .csv file with a column representing the population of a neighbourhood, you would want to make sure this variable is interpreted as a number and not a string of characters so you can easily use this column to calculate additional statistics (e.g.¬†sum of population in all neighbourhoods, population density in each neighbourhood).\nSee the table below for a list of common data types used in data analysis software.\n\n\n\n\n\n\n\n\nData Type\nDescription\nExample\n\n\n\n\nInteger (int)\nWhole numbers without decimal points.\n5, -3, 42\n\n\nFloat\nNumbers with decimal points, representing real numbers.\n3.14, -0.001, 2.718\n\n\nString (str)\nA sequence of characters, used for textual data.\n\"Hello\", \"Data analysis\", \"123\"\n\n\nBoolean (bool)\nRepresents binary values: True or False.\nTrue, False\n\n\nList\nOrdered collection of items, can contain different data types.\n[1, 2, 3], [\"apple\", \"banana\"]\n\n\nTuple\nImmutable sequence of items, like lists but cannot be modified.\n(1, 2, 3), (\"apple\", \"banana\")\n\n\nDictionary (dict)\nCollection of key-value pairs, often used for mapping.\n{\"country\": \"Alice\", \"age\": 30}\n\n\nDateTime\nUsed for representing date and time.\n2025-04-09 14:32:00, 2021-01-01\n\n\nSet\nUnordered collection of unique items.\n{1, 2, 3}, {\"apple\", \"banana\"}\n\n\nNoneType\nRepresents the absence of a value, or null.\nNone\n\n\n\nDifferent software (e.g.¬†Excel, Python, R, QGIS, etc.) might have slightly different names for each of the above. For example, this page provides a nice summary of data types in Python.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#levels-of-measurement",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#levels-of-measurement",
    "title": "Introduction to urban data",
    "section": "",
    "text": "While data types specify what kind of data a variable can hold, levels of measurement describe how data is structured and the relationships between different values. They refer to how we can classify and interpret the data in terms of its inherent ordering, spacing, and possible mathematical operations.\n\nNominal: Categorical data with no inherent order (e.g., colors, countries, land-use types e.g.¬†Urban, Wetlands, Forest, etc.).\nOrdinal: Data with a meaningful order, but unknown or not always equal differences between values (e.g., movie ratings like Good,Okay, or Bad, or levels of education e.g.¬†High School, Bachelors, Masters ).\nInterval: Ordered data with equal intervals between values but no true zero (e.g., temperature in Celsius or Fahrenheit, datetime).\nRatio: Ordered data with equal intervals and a true zero point, allowing for meaningful ratios (e.g., length, area, income).",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#software-and-tools",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#software-and-tools",
    "title": "Introduction to urban data",
    "section": "",
    "text": "There are hundreds of software and tools for processing, analyzing, and visualizing data.\nWhen choosing which software to use to analyze or visualize data, one of the main considerations is whether the software is open source or proprietary. Open source software has its source code publicly available and can be modified by anyone on the internet. Proprietary software‚Äôs source code is not publicly available, is typically developed and updated by a closed group, and is licensed to users in exchange for payment. Read this link if you‚Äôre interested in learning more about the difference between the two.\nIn this course, we will focus on open source software because it is free and available to everyone. Below is a list of some of the main open source programming languages, software, and tools that we recommend using for data analysis and mapping.\n\n\n\nPurpose\nSoftware/Tools\n\n\n\n\nAnalyzing and visualizing data\nPython, R, SQL\n\n\nMaking pretty graphics\nInkscape, GIMP\n\n\nWeb development\nHTML, CSS, Javascript\n\n\nWeb-based maps and visualization\nD3, MapLibre, Protomaps\n\n\nHosting / project management\nGitHub",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-analysis-process",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#data-analysis-process",
    "title": "Introduction to urban data",
    "section": "",
    "text": "While is no set of specific step-by-step instructions for data analysis ‚Äì each project involves unique data sources, variables, methodologies, and outputs ‚Äì but there is a general framework that we recommend following:\n\nDefine the problem or research question. What question are you trying to answer with data? Is data analysis the best way to answer that question? Who is the audience for your data analysis, and what do they want to know?\nCollect data. What kind of data do you need to answer your research question, and where can you find it? Does it exist? In what format?\nClean data. Make sure the data has appropriate variable names, does not have misspellings or other errors, and the variables are the correct data types. Get rid of any redundant or irrelevant data that you don‚Äôt need, and determine a method for dealing with any missing values.\nAnalyze data. Start by exploring the data to understand its structure and any statistical patterns. Then perform your analysis ‚Äì for example, are you trying to uncover trends, or measure the relationship among variables?\nVisualize data. Create plots, maps, or other visual representations that illustrate the structure, trends, or relationships present in your data.\nPresent data. Clearly communicate your results to your intended audience. This could involve writing a report, or creating a presentation or interactive dashboard. Whatever gets your message across!\n\nIt is important to iterate through some of these steps and make updates as needed. For example, if step 5 (visualize your data) reveals that you have an imbalanced dataset, you may need to go back to step 3 or 4 to address this. And of course once you hit steps 5. and 6., you might find something super interesting in your data that you‚Äôll want to collect more data and repeat the process all over again! :)",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#getting-help",
    "href": "notebooks/urban-data-analytics/what-and-where-of-data/what-and-where-of-data.html#getting-help",
    "title": "Introduction to urban data",
    "section": "",
    "text": "Learning new software for data analysis or mapping can be confusing and frustrating. Luckily, there are a lot of great resources that can help!\nThe first place you should look when you‚Äôre confused about how to do something is the official documentation. For example, if you‚Äôre having trouble loading a CSV file in Python using the pandas package, take a look at the documentation for .read_csv on the pandas website. Or if you‚Äôre not sure how to create a spatial buffer in QGIS, check out the QGIS buffer operations page.\nIf you‚Äôre still stuck on a question, Google it! Chances are, someone else has dealt with a similar issue, and there is likely a community of people helping them out. For example, one of the most popular resources for coding is Stack Overflow, a website where programmers ask and answer questions. Responses with the most votes are shown at the top, making it easy to find helpful code snippets and explanations that you can adapt for your own needs. The website is so widely used that Stack Overflow posts will usually show up towards the top when you Google search coding questions.\nThere are also websites like W3Schools and GeeksforGeeks that offer online courses and tutorials covering everything from sorting a list in Python to building complicated statistical models. These websites show up often as results for relevant Google searches.\nAnd of course there are AI chatbots like ChatGPT. These tools can be extremely helpful for debugging code, writing code, or providing instructions for GIS. However, be careful and don‚Äôt trust them blindly, as they are often wrong, and sometimes make up packages or functions that don‚Äôt exist. Also, if you use chatbots for help, make sure you understand what they are telling you. Asking for guidance or hints about specific, discrete questions is much better than asking the chatbot to write an entire Python script for you. The more you rely on chatbots, the less you will learn, and the less you will be able to do on your own. Learning data analysis or coding in particular can feel like an uphill battle, but if you start with a solid foundation and thorough understanding of how it works, the better you will be able to prompt and efficiently use chatbots, and importantly you will have a stronger base to then tackle complicated problems in the future.",
    "crumbs": [
      "Urban Data Analytics",
      "Introduction to urban data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "This notebook provides an introduction for analyzing urban data. It will cover ‚Ä¶\n\nExploring, filtering, and sorting data\nCleaning data and removing missing data\nCreating new columns from existing data\nJoining data from multiple tables\nComputing descriptive statistics (sum, mean, etc.)\nAggregating data via cross-tabulations and pivot tables\n\nTo do this, we‚Äôll primarily be using pandas, a Python library for analyzing and organizing tabular data.\n\nimport pandas as pd\n\npandas is probably the most common library for working with both big and small datasets in Python, and is the basis for working with more analytical packages (e.g.¬†numpy, scipy, scikit-learn) and analyzing geographic data (e.g.¬†geopandas). For each section, we‚Äôll also link to relevant documentation for doing similar tasks in Microsoft Excel and Google Sheets.\nHere are download links to this notebook and example datasets.\n\nDownload Notebook\n\ncities.csv\n\ncapitals.csv\n\nnew_york_cities.csv\n\n\n\nA very common way of structuring and analyzing a dataset is in a 2-dimensional table format, where rows are individual records or observations, while columns are attributes (often called variables) about those observations. For example, rows could each be a city and columns could indicate the population for different time periods. Data stored in spreadsheets often take this format.\nIn pandas, a DataFrame is a tabular data structure similar to a spreadsheet, where data is organized in rows and columns. Columns can contain different kinds of data, such as numbers, strings, dates, and so on. When we load data in pandas, we typically load it into the structure of a DataFrame.\nLet‚Äôs first take a look at a small dataset, Canadian municipalities and their population in 2021 and 2016, based on Census data. In Statistics Canada lingo, these are called Census Subdivisions. This dataset only includes municipalities with a population greater than 25,000 in 2021.\nThe main method for loading csv data is to use the read_csv function, but pandas can also read and write many other data formats.\n\ndf = pd.read_csv(\"data/cities.csv\")\n\nGreat! Now our data is stored in the variable df in the structure of a DataFrame.\n\n\n\nIn spreadsheet software, when we open a data file, we will see the top rows of the data right away.\nIn pandas, we can simply type the name of the DataFrame, in this case df, in the cell to view it. By default, it will print the top and bottom rows in the DataFrame\n\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nWinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nWe can specify which rows we want to view.\nLet‚Äôs explore what this data frame looks like. Adding the function .head(N) or .tail(N) prints the top or bottom N rows of the DataFrame. The following prints the first 10 rows.\nTry to edit this to print the bottom 10 rows or a different number of rows.\n\ndf.head(10)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n7\nBlainville\nQue.\n59819.0\nNaN\n\n\n8\nBoisbriand\nQue.\n28308.0\n26884.0\n\n\n9\nBoucherville\nQue.\n41743.0\n41671.0\n\n\n\n\n\n\n\nNotice that each column has a unique name. We can view the data of this column alone by using that name, and see what unique values exist using .unique().\nTry viewing the data of another column. Beware of upper and lower case ‚Äì exact names matter!\n\ndf['Prov/terr'].head(10)  # Top 10 only\n\n0     B.C.\n1    Alta.\n2     Ont.\n3     Que.\n4     Ont.\n5     Ont.\n6     Ont.\n7     Que.\n8     Que.\n9     Que.\nName: Prov/terr, dtype: object\n\n\n\ndf['Prov/terr'].unique()  # Unique values for the *full* dataset - what happens if you do df['Prov/terr'].head(10).unique()?\n\narray(['B.C.', 'Alta.', 'Ont.', 'Que.', 'Man.', nan, 'N.S.', 'P.E.I.',\n       'N.L.', 'N.B.', 'Sask.', 'Y.T.'], dtype=object)\n\n\n\n\n\nWe often want to look at only a portion of our data that fit some set of conditions (e.g.¬†all cities in a province that have a population more than 100,000). This is often called filtering, querying, or subsetting a dataset.\nLet‚Äôs try to do some filtering in pandas with our data of Canadian cities. Check out these links for filtering and sorting in spreadsheet software:\n\nFiltering in Excel\nFiltering in Google Sheets\n\nWe can use the columns to identify data that we might want to filter by. The line below shows data only for Ontario, but see if you can filter for another province or territory.\n\ndf.loc[df['Prov/terr'] == 'Ont.']\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n10\nBradford West Gwillimbury\nOnt.\n42880.0\n35325.0\n\n\n...\n...\n...\n...\n...\n\n\n171\nWhitby\nOnt.\n138501.0\n128377.0\n\n\n172\nWhitchurch-Stouffville\nOnt.\n49864.0\n45837.0\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n67 rows √ó 4 columns\n\n\n\nPandas allows us to use other similar mathematical concepts filter for data. Previously, we asked for all data in Ontario.\nNow, filter for all cities which had a population of at least 100,000 in 2021.\nHINT: in Python, ‚Äúgreater than or equals to‚Äù (i.e., ‚Äúat least‚Äù) is represented using the syntax &gt;=.\nPandas also allows us to combine filtering conditions.\nUse the template below to select for all cities in Ontario with a population of over 100,000 in 2021.\n\ndf.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n\n\n  Cell In[8], line 1\n    df.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n                                          ^\nSyntaxError: invalid syntax. Perhaps you forgot a comma?\n\n\n\n\nNow let‚Äôs count how many cities actually meet these conditions. Run the line below to see how many cities there are in this data set in Ontario.\n\ndf.loc[df['Prov/terr'] == 'Ont.'].count()\n\nName                66\nProv/terr           67\nPopulation, 2021    66\nPopulation, 2016    65\ndtype: int64\n\n\nThe function .count() tells us how much data there is for each column - but if we wanted to just see one column, we could also filter for that individual column using df[COL_NAME].\nTry a different condition and count the amount of data for it.\n\n\n\nYou might have noticed that these cities are in alphabetical order - what if we wanted to see them in the order of population? In pandas, we do this using the sort_values function. The default is to sort in ascending order, so we set this to be False (i.e.¬†descending) so the most populous cities are at the top.\n\ndf.sort_values(by='Population, 2021', ascending=False)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n\n\n89\nMontr√©al\nQue.\n1762949.0\n1704694.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n\n\n...\n...\n...\n...\n...\n\n\n115\nPrince Edward County\nOnt.\n25704.0\n24735.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n40\nDrummondville\nQue.\nNaN\n75423.0\n\n\n109\nPeterborough\nOnt.\nNaN\nNaN\n\n\n169\nWest Kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nLet‚Äôs put some in this together now.\nFilter the data to show all cities which are in the province of Quebec with at least a population of 50,000 in 2016, and then try to sort the cities by their 2016 population.\nHINT: You can do this in two steps (which is more readable) by storing the data that you filter into a variable called df_filtered, then running the command to sort the values on df_filtered.\n\n\n\nOnce we have processed and analyzed our data, we may want to save it for future use or share it with others. pandas makes it easy to export data to various formats, such as CSV or Excel files.\nBelow, we demonstrate how to export a DataFrame to both CSV and Excel formats. This is particularly useful for sharing results or viewing the data in other tools like spreadsheet software.\n\n# Save to CSV\ndf_filtered.to_csv('df_filtered.csv', index=False)\n\n# Save to Excel\ndf_filtered.to_excel('df_filtered.xlsx', index=False)\n\n\n\n\nOften, the data we have might not be in the condition want it to be in. Some data might be missing, and other data might have odd naming conventions.\nA simple example is that we might want all city names to be lowercase - which is what the code below does.\n\ndf['Name'] = df['Name'].str.lower()\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nabbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nairdrie\nAlta.\n74100.0\n61581.0\n\n\n2\najax\nOnt.\n126666.0\n119677.0\n\n\n3\nalma\nQue.\n30331.0\n30771.0\n\n\n4\naurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nwindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nwinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nwood buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nwoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nwoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows √ó 4 columns\n\n\n\npandas has a number of methods like str.lower() to alter data (see the full API). But the important thing to note here is that we directly modified the existing values of a column. We might not always want to do this, but often it is a good way of saving memory and shows that data frames are not just static forms but modifiable.\nLikewise, we might want better names for the columns we have. Take a look at the API for .rename() and modify the data frame df such that we rename the column Name to City. Pandas has more methods than we could ever remember - so learning to navigate the API is a crucial part of using the library.\nHINT: Take a look at the first example\n\n\n\nUnfortunately, it is pretty common that dataset we work with will be missing data values for some rows and columns. This can create complications when we want to produce summary statistics or visualizations. There are different strategies for dealing with this (i.e., imputing data), but the easiest is just to remove them. But first come out let‚Äôs check how much data is missing.\n\ndf.isnull().sum()\n\nName                3\nProv/terr           4\nPopulation, 2021    3\nPopulation, 2016    4\ndtype: int64\n\n\nIt seems that each column has a couple of data points missing. Let‚Äôs take a look at which rows these occur in. Similar to how we created a condition to filter for certain data, the code below creates a condition to filter for rows with missing data.\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n7\nblainville\nQue.\n59819.0\nNaN\n\n\n18\ncaledon\nNaN\n76581.0\n66502.0\n\n\n30\nNaN\nOnt.\n101427.0\n92013.0\n\n\n40\ndrummondville\nQue.\nNaN\n75423.0\n\n\n51\ngrimsby\nOnt.\n28883.0\nNaN\n\n\n64\nla prairie\nNaN\n26406.0\n24110.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n86\nmission\nNaN\n41519.0\n38554.0\n\n\n109\npeterborough\nOnt.\nNaN\nNaN\n\n\n131\nNaN\nQue.\n29954.0\n27359.0\n\n\n157\ntimmins\nNaN\n41145.0\n41788.0\n\n\n169\nwest kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n\n\n\nYou can see that some rows are missing multiple values, While others are just missing one. We can remove rows which have missing data using the function dropna and assign it to df so we‚Äôre working with complete data only going forward. Try to modify the code below to drop rows whose empty values are in one of the two population columns - that is, if the name or province is missing, we want to keep that row still. Look at the API to figure this out, specifically the argument subset for .dropna()\n\ndf = df.dropna()\n\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n\n\n\n\n\nGreat. Now let‚Äôs reset to our original data frame and exclude any rows with missing values.\n\ndf = pd.read_csv(\"data/cities.csv\")\ndf = df.dropna()\n\nInstead of dropping (i.e.¬†removing) rows with missing values, we can instead replace them with specific values with fillna. For example, df.fillna(0) would replace all missing data values with a 0. This wouldn‚Äôt make sense in our data of Canadian cities, but can be useful in other contexts.\nSimilarly we can promgramatically find and replace data in any other column or across our dataset. For example, if we wanted to rename 'Y.T.' to 'Yukon' we would run the replace function as so df = df.replace('Y.T.', 'Yukon')\n\n\nWe can add or delete columns as needed. Let‚Äôs first add a column which shows the change in population between 2021 and 2016 and then sort by the cities that lost the most people. We can calculate that via a simple subtraction as follows.\n\ndf[\"pop_change\"] = df[\"Population, 2021\"] - df[\"Population, 2016\"]\ndf.sort_values(\"pop_change\", ascending = False).head(5)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\npop_change\n\n\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n83206.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n77811.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n67564.0\n\n\n11\nBrampton\nOnt.\n656480.0\n593638.0\n62842.0\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n62785.0\n\n\n\n\n\n\n\nPandas supports mathematical equations between columns, just like the subtraction we did above. Create a new column called pct_pop_change that computes the percentage change in population between 2016 and 2021, and sort by the cities with the greatest increase.\nHINT: the way to compute percent change is 100 * (Y - X) / X.\nNow let‚Äôs clear these new columns out using drop to return to what we had originally.\n\ndf = df.drop(columns=['pop_change', 'pct_pop_change'])\n\n\n\n\nMuch of the time, we are working with multiple sets of data which may overlap in key ways. Perhaps we want to include measures of income in cities, or look at voting patterns - this may require us to combine multiple data frames so we can analyze them.\nPandas methods to combine data frames are quite similar to that of database operations - if you‚Äôve worked with SQL before, this will come very easily to you. There‚Äôs an extensive tutorial on this topic, but we‚Äôll focus on simple cases of pd.concat() and pd.merge(). If you are curious about how to do this in spreadsheet software like Excel, check out this tutorial\nFirst, we can use pd.concat() to stack DataFrames vertically when new data has the same columns but additional rows (e.g., adding cities from another region). This is like adding new entries to a database table.\n\ndf_ny_cities = pd.read_csv(\"./data/new_york_cities.csv\")\ncombined = pd.concat([df, df_ny_cities], ignore_index=True)\nprint(\"Original rows:\", len(df), \"| After append:\", len(combined))\ndisplay(combined.tail(5))  # Show new rows\n\nOriginal rows: 167 | After append: 169\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n164\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n165\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n166\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n167\nNew York City\nN.Y.\n8804190.0\n8537673.0\n\n\n168\nBuffalo\nN.Y.\n278349.0\n258071.0\n\n\n\n\n\n\n\nSecond, we can use pd_merge() (or pd.concat(axis=1)) to combine DataFrames side-by-side when they share a key column (e.g., city names). Here, we‚Äôll a column denoting whether the city is a provincial capital by matching city names.\nLet‚Äôs first load this data:\n\ndf_capitals = pd.read_csv('./data/capitals.csv')\ndf_capitals\n\n\n\n\n\n\n\n\nName\nIs_Capital\n\n\n\n\n0\nToronto\nTrue\n\n\n1\nQu√©bec\nTrue\n\n\n2\nVictoria\nTrue\n\n\n3\nEdmonton\nTrue\n\n\n4\nWinnipeg\nTrue\n\n\n5\nFredericton\nTrue\n\n\n6\nHalifax\nTrue\n\n\n7\nCharlottetown\nTrue\n\n\n8\nSt. John's\nTrue\n\n\n9\nRegina\nTrue\n\n\n10\nWhitehorse\nTrue\n\n\n\n\n\n\n\n\ndf_with_capitals = pd.merge(\n    df,  # Original data\n    df_capitals,  # New data\n    on=\"Name\",  # The same column \n    how=\"left\"  # Keep all data from the \"left\", ie. original\n)\n\n# Set non-capitals to False\ndf_with_capitals[\"Is_Capital\"] = df_with_capitals[\"Is_Capital\"].astype('boolean').fillna(False)\n\n# Verify: Show capitals and counts\nprint(f\"Found {df_with_capitals['Is_Capital'].sum()} capitals:\")\ndf_with_capitals[df_with_capitals[\"Is_Capital\"]]\n\nFound 11 capitals:\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\nIs_Capital\n\n\n\n\n23\nCharlottetown\nP.E.I.\n38809.0\n36094.0\nTrue\n\n\n38\nEdmonton\nAlta.\n1010899.0\n933088.0\nTrue\n\n\n41\nFredericton\nN.B.\n63116.0\n58721.0\nTrue\n\n\n49\nHalifax\nN.S.\n439819.0\n403131.0\nTrue\n\n\n108\nQu√©bec\nQue.\n549459.0\n531902.0\nTrue\n\n\n111\nRegina\nSask.\n226404.0\n215106.0\nTrue\n\n\n139\nSt. John's\nN.L.\n110525.0\n108860.0\nTrue\n\n\n147\nToronto\nOnt.\n2794356.0\n2731571.0\nTrue\n\n\n154\nVictoria\nB.C.\n91867.0\n85792.0\nTrue\n\n\n161\nWhitehorse\nY.T.\n28201.0\n25085.0\nTrue\n\n\n163\nWinnipeg\nMan.\n749607.0\n705244.0\nTrue\n\n\n\n\n\n\n\n\n\n\n\nThe data we have is only as good as we understand what‚Äôs going on. There‚Äôs some basic methods in pandas we can use to get an idea of what the data says.\nThe most basic function you can use to get an idea of what‚Äôs going on is .describe(). It shows you how much data there is, and a number of summary statistics.\nModify the code below to report summary statistics for cities in Quebec only.\n\ndf.describe()\n\n\n\n\n\n\n\n\nPopulation, 2021\nPopulation, 2016\n\n\n\n\ncount\n1.670000e+02\n1.670000e+02\n\n\nmean\n1.573169e+05\n1.487358e+05\n\n\nstd\n3.074452e+05\n2.959960e+05\n\n\nmin\n2.570400e+04\n2.378700e+04\n\n\n25%\n3.770050e+04\n3.449850e+04\n\n\n50%\n6.414100e+04\n6.316600e+04\n\n\n75%\n1.348910e+05\n1.255940e+05\n\n\nmax\n2.794356e+06\n2.731571e+06\n\n\n\n\n\n\n\nInstead of picking out an examining a subset of the data one by one, we can use the function .groupby(). Given a column name, it will group rows which have the same value. In the example below, that means grouping every row which has the same province name. We can then apply a function to this (or multiple functions using .agg()) to examine different aspects of the data.\n\n# Group by province and calculate total population\nprovince_pop = df.groupby('Prov/terr')['Population, 2021'].sum()\nprint(\"Total population by province:\")\nprovince_pop.sort_values(ascending=False)\n\nTotal population by province:\n\n\nProv/terr\nOnt.      11598308.0\nQue.       5474109.0\nB.C.       3662922.0\nAlta.      3192892.0\nMan.        800920.0\nSask.       563966.0\nN.S.        533513.0\nN.B.        240595.0\nN.L.        137693.0\nP.E.I.       38809.0\nY.T.         28201.0\nName: Population, 2021, dtype: float64\n\n\n\n# Multiple aggregation statistics\nstats = df.groupby('Prov/terr')['Population, 2021'].agg(['count', 'mean', 'max', 'sum'])\nstats\n\n\n\n\n\n\n\n\ncount\nmean\nmax\nsum\n\n\nProv/terr\n\n\n\n\n\n\n\n\nAlta.\n17\n187817.176471\n1306784.0\n3192892.0\n\n\nB.C.\n29\n126307.655172\n662248.0\n3662922.0\n\n\nMan.\n2\n400460.000000\n749607.0\n800920.0\n\n\nN.B.\n4\n60148.750000\n79470.0\n240595.0\n\n\nN.L.\n2\n68846.500000\n110525.0\n137693.0\n\n\nN.S.\n2\n266756.500000\n439819.0\n533513.0\n\n\nOnt.\n64\n181223.562500\n2794356.0\n11598308.0\n\n\nP.E.I.\n1\n38809.000000\n38809.0\n38809.0\n\n\nQue.\n41\n133514.853659\n1762949.0\n5474109.0\n\n\nSask.\n4\n140991.500000\n266141.0\n563966.0\n\n\nY.T.\n1\n28201.000000\n28201.0\n28201.0\n\n\n\n\n\n\n\nBelow, we‚Äôve added a column which shows the percent growth for each city. Use .groupby('Prov/terr') and use the function .median() (just as we used .sum() above) to observe the median growth rate per province or territory. Make sure to use sort_values() and set ascending to False.\n\ndf['Percent_Growth'] = 100 * (df['Population, 2021'] - df['Population, 2016']) / df['Population, 2016'] \n\n\n\n\nA cross tabulation, also called a frequency table or a contingency table, is a table that shows the summarizes two categorical variables by displaying the number of occurrences in each pair of categories.\nLet‚Äôs show an example by counting the number of cities in each province by a categorization of city size.\nWe will need two tools to do this: - cut, which bins continuous numbers (like population) into categories (e.g., ‚ÄúSmall‚Äù/‚ÄúMedium‚Äù/‚ÄúLarge‚Äù), turning numbers into meaningful groups. - crosstab, which counts how often these groups appear together‚Äîlike how many ‚ÄúMedium‚Äù cities exist per province‚Äîrevealing patterns that raw numbers hide.\n\n# Create a size category column\ndf['Size'] = pd.cut(df['Population, 2021'],\n                    bins=[0, 50000, 200000, float('inf')],\n                    labels=['Small', 'Medium', 'Large'])\n\n# Cross-tab: Province vs. Size\nsize_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\nsize_table\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Create a size category column\n----&gt; 2 df['Size'] = pd.cut(df['Population, 2021'],\n      3                     bins=[0, 50000, 200000, float('inf')],\n      4                     labels=['Small', 'Medium', 'Large'])\n      6 # Cross-tab: Province vs. Size\n      7 size_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\n\nNameError: name 'pd' is not defined\n\n\n\nRecall that we just created the column 'Percent_Growth' as well. Use these two functions to create different bins for different levels of growth and cross tabulate them.\nIf you‚Äôve worked with Excel or Google Sheets, this is very similar to doing a Pivot Table.\nPivot tables are able to summarize data across several categories, and for different types of summaries (e.g.¬†sum, mean, median, etc.)\nThe pivot_table function in pandas to aggregate data, and has more options than the crosstab function. Both are quite similar though. Here‚Äôs an example where we are using pivot_table to sum the population in each province by the 3 size groups.\nTry to edit this to compute the mean or median city Percent_Growth\n\npd.pivot_table(data = df, values = ['Population, 2021'], index = ['Prov/terr'], columns = ['Size'], aggfunc = 'sum', observed=True)\n\n\n\n\n\n\n\n\nPopulation, 2021\n\n\nSize\nSmall\nMedium\nLarge\n\n\nProv/terr\n\n\n\n\n\n\n\nAlta.\n234664.0\n640545.0\n2317683.0\n\n\nB.C.\n330537.0\n1642753.0\n1689632.0\n\n\nMan.\nNaN\n51313.0\n749607.0\n\n\nN.B.\n28114.0\n212481.0\nNaN\n\n\nN.L.\n27168.0\n110525.0\nNaN\n\n\nN.S.\nNaN\n93694.0\n439819.0\n\n\nOnt.\n930812.0\n2925641.0\n7741855.0\n\n\nP.E.I.\n38809.0\nNaN\nNaN\n\n\nQue.\n806267.0\n1371544.0\n3296298.0\n\n\nSask.\n71421.0\nNaN\n492545.0\n\n\nY.T.\n28201.0\nNaN\nNaN\n\n\n\n\n\n\n\nThere are often multiple ways to achieve similar results. In the previous section we looked at the groupby function to summarize data by one column, while above we used crosstab or pivot_table. However, we could also use the groupby function for this purpose. Check out the example below.\nYou should notice that it has created a long-table formate rather than a wide-table format. Both formats can be useful, wide-table formats are easier for viewing data for only 2 categories, while long-table formats are often used for inputting data into modelling or visualization libraries.\n\ndf.groupby(['Prov/terr', 'Size'], observed=False)['Population, 2021'].agg(['sum'])\n\n\n\n\n\n\n\n\n\nsum\n\n\nProv/terr\nSize\n\n\n\n\n\nAlta.\nSmall\n234664.0\n\n\nMedium\n640545.0\n\n\nLarge\n2317683.0\n\n\nB.C.\nSmall\n330537.0\n\n\nMedium\n1642753.0\n\n\nLarge\n1689632.0\n\n\nMan.\nSmall\n0.0\n\n\nMedium\n51313.0\n\n\nLarge\n749607.0\n\n\nN.B.\nSmall\n28114.0\n\n\nMedium\n212481.0\n\n\nLarge\n0.0\n\n\nN.L.\nSmall\n27168.0\n\n\nMedium\n110525.0\n\n\nLarge\n0.0\n\n\nN.S.\nSmall\n0.0\n\n\nMedium\n93694.0\n\n\nLarge\n439819.0\n\n\nOnt.\nSmall\n930812.0\n\n\nMedium\n2925641.0\n\n\nLarge\n7741855.0\n\n\nP.E.I.\nSmall\n38809.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0\n\n\nQue.\nSmall\n806267.0\n\n\nMedium\n1371544.0\n\n\nLarge\n3296298.0\n\n\nSask.\nSmall\n71421.0\n\n\nMedium\n0.0\n\n\nLarge\n492545.0\n\n\nY.T.\nSmall\n28201.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#tables-dataframes",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#tables-dataframes",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "A very common way of structuring and analyzing a dataset is in a 2-dimensional table format, where rows are individual records or observations, while columns are attributes (often called variables) about those observations. For example, rows could each be a city and columns could indicate the population for different time periods. Data stored in spreadsheets often take this format.\nIn pandas, a DataFrame is a tabular data structure similar to a spreadsheet, where data is organized in rows and columns. Columns can contain different kinds of data, such as numbers, strings, dates, and so on. When we load data in pandas, we typically load it into the structure of a DataFrame.\nLet‚Äôs first take a look at a small dataset, Canadian municipalities and their population in 2021 and 2016, based on Census data. In Statistics Canada lingo, these are called Census Subdivisions. This dataset only includes municipalities with a population greater than 25,000 in 2021.\nThe main method for loading csv data is to use the read_csv function, but pandas can also read and write many other data formats.\n\ndf = pd.read_csv(\"data/cities.csv\")\n\nGreat! Now our data is stored in the variable df in the structure of a DataFrame.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#viewing-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#viewing-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "In spreadsheet software, when we open a data file, we will see the top rows of the data right away.\nIn pandas, we can simply type the name of the DataFrame, in this case df, in the cell to view it. By default, it will print the top and bottom rows in the DataFrame\n\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nWinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nWe can specify which rows we want to view.\nLet‚Äôs explore what this data frame looks like. Adding the function .head(N) or .tail(N) prints the top or bottom N rows of the DataFrame. The following prints the first 10 rows.\nTry to edit this to print the bottom 10 rows or a different number of rows.\n\ndf.head(10)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nAbbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nAirdrie\nAlta.\n74100.0\n61581.0\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n3\nAlma\nQue.\n30331.0\n30771.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n7\nBlainville\nQue.\n59819.0\nNaN\n\n\n8\nBoisbriand\nQue.\n28308.0\n26884.0\n\n\n9\nBoucherville\nQue.\n41743.0\n41671.0\n\n\n\n\n\n\n\nNotice that each column has a unique name. We can view the data of this column alone by using that name, and see what unique values exist using .unique().\nTry viewing the data of another column. Beware of upper and lower case ‚Äì exact names matter!\n\ndf['Prov/terr'].head(10)  # Top 10 only\n\n0     B.C.\n1    Alta.\n2     Ont.\n3     Que.\n4     Ont.\n5     Ont.\n6     Ont.\n7     Que.\n8     Que.\n9     Que.\nName: Prov/terr, dtype: object\n\n\n\ndf['Prov/terr'].unique()  # Unique values for the *full* dataset - what happens if you do df['Prov/terr'].head(10).unique()?\n\narray(['B.C.', 'Alta.', 'Ont.', 'Que.', 'Man.', nan, 'N.S.', 'P.E.I.',\n       'N.L.', 'N.B.', 'Sask.', 'Y.T.'], dtype=object)",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#filtering-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#filtering-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "We often want to look at only a portion of our data that fit some set of conditions (e.g.¬†all cities in a province that have a population more than 100,000). This is often called filtering, querying, or subsetting a dataset.\nLet‚Äôs try to do some filtering in pandas with our data of Canadian cities. Check out these links for filtering and sorting in spreadsheet software:\n\nFiltering in Excel\nFiltering in Google Sheets\n\nWe can use the columns to identify data that we might want to filter by. The line below shows data only for Ontario, but see if you can filter for another province or territory.\n\ndf.loc[df['Prov/terr'] == 'Ont.']\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n2\nAjax\nOnt.\n126666.0\n119677.0\n\n\n4\nAurora\nOnt.\n62057.0\n55445.0\n\n\n5\nBarrie\nOnt.\n147829.0\n141434.0\n\n\n6\nBelleville\nOnt.\n55071.0\n50716.0\n\n\n10\nBradford West Gwillimbury\nOnt.\n42880.0\n35325.0\n\n\n...\n...\n...\n...\n...\n\n\n171\nWhitby\nOnt.\n138501.0\n128377.0\n\n\n172\nWhitchurch-Stouffville\nOnt.\n49864.0\n45837.0\n\n\n174\nWindsor\nOnt.\n229660.0\n217188.0\n\n\n177\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n67 rows √ó 4 columns\n\n\n\nPandas allows us to use other similar mathematical concepts filter for data. Previously, we asked for all data in Ontario.\nNow, filter for all cities which had a population of at least 100,000 in 2021.\nHINT: in Python, ‚Äúgreater than or equals to‚Äù (i.e., ‚Äúat least‚Äù) is represented using the syntax &gt;=.\nPandas also allows us to combine filtering conditions.\nUse the template below to select for all cities in Ontario with a population of over 100,000 in 2021.\n\ndf.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n\n\n  Cell In[8], line 1\n    df.loc[(df[\"Prov/terr\"] == \"Ont.\") & (YOUR CONDITION HERE)]\n                                          ^\nSyntaxError: invalid syntax. Perhaps you forgot a comma?\n\n\n\n\nNow let‚Äôs count how many cities actually meet these conditions. Run the line below to see how many cities there are in this data set in Ontario.\n\ndf.loc[df['Prov/terr'] == 'Ont.'].count()\n\nName                66\nProv/terr           67\nPopulation, 2021    66\nPopulation, 2016    65\ndtype: int64\n\n\nThe function .count() tells us how much data there is for each column - but if we wanted to just see one column, we could also filter for that individual column using df[COL_NAME].\nTry a different condition and count the amount of data for it.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#sorting-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#sorting-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "You might have noticed that these cities are in alphabetical order - what if we wanted to see them in the order of population? In pandas, we do this using the sort_values function. The default is to sort in ascending order, so we set this to be False (i.e.¬†descending) so the most populous cities are at the top.\n\ndf.sort_values(by='Population, 2021', ascending=False)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n\n\n89\nMontr√©al\nQue.\n1762949.0\n1704694.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n\n\n...\n...\n...\n...\n...\n\n\n115\nPrince Edward County\nOnt.\n25704.0\n24735.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n40\nDrummondville\nQue.\nNaN\n75423.0\n\n\n109\nPeterborough\nOnt.\nNaN\nNaN\n\n\n169\nWest Kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n179 rows √ó 4 columns\n\n\n\nLet‚Äôs put some in this together now.\nFilter the data to show all cities which are in the province of Quebec with at least a population of 50,000 in 2016, and then try to sort the cities by their 2016 population.\nHINT: You can do this in two steps (which is more readable) by storing the data that you filter into a variable called df_filtered, then running the command to sort the values on df_filtered.",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#exporting-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#exporting-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Once we have processed and analyzed our data, we may want to save it for future use or share it with others. pandas makes it easy to export data to various formats, such as CSV or Excel files.\nBelow, we demonstrate how to export a DataFrame to both CSV and Excel formats. This is particularly useful for sharing results or viewing the data in other tools like spreadsheet software.\n\n# Save to CSV\ndf_filtered.to_csv('df_filtered.csv', index=False)\n\n# Save to Excel\ndf_filtered.to_excel('df_filtered.xlsx', index=False)",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#updating-and-renaming-columns",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#updating-and-renaming-columns",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Often, the data we have might not be in the condition want it to be in. Some data might be missing, and other data might have odd naming conventions.\nA simple example is that we might want all city names to be lowercase - which is what the code below does.\n\ndf['Name'] = df['Name'].str.lower()\ndf\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n0\nabbotsford\nB.C.\n153524.0\n141397.0\n\n\n1\nairdrie\nAlta.\n74100.0\n61581.0\n\n\n2\najax\nOnt.\n126666.0\n119677.0\n\n\n3\nalma\nQue.\n30331.0\n30771.0\n\n\n4\naurora\nOnt.\n62057.0\n55445.0\n\n\n...\n...\n...\n...\n...\n\n\n174\nwindsor\nOnt.\n229660.0\n217188.0\n\n\n175\nwinnipeg\nMan.\n749607.0\n705244.0\n\n\n176\nwood buffalo\nAlta.\n72326.0\n71594.0\n\n\n177\nwoodstock\nOnt.\n46705.0\n41098.0\n\n\n178\nwoolwich\nOnt.\n26999.0\n25006.0\n\n\n\n\n179 rows √ó 4 columns\n\n\n\npandas has a number of methods like str.lower() to alter data (see the full API). But the important thing to note here is that we directly modified the existing values of a column. We might not always want to do this, but often it is a good way of saving memory and shows that data frames are not just static forms but modifiable.\nLikewise, we might want better names for the columns we have. Take a look at the API for .rename() and modify the data frame df such that we rename the column Name to City. Pandas has more methods than we could ever remember - so learning to navigate the API is a crucial part of using the library.\nHINT: Take a look at the first example",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#handling-missing-data",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#handling-missing-data",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "Unfortunately, it is pretty common that dataset we work with will be missing data values for some rows and columns. This can create complications when we want to produce summary statistics or visualizations. There are different strategies for dealing with this (i.e., imputing data), but the easiest is just to remove them. But first come out let‚Äôs check how much data is missing.\n\ndf.isnull().sum()\n\nName                3\nProv/terr           4\nPopulation, 2021    3\nPopulation, 2016    4\ndtype: int64\n\n\nIt seems that each column has a couple of data points missing. Let‚Äôs take a look at which rows these occur in. Similar to how we created a condition to filter for certain data, the code below creates a condition to filter for rows with missing data.\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n7\nblainville\nQue.\n59819.0\nNaN\n\n\n18\ncaledon\nNaN\n76581.0\n66502.0\n\n\n30\nNaN\nOnt.\n101427.0\n92013.0\n\n\n40\ndrummondville\nQue.\nNaN\n75423.0\n\n\n51\ngrimsby\nOnt.\n28883.0\nNaN\n\n\n64\nla prairie\nNaN\n26406.0\n24110.0\n\n\n78\nNaN\nN.S.\n25545.0\n24863.0\n\n\n86\nmission\nNaN\n41519.0\n38554.0\n\n\n109\npeterborough\nOnt.\nNaN\nNaN\n\n\n131\nNaN\nQue.\n29954.0\n27359.0\n\n\n157\ntimmins\nNaN\n41145.0\n41788.0\n\n\n169\nwest kelowna\nB.C.\nNaN\nNaN\n\n\n\n\n\n\n\nYou can see that some rows are missing multiple values, While others are just missing one. We can remove rows which have missing data using the function dropna and assign it to df so we‚Äôre working with complete data only going forward. Try to modify the code below to drop rows whose empty values are in one of the two population columns - that is, if the name or province is missing, we want to keep that row still. Look at the API to figure this out, specifically the argument subset for .dropna()\n\ndf = df.dropna()\n\n\ndf.loc[df.isnull().any(axis=1)]\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n\n\n\n\n\nGreat. Now let‚Äôs reset to our original data frame and exclude any rows with missing values.\n\ndf = pd.read_csv(\"data/cities.csv\")\ndf = df.dropna()\n\nInstead of dropping (i.e.¬†removing) rows with missing values, we can instead replace them with specific values with fillna. For example, df.fillna(0) would replace all missing data values with a 0. This wouldn‚Äôt make sense in our data of Canadian cities, but can be useful in other contexts.\nSimilarly we can promgramatically find and replace data in any other column or across our dataset. For example, if we wanted to rename 'Y.T.' to 'Yukon' we would run the replace function as so df = df.replace('Y.T.', 'Yukon')\n\n\nWe can add or delete columns as needed. Let‚Äôs first add a column which shows the change in population between 2021 and 2016 and then sort by the cities that lost the most people. We can calculate that via a simple subtraction as follows.\n\ndf[\"pop_change\"] = df[\"Population, 2021\"] - df[\"Population, 2016\"]\ndf.sort_values(\"pop_change\", ascending = False).head(5)\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\npop_change\n\n\n\n\n106\nOttawa\nOnt.\n1017449.0\n934243.0\n83206.0\n\n\n42\nEdmonton\nAlta.\n1010899.0\n933088.0\n77811.0\n\n\n19\nCalgary\nAlta.\n1306784.0\n1239220.0\n67564.0\n\n\n11\nBrampton\nOnt.\n656480.0\n593638.0\n62842.0\n\n\n158\nToronto\nOnt.\n2794356.0\n2731571.0\n62785.0\n\n\n\n\n\n\n\nPandas supports mathematical equations between columns, just like the subtraction we did above. Create a new column called pct_pop_change that computes the percentage change in population between 2016 and 2021, and sort by the cities with the greatest increase.\nHINT: the way to compute percent change is 100 * (Y - X) / X.\nNow let‚Äôs clear these new columns out using drop to return to what we had originally.\n\ndf = df.drop(columns=['pop_change', 'pct_pop_change'])\n\n\n\n\nMuch of the time, we are working with multiple sets of data which may overlap in key ways. Perhaps we want to include measures of income in cities, or look at voting patterns - this may require us to combine multiple data frames so we can analyze them.\nPandas methods to combine data frames are quite similar to that of database operations - if you‚Äôve worked with SQL before, this will come very easily to you. There‚Äôs an extensive tutorial on this topic, but we‚Äôll focus on simple cases of pd.concat() and pd.merge(). If you are curious about how to do this in spreadsheet software like Excel, check out this tutorial\nFirst, we can use pd.concat() to stack DataFrames vertically when new data has the same columns but additional rows (e.g., adding cities from another region). This is like adding new entries to a database table.\n\ndf_ny_cities = pd.read_csv(\"./data/new_york_cities.csv\")\ncombined = pd.concat([df, df_ny_cities], ignore_index=True)\nprint(\"Original rows:\", len(df), \"| After append:\", len(combined))\ndisplay(combined.tail(5))  # Show new rows\n\nOriginal rows: 167 | After append: 169\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\n\n\n\n\n164\nWood Buffalo\nAlta.\n72326.0\n71594.0\n\n\n165\nWoodstock\nOnt.\n46705.0\n41098.0\n\n\n166\nWoolwich\nOnt.\n26999.0\n25006.0\n\n\n167\nNew York City\nN.Y.\n8804190.0\n8537673.0\n\n\n168\nBuffalo\nN.Y.\n278349.0\n258071.0\n\n\n\n\n\n\n\nSecond, we can use pd_merge() (or pd.concat(axis=1)) to combine DataFrames side-by-side when they share a key column (e.g., city names). Here, we‚Äôll a column denoting whether the city is a provincial capital by matching city names.\nLet‚Äôs first load this data:\n\ndf_capitals = pd.read_csv('./data/capitals.csv')\ndf_capitals\n\n\n\n\n\n\n\n\nName\nIs_Capital\n\n\n\n\n0\nToronto\nTrue\n\n\n1\nQu√©bec\nTrue\n\n\n2\nVictoria\nTrue\n\n\n3\nEdmonton\nTrue\n\n\n4\nWinnipeg\nTrue\n\n\n5\nFredericton\nTrue\n\n\n6\nHalifax\nTrue\n\n\n7\nCharlottetown\nTrue\n\n\n8\nSt. John's\nTrue\n\n\n9\nRegina\nTrue\n\n\n10\nWhitehorse\nTrue\n\n\n\n\n\n\n\n\ndf_with_capitals = pd.merge(\n    df,  # Original data\n    df_capitals,  # New data\n    on=\"Name\",  # The same column \n    how=\"left\"  # Keep all data from the \"left\", ie. original\n)\n\n# Set non-capitals to False\ndf_with_capitals[\"Is_Capital\"] = df_with_capitals[\"Is_Capital\"].astype('boolean').fillna(False)\n\n# Verify: Show capitals and counts\nprint(f\"Found {df_with_capitals['Is_Capital'].sum()} capitals:\")\ndf_with_capitals[df_with_capitals[\"Is_Capital\"]]\n\nFound 11 capitals:\n\n\n\n\n\n\n\n\n\nName\nProv/terr\nPopulation, 2021\nPopulation, 2016\nIs_Capital\n\n\n\n\n23\nCharlottetown\nP.E.I.\n38809.0\n36094.0\nTrue\n\n\n38\nEdmonton\nAlta.\n1010899.0\n933088.0\nTrue\n\n\n41\nFredericton\nN.B.\n63116.0\n58721.0\nTrue\n\n\n49\nHalifax\nN.S.\n439819.0\n403131.0\nTrue\n\n\n108\nQu√©bec\nQue.\n549459.0\n531902.0\nTrue\n\n\n111\nRegina\nSask.\n226404.0\n215106.0\nTrue\n\n\n139\nSt. John's\nN.L.\n110525.0\n108860.0\nTrue\n\n\n147\nToronto\nOnt.\n2794356.0\n2731571.0\nTrue\n\n\n154\nVictoria\nB.C.\n91867.0\n85792.0\nTrue\n\n\n161\nWhitehorse\nY.T.\n28201.0\n25085.0\nTrue\n\n\n163\nWinnipeg\nMan.\n749607.0\n705244.0\nTrue",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#summary-statistics",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#summary-statistics",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "The data we have is only as good as we understand what‚Äôs going on. There‚Äôs some basic methods in pandas we can use to get an idea of what the data says.\nThe most basic function you can use to get an idea of what‚Äôs going on is .describe(). It shows you how much data there is, and a number of summary statistics.\nModify the code below to report summary statistics for cities in Quebec only.\n\ndf.describe()\n\n\n\n\n\n\n\n\nPopulation, 2021\nPopulation, 2016\n\n\n\n\ncount\n1.670000e+02\n1.670000e+02\n\n\nmean\n1.573169e+05\n1.487358e+05\n\n\nstd\n3.074452e+05\n2.959960e+05\n\n\nmin\n2.570400e+04\n2.378700e+04\n\n\n25%\n3.770050e+04\n3.449850e+04\n\n\n50%\n6.414100e+04\n6.316600e+04\n\n\n75%\n1.348910e+05\n1.255940e+05\n\n\nmax\n2.794356e+06\n2.731571e+06\n\n\n\n\n\n\n\nInstead of picking out an examining a subset of the data one by one, we can use the function .groupby(). Given a column name, it will group rows which have the same value. In the example below, that means grouping every row which has the same province name. We can then apply a function to this (or multiple functions using .agg()) to examine different aspects of the data.\n\n# Group by province and calculate total population\nprovince_pop = df.groupby('Prov/terr')['Population, 2021'].sum()\nprint(\"Total population by province:\")\nprovince_pop.sort_values(ascending=False)\n\nTotal population by province:\n\n\nProv/terr\nOnt.      11598308.0\nQue.       5474109.0\nB.C.       3662922.0\nAlta.      3192892.0\nMan.        800920.0\nSask.       563966.0\nN.S.        533513.0\nN.B.        240595.0\nN.L.        137693.0\nP.E.I.       38809.0\nY.T.         28201.0\nName: Population, 2021, dtype: float64\n\n\n\n# Multiple aggregation statistics\nstats = df.groupby('Prov/terr')['Population, 2021'].agg(['count', 'mean', 'max', 'sum'])\nstats\n\n\n\n\n\n\n\n\ncount\nmean\nmax\nsum\n\n\nProv/terr\n\n\n\n\n\n\n\n\nAlta.\n17\n187817.176471\n1306784.0\n3192892.0\n\n\nB.C.\n29\n126307.655172\n662248.0\n3662922.0\n\n\nMan.\n2\n400460.000000\n749607.0\n800920.0\n\n\nN.B.\n4\n60148.750000\n79470.0\n240595.0\n\n\nN.L.\n2\n68846.500000\n110525.0\n137693.0\n\n\nN.S.\n2\n266756.500000\n439819.0\n533513.0\n\n\nOnt.\n64\n181223.562500\n2794356.0\n11598308.0\n\n\nP.E.I.\n1\n38809.000000\n38809.0\n38809.0\n\n\nQue.\n41\n133514.853659\n1762949.0\n5474109.0\n\n\nSask.\n4\n140991.500000\n266141.0\n563966.0\n\n\nY.T.\n1\n28201.000000\n28201.0\n28201.0\n\n\n\n\n\n\n\nBelow, we‚Äôve added a column which shows the percent growth for each city. Use .groupby('Prov/terr') and use the function .median() (just as we used .sum() above) to observe the median growth rate per province or territory. Make sure to use sort_values() and set ascending to False.\n\ndf['Percent_Growth'] = 100 * (df['Population, 2021'] - df['Population, 2016']) / df['Population, 2016']",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#cross-tabulations-and-pivot-tables",
    "href": "notebooks/urban-data-analytics/data-analytics-and-processing/data-analytics-and-processing.html#cross-tabulations-and-pivot-tables",
    "title": "Processing and analyzing data",
    "section": "",
    "text": "A cross tabulation, also called a frequency table or a contingency table, is a table that shows the summarizes two categorical variables by displaying the number of occurrences in each pair of categories.\nLet‚Äôs show an example by counting the number of cities in each province by a categorization of city size.\nWe will need two tools to do this: - cut, which bins continuous numbers (like population) into categories (e.g., ‚ÄúSmall‚Äù/‚ÄúMedium‚Äù/‚ÄúLarge‚Äù), turning numbers into meaningful groups. - crosstab, which counts how often these groups appear together‚Äîlike how many ‚ÄúMedium‚Äù cities exist per province‚Äîrevealing patterns that raw numbers hide.\n\n# Create a size category column\ndf['Size'] = pd.cut(df['Population, 2021'],\n                    bins=[0, 50000, 200000, float('inf')],\n                    labels=['Small', 'Medium', 'Large'])\n\n# Cross-tab: Province vs. Size\nsize_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\nsize_table\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # Create a size category column\n----&gt; 2 df['Size'] = pd.cut(df['Population, 2021'],\n      3                     bins=[0, 50000, 200000, float('inf')],\n      4                     labels=['Small', 'Medium', 'Large'])\n      6 # Cross-tab: Province vs. Size\n      7 size_table = pd.crosstab(df['Prov/terr'], df['Size'], margins=True)\n\nNameError: name 'pd' is not defined\n\n\n\nRecall that we just created the column 'Percent_Growth' as well. Use these two functions to create different bins for different levels of growth and cross tabulate them.\nIf you‚Äôve worked with Excel or Google Sheets, this is very similar to doing a Pivot Table.\nPivot tables are able to summarize data across several categories, and for different types of summaries (e.g.¬†sum, mean, median, etc.)\nThe pivot_table function in pandas to aggregate data, and has more options than the crosstab function. Both are quite similar though. Here‚Äôs an example where we are using pivot_table to sum the population in each province by the 3 size groups.\nTry to edit this to compute the mean or median city Percent_Growth\n\npd.pivot_table(data = df, values = ['Population, 2021'], index = ['Prov/terr'], columns = ['Size'], aggfunc = 'sum', observed=True)\n\n\n\n\n\n\n\n\nPopulation, 2021\n\n\nSize\nSmall\nMedium\nLarge\n\n\nProv/terr\n\n\n\n\n\n\n\nAlta.\n234664.0\n640545.0\n2317683.0\n\n\nB.C.\n330537.0\n1642753.0\n1689632.0\n\n\nMan.\nNaN\n51313.0\n749607.0\n\n\nN.B.\n28114.0\n212481.0\nNaN\n\n\nN.L.\n27168.0\n110525.0\nNaN\n\n\nN.S.\nNaN\n93694.0\n439819.0\n\n\nOnt.\n930812.0\n2925641.0\n7741855.0\n\n\nP.E.I.\n38809.0\nNaN\nNaN\n\n\nQue.\n806267.0\n1371544.0\n3296298.0\n\n\nSask.\n71421.0\nNaN\n492545.0\n\n\nY.T.\n28201.0\nNaN\nNaN\n\n\n\n\n\n\n\nThere are often multiple ways to achieve similar results. In the previous section we looked at the groupby function to summarize data by one column, while above we used crosstab or pivot_table. However, we could also use the groupby function for this purpose. Check out the example below.\nYou should notice that it has created a long-table formate rather than a wide-table format. Both formats can be useful, wide-table formats are easier for viewing data for only 2 categories, while long-table formats are often used for inputting data into modelling or visualization libraries.\n\ndf.groupby(['Prov/terr', 'Size'], observed=False)['Population, 2021'].agg(['sum'])\n\n\n\n\n\n\n\n\n\nsum\n\n\nProv/terr\nSize\n\n\n\n\n\nAlta.\nSmall\n234664.0\n\n\nMedium\n640545.0\n\n\nLarge\n2317683.0\n\n\nB.C.\nSmall\n330537.0\n\n\nMedium\n1642753.0\n\n\nLarge\n1689632.0\n\n\nMan.\nSmall\n0.0\n\n\nMedium\n51313.0\n\n\nLarge\n749607.0\n\n\nN.B.\nSmall\n28114.0\n\n\nMedium\n212481.0\n\n\nLarge\n0.0\n\n\nN.L.\nSmall\n27168.0\n\n\nMedium\n110525.0\n\n\nLarge\n0.0\n\n\nN.S.\nSmall\n0.0\n\n\nMedium\n93694.0\n\n\nLarge\n439819.0\n\n\nOnt.\nSmall\n930812.0\n\n\nMedium\n2925641.0\n\n\nLarge\n7741855.0\n\n\nP.E.I.\nSmall\n38809.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0\n\n\nQue.\nSmall\n806267.0\n\n\nMedium\n1371544.0\n\n\nLarge\n3296298.0\n\n\nSask.\nSmall\n71421.0\n\n\nMedium\n0.0\n\n\nLarge\n492545.0\n\n\nY.T.\nSmall\n28201.0\n\n\nMedium\n0.0\n\n\nLarge\n0.0",
    "crumbs": [
      "Urban Data Analytics",
      "Processing and analyzing data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html",
    "title": "Spatial data in Python",
    "section": "",
    "text": "There are a variety of libraries, methods, and functions for working with spatial data in Python.\nGeoPandas is a library for working with spatial (typically geographic) data. It is used a lot since it extends the functionality of pandas to support spatial data types and operations, making it easier to analyze, visualize, and manipulate spatial data.\nMany of the tasks that are typically done within a GIS can be done via geopandas. And often, it is preferable to work in geopandas versus point-and-click GIS (e.g.¬†QGIS) since it‚Äôs easier to chain together and automate processes, have access to the full capabilities of pandas and other Python libraries, as well as can be better scaled for handling very large datasets.\nThis tutorial provides and introduction to geopandas, its gemometry data types, how to quickly plot spatial data, and methods for spatial calculations (e.g.¬†computing areas, bounding boxes, etc.). The last section of this tutorial provides direction for next steps, including libraries and examples for doing advanced spatial data processing, analyses, and visualizations.\nHere are the links to download this notebook and example data: - Notebook - Toronto transit routes - Toronto transit stops - Toronto transit stops - Toronto wards\nThis tutorial assumes you have base knowledge of working with spatial data (e.g.¬†in GIS) and working with dataframes in pandas. Check out the following if you want a refresher: - Spatial data in GIS - Data analytics and processing\nWe‚Äôll begin this tutorial by loading in pandas, geopandas, and matplotlib (the latter for showing how to quickly plot and view data). If you don‚Äôt have these installed already, you‚Äôll have to install them via pip or conda.\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#loading-and-exploring-geometric-data",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#loading-and-exploring-geometric-data",
    "title": "Spatial data in Python",
    "section": "Loading and exploring geometric data",
    "text": "Loading and exploring geometric data\nGeospatial data represents real-world features using three primary geometric types: - Points: Single (x,y) coordinates for discrete locations like transit stops or landmarks. - Lines: Connected sequences of points forming paths, such as roads or rivers. - Polygons: Closed shapes defining areas like census tracts or property boundaries.\n\ntransit_stops = gpd.read_file(\"data/ttc_stops.geojson\")\ntransit_routes = gpd.read_file(\"data/ttc_routes.geojson\")\nwards = gpd.read_file(\"data/city-wards.geojson\")\n\nIn geopandas, we typically load data with a more agnostic read_file() function. For this workshop, we‚Äôre going to use four sources of data: - transit_stops: each of the stops for the TTC - transit_routes: each of the lines for the TTC - wards: polygons of City of Toronto wards (i.e.¬†council districts)\nLet‚Äôs take a look at the first layer, the transit stops. We have two columns with text, and a third with geometry data.\nHere the data are coded as a MULTILINESTRING, essentially a combination of lines that combine into one object.\n\ntransit_routes\n\n\n\n\n\n\n\n\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nExisting\nSubway\nLine 4: Sheppard Subway\nMULTILINESTRING ((-79.41092 43.76152, -79.4096...\n\n\n1\nExisting\nSubway\nLine 1: Yonge-University Subway\nMULTILINESTRING ((-79.52727 43.79351, -79.5261...\n\n\n2\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nMULTILINESTRING ((-79.26453 43.73227, -79.2669...\n\n\n3\nIn Delivery\nLRT / BRT\nEglinton Crosstown LRT\nMULTILINESTRING ((-79.26453 43.73227, -79.2679...\n\n\n4\nIn Delivery\nLRT / BRT\nEglinton Crosstown West Extension\nMULTILINESTRING ((-79.48726 43.68739, -79.4901...\n\n\n5\nIn Delivery\nSubway\nScarborough Subway Extension\nMULTILINESTRING ((-79.26453 43.73227, -79.2630...\n\n\n6\nIn Delivery\nLRT / BRT\nFinch West LRT\nMULTILINESTRING ((-79.49099 43.76349, -79.4922...\n\n\n7\nIn Delivery\nSubway\nOntario Line\nMULTILINESTRING ((-79.35168 43.69644, -79.3414...\n\n\n8\nIn Delivery\nSubway\nYonge North Subway Extension\nMULTILINESTRING ((-79.41557 43.77977, -79.4157...\n\n\n\n\n\n\n\nThe data can be manipulated like a regular pandas data frame. For example, if we want to filter out routes that currently do not operate, like the incomplete ‚ÄúEglinton Crosstown LRT‚Äù and defunct ‚ÄúScarborough RT‚Äù, we can do so by filtering the STATUS column.\n\ntransit_routes = transit_routes.loc[transit_routes[\"STATUS\"] == \"Existing\"]\ntransit_routes\n\n\n\n\n\n\n\n\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nExisting\nSubway\nLine 4: Sheppard Subway\nMULTILINESTRING ((-79.41092 43.76152, -79.4096...\n\n\n1\nExisting\nSubway\nLine 1: Yonge-University Subway\nMULTILINESTRING ((-79.52727 43.79351, -79.5261...\n\n\n2\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nMULTILINESTRING ((-79.26453 43.73227, -79.2669...\n\n\n\n\n\n\n\nLet‚Äôs do the same for the stops.\nThe geometry for the stops are simpler, just POINT.\n\ntransit_stops = transit_stops.loc[transit_stops[\"STATUS\"] == \"Existing\"]\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nKipling\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.53628 43.63694)\n\n\n1\nIslington\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.5246 43.64533)\n\n\n2\nRoyal York\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.51129 43.64812)\n\n\n3\nOld Mill\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.49509 43.65008)\n\n\n4\nJane\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.48446 43.6498)\n\n\n...\n...\n...\n...\n...\n...\n\n\n69\nVaughan Metropolitan Centre\nExisting\nSubway\nLine 1: Yonge-University Subway\nPOINT (-79.52727 43.79351)\n\n\n70\nSheppard-Yonge\nExisting\nSubway\nLine 4: Sheppard Subway\nPOINT (-79.41092 43.76152)\n\n\n71\nSpadina\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.40397 43.66729)\n\n\n72\nSt. George\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.39931 43.66828)\n\n\n73\nBloor-Yonge\nExisting\nSubway\nLine 2: Bloor-Danforth Subway\nPOINT (-79.38572 43.671)\n\n\n\n\n74 rows √ó 5 columns\n\n\n\nBefore we go on, it‚Äôs important to have an idea of the metadata of geometric files that we work with. There‚Äôs two key parts to this. - CRS (Coordinate Reference System): The crs attribute defines a geodataset‚Äôs spatial ‚Äúcoordinate system‚Äù (e.g., latitude/longitude, meters-based projections). We can use it to ensure layers align‚Äîfor example, combining Toronto census tracts (EPSG:3347) with a Web Mercator basemap (EPSG:3857). - Total Bounds: The total_bounds attribute returns the min/max coordinates (xmin, ymin, xmax, ymax) of your data‚Äôs extent. It‚Äôs useful for setting map zoom levels or clipping other datasets to the same area‚Äîlike focusing a transit map on Toronto‚Äôs downtown core.\n\ntransit_stops.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ntransit_stops.total_bounds\n\narray([-79.53628,  43.63694, -79.26453,  43.79351])",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#plotting-geographic-data",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#plotting-geographic-data",
    "title": "Spatial data in Python",
    "section": "Plotting geographic data",
    "text": "Plotting geographic data\nWe explore geometry simply by plotting using .plot(). We can do this for any row, or the entire GeoDataFrame\n\ntransit_stops.plot()\n\n\n\n\n\n\n\n\nThis is the default plot, but we can tweak the colours, add multiple layers, and change some of the layout options using matplotlib, a commonly used Python plotting library. Here‚Äôs a very simple schematic of rapid transit in Toronto.\n\nfig, ax = plt.subplots(ncols = 1, figsize=(4, 4))\n\nwards.plot(\n    linewidth = 1,\n    color=\"LightGray\",\n    edgecolor=\"White\",\n    ax = ax\n)\n\ntransit_stops.plot(\n    color=\"Black\",\n    markersize = 6,\n    ax = ax\n)\n\ntransit_routes.plot(\n    linewidth = 1,\n    color=\"Black\",\n    ax = ax\n).set_axis_off()",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#interactive-exploration",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#interactive-exploration",
    "title": "Spatial data in Python",
    "section": "Interactive Exploration",
    "text": "Interactive Exploration\nGeoPandas‚Äô explore() function generates an interactive Leaflet map (like Google Maps) from your geodata. We can use it to better understand the data we are working with and how it might be viewed from the user side on a web application (e.g., Svelte).\nYou‚Äôll need to install a couple libraries in order for this to work - matplotlib, folium, and mapclassify. This can be done in the environment that you‚Äôre working in with a command like pip install folium matplotlib mapclassify.\n\ntransit_stops.explore(\n    column='NAME',\n    tiles=\"CartoDB Positron\", \n    marker_kwds={\"radius\": 7}\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#geomtery-properties",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#geomtery-properties",
    "title": "Spatial data in Python",
    "section": "Geomtery properties",
    "text": "Geomtery properties\ngeopandas has a number of functions you can run on geometry columns that can be super useful for analyzing and summarizing data. Here‚Äôs a list of a few examples of using these to create a new column of data (more can be found in the official documentation)\n\n\n\n\n\n\n\n\nProperty\nDescription\nExample\n\n\n\n\n.area\nArea of the geometry (in CRS units)\ngdf[\"area\"] = gdf.geometry.area\n\n\n.length\nLine length (perimeter length for polygons)\ngdf[\"length\"] = gdf.geometry.length\n\n\n.is_valid\nGeometry validity check\ngdf[\"valid\"] = gdf.geometry.is_valid\n\n\n.is_empty\nWhether geometry is empty\ngdf[\"empty\"] = gdf.geometry.is_empty\n\n\n.is_simple\nNo self-intersections\ngdf[\"simple\"] = gdf.geometry.is_simple\n\n\n.type\nType of geometry (e.g., Polygon)\ngdf[\"geom_type\"] = gdf.geometry.type\n\n\n.bounds\nBounding box as (minx, miny, maxx, maxy)\ngdf[\"bounds\"] = gdf.geometry.bounds",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#processing-and-analyzing-spatial-data",
    "href": "notebooks/urban-data-analytics/spatial-data-in-python/spatial-data-in-python.html#processing-and-analyzing-spatial-data",
    "title": "Spatial data in Python",
    "section": "Processing and analyzing spatial data",
    "text": "Processing and analyzing spatial data\nThis tutorial provided quick overview of how we can working with spatial data in geopandas. However, this was just scratching the surface.\nIf you‚Äôre interested the why and how of spatial data processing, i.e.¬†about covnerting spatial data from one format to another (e.g.¬†generating centroids of polygons, buffers around points, joining and linking multiple spatial datasets to each other, etc.), check out our Processing spatial data tutorial.\nBeyond that, you can check out the following Python libraries for working with spatial data:\n\nShapely (wide variety of tools for manipulation of 2d geometry data, some of which geopandas uses.)\nPysal (spatial data analysis library, including various descriptive statistics and spatial modelling functions)\nRasterio (library for working with raster data)\nFolium (more interactive maps!)",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data in Python"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/choropleth-maps/spatial-data-in-python-v1.html",
    "href": "notebooks/urban-data-visualization/choropleth-maps/spatial-data-in-python-v1.html",
    "title": "Spatial data in Python",
    "section": "",
    "text": "GeoPandas is a library for working with spatial (typically geographic) data. It extends the functionality of pandas to support spatial data types and operations, making it easier to analyze, visualize, and manipulate spatial data.\nMany of the tasks that are typically done within a GIS can be done via geopandas. And often, it is preferable to work in geopandas versus point-and-click GIS (e.g.¬†QGIS) since it‚Äôs easier to chain together and automate processes, have access to the full capabilities of pandas and other Python libraries, as well as can be better scaled for handling very large datasets.\nWe‚Äôll begin this tutorial by loading in pandas, geopandas, and matplotlib (the latter for showing how to quickly plot and view data). If you don‚Äôt have these installed already, you‚Äôll have to install them via pip or conda.\nThis tutorial assumes you have base knowledge of working with spatial data (e.g.¬†in GIS) and working with dataframes in pandas. Check out the following if you want a refresher: - Spatial data in GIS - Data analytics and processing\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/urban-data-visualization/choropleth-maps/spatial-data-in-python-v1.html#loading-and-exploring-geometric-data",
    "href": "notebooks/urban-data-visualization/choropleth-maps/spatial-data-in-python-v1.html#loading-and-exploring-geometric-data",
    "title": "Spatial data in Python",
    "section": "Loading and exploring geometric data",
    "text": "Loading and exploring geometric data\nGeospatial data represents real-world features using three primary geometric types: - Points: Single (x,y) coordinates for discrete locations like transit stops or landmarks. - Lines: Connected sequences of points forming paths, such as roads or rivers. - Polygons: Closed shapes defining areas like census tracts or property boundaries.\n\ntransit_stops = gpd.read_file(\"data/ttc_stops.geojson\")\ntransit_routes = gpd.read_file(\"data/ttc_routes.geojson\")\ncensus_tracts = gpd.read_file(\"data/toronto_census_tract_2021.shp\")\ncensus_data = pd.read_csv(\"data/census_tract_data_sample.csv\")\n\nIn geopandas, we typically load data with a more agnostic read_file() function. For this workshop, we‚Äôre going to use four sources of data: - transit_stops: each of the stops for the TTC - transit_routes: each of the lines for the TTC - census_tracts: The geometry of Canadian census tracts within Toronto - census_data: the data of Canadian census tracts within Toronto\nLet‚Äôs take a look at the first layer, the transit stops. We have two columns with text, and a third with geometry data.\n\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nNAME\ngeometry\n\n\n\n\n0\nAvenue\nEglinton Crosstown LRT\nPOINT (-79.40851 43.7046)\n\n\n1\nForest Hill\nEglinton Crosstown LRT\nPOINT (-79.42556 43.70102)\n\n\n2\nLeaside\nEglinton Crosstown LRT\nPOINT (-79.37715 43.71105)\n\n\n3\nSloane\nEglinton Crosstown LRT\nPOINT (-79.31352 43.72597)\n\n\n4\nBirchmount\nEglinton Crosstown LRT\nPOINT (-79.27791 43.73006)\n\n\n...\n...\n...\n...\n\n\n101\nVaughan Metropolitan Centre\nToronto-York Spadina Subway Extension\nPOINT (-79.52727 43.79351)\n\n\n102\nSheppard-Yonge\nSheppard Subway\nPOINT (-79.41092 43.76151)\n\n\n103\nSpadina\nBloor-Danforth Subway\nPOINT (-79.40397 43.66728)\n\n\n104\nSt. George\nBloor-Danforth Subway\nPOINT (-79.3993 43.66827)\n\n\n105\nBloor-Yonge\nBloor-Danforth Subway\nPOINT (-79.38572 43.671)\n\n\n\n\n106 rows √ó 3 columns\n\n\n\nThe data can be manipulated like a regular pandas data frame. For example, if we want to filter out all stations on the ‚ÄúEglinton Crosstown LRT‚Äù line, since it hasn‚Äôt been completed yet (at the time of writing), we can do so as follows:\n\ntransit_stops = transit_stops.loc[transit_stops[\"NAME\"] != \"Eglinton Crosstown LRT\"]\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nNAME\ngeometry\n\n\n\n\n25\nKipling\nBloor-Danforth Subway\nPOINT (-79.53628 43.63694)\n\n\n26\nIslington\nBloor-Danforth Subway\nPOINT (-79.52459 43.64532)\n\n\n27\nRoyal York\nBloor-Danforth Subway\nPOINT (-79.51129 43.64811)\n\n\n28\nOld Mill\nBloor-Danforth Subway\nPOINT (-79.49509 43.65007)\n\n\n29\nJane\nBloor-Danforth Subway\nPOINT (-79.48446 43.64979)\n\n\n...\n...\n...\n...\n\n\n101\nVaughan Metropolitan Centre\nToronto-York Spadina Subway Extension\nPOINT (-79.52727 43.79351)\n\n\n102\nSheppard-Yonge\nSheppard Subway\nPOINT (-79.41092 43.76151)\n\n\n103\nSpadina\nBloor-Danforth Subway\nPOINT (-79.40397 43.66728)\n\n\n104\nSt. George\nBloor-Danforth Subway\nPOINT (-79.3993 43.66827)\n\n\n105\nBloor-Yonge\nBloor-Danforth Subway\nPOINT (-79.38572 43.671)\n\n\n\n\n81 rows √ó 3 columns\n\n\n\nWe can do the same for the transit lines data. Here the data are coded as a MULTILINESTRING, essentially a combination of lines that combine into one object. There are also MULTIPOLYGON geometry types\n\ntransit_routes = transit_routes.loc[transit_routes[\"NAME\"] != \"Eglinton Crosstown LRT\"]\ntransit_routes\n\n\n\n\n\n\n\n\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nExisting\nSubway\nSheppard Subway\nMULTILINESTRING ((-79.41092 43.76151, -79.4096...\n\n\n1\nExisting\nSubway\nYonge-University-Spadina Subway\nMULTILINESTRING ((-79.46247 43.75043, -79.4621...\n\n\n2\nExisting\nSubway\nSpadina Subway Extension\nMULTILINESTRING ((-79.52727 43.79351, -79.5261...\n\n\n4\nExisting\nSubway\nScarborough RT\nMULTILINESTRING ((-79.26453 43.73226, -79.2632...\n\n\n5\nExisting\nSubway\nBloor Subway\nMULTILINESTRING ((-79.26453 43.73226, -79.2669...\n\n\n\n\n\n\n\nBefore we go on, it‚Äôs important to have an idea of the metadata of geometric files that we work with. There‚Äôs two key parts to this. - CRS (Coordinate Reference System): The crs attribute defines a geodataset‚Äôs spatial ‚Äúcoordinate system‚Äù (e.g., latitude/longitude, meters-based projections). We can use it to ensure layers align‚Äîfor example, combining Toronto census tracts (EPSG:3347) with a Web Mercator basemap (EPSG:3857). - Total Bounds: The total_bounds attribute returns the min/max coordinates (xmin, ymin, xmax, ymax) of your data‚Äôs extent. It‚Äôs useful for setting map zoom levels or clipping other datasets to the same area‚Äîlike focusing a transit map on Toronto‚Äôs downtown core.\n\ntransit_stops.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ntransit_stops.total_bounds\n\narray([-79.53627668,  43.63693527, -79.25160004,  43.79350523])"
  },
  {
    "objectID": "notebooks/urban-data-visualization/choropleth-maps/spatial-data-in-python-v1.html#plotting-geographic-data",
    "href": "notebooks/urban-data-visualization/choropleth-maps/spatial-data-in-python-v1.html#plotting-geographic-data",
    "title": "Spatial data in Python",
    "section": "Plotting geographic data",
    "text": "Plotting geographic data\nWe explore geometry simply by plotting using .plot(). We can do this for any row, or the entire GeoDataFrame\n\ntransit_stops.plot()\n\n\n\n\n\n\n\n\nThis is the default plot, but we can tweak the colours, add multiple layers, and change some of the layout options using matplotlib, probably the most commonly used map. Here‚Äôs a very simple schematic of rapid transit in Toronto (circa 2021)\n\nfig, ax = plt.subplots(ncols = 1, figsize=(3, 3))\n\ntransit_stops.plot(\n    color=\"Black\",\n    markersize = 6,\n    ax = ax\n)\n\ntransit_routes.plot(\n    linewidth = 1,\n    color=\"Black\",\n    ax = ax\n).set_axis_off()"
  },
  {
    "objectID": "notebooks/urban-data-visualization/choropleth-maps/spatial-data-in-python-v1.html#interactive-exploration",
    "href": "notebooks/urban-data-visualization/choropleth-maps/spatial-data-in-python-v1.html#interactive-exploration",
    "title": "Spatial data in Python",
    "section": "Interactive Exploration",
    "text": "Interactive Exploration\nGeoPandas‚Äô explore() function generates an interactive Leaflet map (like Google Maps) from your geodata. We can use it to better understand the data we are working with and how it might be viewed from the user side on a web application (e.g., Svelte).\nYou‚Äôll need to install a couple libraries in order for this to work - matplotlib, folium, and mapclassify. This can be done in the environment that you‚Äôre working in with a command like pip install folium matplotlib mapclassify.\n\ntransit_stops.explore(\n    column='NAME',  # Popup labels\n    tiles=\"CartoDB Positron\", \n    marker_kwds={\"radius\": 7}\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\ncensus_tracts\n\n\n\n\n\n\n\n\nid\nctuid\ndguid\nctname\nlandarea\npruid\ngeometry\n\n\n\n\n0\n487\n5350128.04\n2021S05075350128.04\n0128.04\n0.1620\n35\nPOLYGON ((629437.750 4839364.950, 629247.561 4...\n\n\n1\n502\n5350363.06\n2021S05075350363.06\n0363.06\n0.8210\n35\nPOLYGON ((640741.738 4848050.419, 640723.345 4...\n\n\n2\n506\n5350363.07\n2021S05075350363.07\n0363.07\n2.2422\n35\nPOLYGON ((642782.718 4849973.938, 642781.180 4...\n\n\n3\n508\n5350378.23\n2021S05075350378.23\n0378.23\n1.5314\n35\nPOLYGON ((639248.900 4849901.332, 639248.900 4...\n\n\n4\n509\n5350378.24\n2021S05075350378.24\n0378.24\n2.5129\n35\nPOLYGON ((639952.255 4850407.204, 639952.255 4...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n580\n5861\n5350210.04\n2021S05075350210.04\n0210.04\n0.4751\n35\nMULTIPOLYGON (((623047.314 4831182.748, 623047...\n\n\n581\n5862\n5350062.03\n2021S05075350062.03\n0062.03\n0.4638\n35\nPOLYGON ((629776.795 4835352.843, 629766.377 4...\n\n\n582\n5863\n5350062.04\n2021S05075350062.04\n0062.04\n0.1215\n35\nPOLYGON ((630319.668 4835517.832, 630149.660 4...\n\n\n583\n5864\n5350017.01\n2021S05075350017.01\n0017.01\n0.8026\n35\nPOLYGON ((633075.947 4834744.346, 633089.159 4...\n\n\n584\n5865\n5350017.02\n2021S05075350017.02\n0017.02\n0.5681\n35\nPOLYGON ((631852.697 4833570.180, 631843.913 4...\n\n\n\n\n585 rows √ó 7 columns\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\ncensus_tracts.plot(\n    edgecolor=\"Black\",\n    color=\"White\",\n    linewidth=0.5,\n    ax = ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nLet‚Äôs try to make a choropleth map. We‚Äôll have to join in the tabular data in census_data\n\ncensus_tracts_data = census_tracts.merge(census_data, how='left', on='ctuid')\n\n\ncensus_tracts_data[\"population density\"] = census_tracts_data[\"population_2021\"] / census_tracts_data[\"landarea\"]\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\ncensus_tracts_data.to_crs(4326).plot(\n    column = \"population density\",\n    edgecolor=\"White\",\n    cmap = 'YlOrRd', \n    k = 7,\n    scheme = \"Quantiles\", \n    linewidth=0.5,\n    legend = True,\n    zorder=1,\n    legend_kwds = {\n        \"loc\": \"lower right\",\n        \"fontsize\": 7,\n        \"title\": \"Population Density\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    },\n    ax=ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nWe can choose a different variable and make a map of median income. Notice that we‚Äôve also switched the color parameter cmap; just as in matplotlib generally, we can style the map as we please.\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\ncensus_tracts_data.plot(\n    column = \"median_aftertax_hhld_income_2020\",\n    edgecolor=\"Black\",\n    cmap = 'coolwarm_r', \n    k = 4,\n    scheme = \"Quantiles\", \n    linewidth=0.5,\n    legend = True,\n    legend_kwds = {\n        \"loc\": \"lower right\",\n        \"fontsize\": 7,\n        \"title\": \"Median Income\\n(after tax)\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    },\n    ax=ax\n).set_axis_off()"
  },
  {
    "objectID": "notebooks/urban-data-visualization/choropleth-maps/spatial-data-in-python-v1.html#layering-multiple-datasets",
    "href": "notebooks/urban-data-visualization/choropleth-maps/spatial-data-in-python-v1.html#layering-multiple-datasets",
    "title": "Spatial data in Python",
    "section": "Layering Multiple Datasets",
    "text": "Layering Multiple Datasets\nLayering lets you combine different geographic datasets (like roads on top of neighborhoods) to reveal spatial relationships. In GeoPandas, each plot() call adds a new visual layer, with zorder controlling which appears on top. These plots are highly customizable, as we see below.\n\n# Initialize the canvas (all layers will share this axis)\nfig, ax = plt.subplots(figsize=(7, 7))  \n\n# Layer 1: Transit routes (black lines on top, zorder=3)\ntransit_routes.to_crs(4326).plot(\n    linewidth=1,        \n    color=\"Black\",      \n    ax=ax,              # Draw on our shared axis\n    zorder=3            # Top layer (overlaps others)\n)\n\n# Layer 2: Transit stops (black dots below routes, zorder=2)\ntransit_stops.to_crs(4326).plot(\n    color=\"Black\",      \n    markersize=16,      \n    ax=ax,              # Same shared axis\n    zorder=2            # Middle layer (above tracts)\n)\n\n# Layer 3: Census tracts (colored by density, zorder=1)\ncensus_tracts_data.to_crs(4326).plot(\n    column=\"population density\",  \n    edgecolor=\"White\",            \n    cmap='YlOrRd',                \n    k=7,                          \n    scheme=\"Quantiles\",           \n    linewidth=0.5,                \n    legend=True,                  \n    zorder=1,                     # Bottom layer\n    legend_kwds={\n        \"loc\": \"lower right\",     \n        \"fontsize\": 7,            \n        \"title\": \"Population Density\",  \n        \"alignment\": \"left\",      \n        \"reverse\": True           \n    },\n    ax=ax\n).set_axis_off()  # Hide distracting x/y axes"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Home page",
    "section": "",
    "text": "Home page"
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Maps and spatial data visualizations can either created be at the research stage where we are trying to uncover patterns in the data, or as polished communication visuals where we want to convey patterns in the data to a our intended audience.\nCartography is often defined as the science and art of creating maps. It often involves decisions about ‚Ä¶\n\nselection of data to include on the map\ngeneralization of data to reduce complexity and clutter\nhow we want to style and symbolize our data\nhow to order and layer data\nwhat other map layout elements to include (e.g.¬†title, legend, north arrow), as well as how to design these to make a map easy to understand\n\nThis page will cover the basics of what goes into making maps and spatial data visualizations, following each of these steps.\nThe last part of this notebook shows an example in QGIS for making a simple map ‚Ä¶‚Ä¶‚Ä¶‚Ä¶. The\nIf you are interested in creating specific types of maps, check out the following notebooks:\n\nChoropleth maps\nProportional symbol maps\nMapping point density (dot maps, grid maps, heat maps)\n\n\n\n\n\n\nAlmost all maps can be thought of as either\n\nReference maps, which provide base geographic information (e.g.¬†streets, land forms, water, etc.) often for locating places or for navigation. Google Maps is probably the most well-used example.\nThematic maps, also called geographic or spatial data visualizations, emphasize patterns or trends tied to a specific theme (e.g., population density, election results, air pollution, etc.).\n\nReference maps are often used as base layers for thematic maps.\nFor example, if we‚Äôve collected a dataset about the location of public washrooms in a city, the first thing we might want to do is simply see where they are located by overlaying them onto a base map in GIS. This would be an example of a simple thematic map.\nHere are a few examples of thematic maps that we‚Äôve created, each with different urban data, objectives, and audience in mind.\n\n\n\nHeatmap of Bike Share trips in Toronto in 06/2024\n\n\n\n\n\nIsochrone map of access to outdoor skating rinks in Toronto\n\n\n\n\n\nBi-variate map of public transit accessibility and recent immigrant settlement patterns\n\n\n\n\n\nStatic maps are fixed, pre-generated images that display geographic information in a single, unchangeable format. They are ideal for simple visualization, printing, or when interactivity isn‚Äôt needed‚Äîsuch as in posters, reports, or static web images.\nStatic maps can vary in size from just a few dozen pixels on a screen or a small figure in a report, to graphics in a slide deck, to large maps printed as posters or dozens of maps as part of a series, like in an atlas. The three maps shown above are examples of static maps.\nInteractive maps are digital and allow users to engage with the content by zooming, panning, clicking for details, or filtering data. They are best for exploration, real-time applications, user-driven analysis, or providing increased engagement via animation in a data-story. Here are a few examples that we‚Äôve developed at the School of Cities‚Äù\n\nUrban Activity Atlas\nHeat Vulnerability in Toronto\nKnowledge of Languages in the Greater Toronto & Hamilton Area\n\nInteractive maps can provide flexibility and deeper engagement with spatial data. However, interactive maps typically are much more work to create than a static map as they include all the visual design thinking in a static map, plus additional thinking about how users interact with them and how the maps respond to user inputs, as well as increased technical development time.\nOur general recommendation is to always start with creating static maps, and then move on to creating an interactive version only if a static version is insufficient at conveying the story in your data that you want to convey.\nIf designed well, static maps can be super effective at highlighting key trends and stories in spatial data in a quick and easy to read way.\n\n\n\n\nHow do we decide what were are going to include on our map? Sometimes this may seem obvious, the key dataset(s) that we want to visualize should be there, but particularly with our reference layers, this can often require a lot of little subtle choices.\nThis is because maps always abstractions of reality. It is impossible to show everything in the world on a piece of paper or a screen - nor would we want to, as we usually want to focus our reader‚Äôs attention on specific data point(s), trend(s), or story.\nMaking maps and spatial data visualizations therefore often involve various decisions on selection and generalization of spatial data to reduce visual clutter to focus on specific data points.\n\n\nCartographic selection is about picking the most important things to draw on a map so it‚Äôs not too crowded. Imagine drawing a treasure map ‚Äî you‚Äôd likely want to show big landmarks like mountains or forests, but leave out tiny rocks or every individual tree.\nIn practice, this can include deciding about whether or not to include a dataset at all when creating a map (e.g.¬†in GIS or Python or other map-making software), or if we do include the dataset, if we want to filter it to only show certain features.\nFor example, you may want to include a dataset of public transit lines as a reference layer for a map of your city. However, you may not want to include every single transit route. An example of cartographic selection would be to only show major transit lines by pre-filtering the data by mode (e.g.¬†only show metro/subway) or frequency (e.g.¬†only show routes where a bus or train comes every 10 minutes or less). Now of course if the goal of your map and research is specifically about public transit, you may want to include all the routes.\nThe process of selecting some, but not all of the data that you have available, reduces clutter on your maps and can make them easier to read.\nWhat data to include depends on your objectives, audience, and story.\n\n\n\nSimilar to selection is cartographic generalization, this is when mapmakers simplify real-world details to make maps clearer and less cluttered, especially at smaller scales.\nFor example, a coastline with tons of tiny twists and inlets might get smoothed out‚Äîkeeping the overall shape but removing unnecessary complexity. This helps the map stay readable without losing its key features. It‚Äôs like sketching a quick but accurate version of a photo instead of drawing every single pixel.\n\n\n\nShoreline of eastern Canada at two different levels of simplification\n\n\nAnother example would be that if you had a dataset of sports and recreation facilities in a city with data on the type of activity each is predominately used by (e.g.¬†baseball, football, tennis, etc.). We can choose to have a different colour or symbol for each of these types, or generalize these to have them all look the same (e.g.¬†the same shade of green).\n\n\n\n\nOnce we‚Äôve decided what we want to include on our map, we have to decide how we want to style each layer. In some mapping tools, like QGIS, style options are often called Symbology\nThese are the common graphic styling options available for vector data layers. If you‚Äôve worked with other design software, especially vector graphic software, many of these will be familiar.\nPoints:\n\nSymbol type (e.g.¬†circle, square, etc.)\nSize\nFill colour\nStroke colour\nStroke width\nOpacity\n\nLines:\n\nWidth\nColour\nStroke style (e.g.¬†solid, dashed, etc.)\nOpacity\n\nPolygons:\n\nFill colour\nFill pattern (solid, hatching, etc.)\nStroke colour\nStroke width\nStroke style (e.g.¬†solid, dashed, etc.)\nOpacity\n\nNote that there are other options as well, the above are just the most commonly used.\n\n\nSingle-rule styling is when we want all features in a dataset to look the same, regardless of how they may different in terms of their attributes\nIn the example in our notebook on introducing spatial data and GIS, we created a simple map of Toronto where we had two vector datasets 1) ward administrative boundaries (polygons) and 2) public libraries (points).\nEach of these layers is styled via a single rule. For example, all public libraries are shown as blue squares, even though some libraries may be larger than others or have longer opening hours.\n\n\n\nScreenshot of QGIS with single-rule based styling for two layers\n\n\n\n\n\nA power using tools like QGIS and Python or others like them, is that we can visualize spatial data based on a set of rules relating to their associated attribute data. For example ‚Ä¶\n\nColour census tract data based on median income\nSize business point data based on their number of employees\nShade different land-cover (wetlands, forest, farmland) with different textures for each category\nStyle street network data based on quality and safety of cycling infrastructure\nMany more!\n\nMany of these options we use visual variables like size, hue, saturation, orientation, etc. that we showed in our data visualization notebook, but apply these directly to spatial data on a map.\nLet‚Äôs look at a couple examples in QGIS!\n\n\n\nLand cover?\n\n\n\nColouring some points libraries by number of books\nHow we decide to group (i.e.¬†classify or bin) or data, as well as what colours we pick can\n\n\n\n\nMaps are often the product of multiple layers, each with their own defined styling. Thinking about the ordering of the layers and their styling relative to each other goes a long way in making an effective map.\nWell designed maps, especially thematic maps, often have a visual hierarchy of background and foreground - where the background provides geographic reference (e.g.¬†streets, bodies of water), while the foreground are the key data that we want to show.\nThese are two examples of maps relating to the 2023 wildfires in Yellowknife where we tried to design them to a have strong visual hierarchy of the key data (e.g.¬†fire area, evacuation routes) relative to their geographic reference backgrounds. Both these maps are quite simple from data analytics standpoint, but were part of a story highlighting the scale and impact of the fires.\n\n\n\nMap of the 2023 Yellowknife wildfires\n\n\n\n\n\nScale and location of evacuation resulting from the 2023 Yellowknife wildfires",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#overview",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#overview",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Maps and spatial data visualizations can either created be at the research stage where we are trying to uncover patterns in the data, or as polished communication visuals where we want to convey patterns in the data to a our intended audience.\nCartography is often defined as the science and art of creating maps. It often involves decisions about ‚Ä¶\n\nselection of data to include on the map\ngeneralization of data to reduce complexity and clutter\nhow we want to style and symbolize our data\nhow to order and layer data\nwhat other map layout elements to include (e.g.¬†title, legend, north arrow), as well as how to design these to make a map easy to understand\n\nThis page will cover the basics of what goes into making maps and spatial data visualizations, following each of these steps.\nThe last part of this notebook shows an example in QGIS for making a simple map ‚Ä¶‚Ä¶‚Ä¶‚Ä¶. The\nIf you are interested in creating specific types of maps, check out the following notebooks:\n\nChoropleth maps\nProportional symbol maps\nMapping point density (dot maps, grid maps, heat maps)",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#types-of-maps",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#types-of-maps",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Almost all maps can be thought of as either\n\nReference maps, which provide base geographic information (e.g.¬†streets, land forms, water, etc.) often for locating places or for navigation. Google Maps is probably the most well-used example.\nThematic maps, also called geographic or spatial data visualizations, emphasize patterns or trends tied to a specific theme (e.g., population density, election results, air pollution, etc.).\n\nReference maps are often used as base layers for thematic maps.\nFor example, if we‚Äôve collected a dataset about the location of public washrooms in a city, the first thing we might want to do is simply see where they are located by overlaying them onto a base map in GIS. This would be an example of a simple thematic map.\nHere are a few examples of thematic maps that we‚Äôve created, each with different urban data, objectives, and audience in mind.\n\n\n\nHeatmap of Bike Share trips in Toronto in 06/2024\n\n\n\n\n\nIsochrone map of access to outdoor skating rinks in Toronto\n\n\n\n\n\nBi-variate map of public transit accessibility and recent immigrant settlement patterns\n\n\n\n\n\nStatic maps are fixed, pre-generated images that display geographic information in a single, unchangeable format. They are ideal for simple visualization, printing, or when interactivity isn‚Äôt needed‚Äîsuch as in posters, reports, or static web images.\nStatic maps can vary in size from just a few dozen pixels on a screen or a small figure in a report, to graphics in a slide deck, to large maps printed as posters or dozens of maps as part of a series, like in an atlas. The three maps shown above are examples of static maps.\nInteractive maps are digital and allow users to engage with the content by zooming, panning, clicking for details, or filtering data. They are best for exploration, real-time applications, user-driven analysis, or providing increased engagement via animation in a data-story. Here are a few examples that we‚Äôve developed at the School of Cities‚Äù\n\nUrban Activity Atlas\nHeat Vulnerability in Toronto\nKnowledge of Languages in the Greater Toronto & Hamilton Area\n\nInteractive maps can provide flexibility and deeper engagement with spatial data. However, interactive maps typically are much more work to create than a static map as they include all the visual design thinking in a static map, plus additional thinking about how users interact with them and how the maps respond to user inputs, as well as increased technical development time.\nOur general recommendation is to always start with creating static maps, and then move on to creating an interactive version only if a static version is insufficient at conveying the story in your data that you want to convey.\nIf designed well, static maps can be super effective at highlighting key trends and stories in spatial data in a quick and easy to read way.",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#whats-on-the-map",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#whats-on-the-map",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "How do we decide what were are going to include on our map? Sometimes this may seem obvious, the key dataset(s) that we want to visualize should be there, but particularly with our reference layers, this can often require a lot of little subtle choices.\nThis is because maps always abstractions of reality. It is impossible to show everything in the world on a piece of paper or a screen - nor would we want to, as we usually want to focus our reader‚Äôs attention on specific data point(s), trend(s), or story.\nMaking maps and spatial data visualizations therefore often involve various decisions on selection and generalization of spatial data to reduce visual clutter to focus on specific data points.\n\n\nCartographic selection is about picking the most important things to draw on a map so it‚Äôs not too crowded. Imagine drawing a treasure map ‚Äî you‚Äôd likely want to show big landmarks like mountains or forests, but leave out tiny rocks or every individual tree.\nIn practice, this can include deciding about whether or not to include a dataset at all when creating a map (e.g.¬†in GIS or Python or other map-making software), or if we do include the dataset, if we want to filter it to only show certain features.\nFor example, you may want to include a dataset of public transit lines as a reference layer for a map of your city. However, you may not want to include every single transit route. An example of cartographic selection would be to only show major transit lines by pre-filtering the data by mode (e.g.¬†only show metro/subway) or frequency (e.g.¬†only show routes where a bus or train comes every 10 minutes or less). Now of course if the goal of your map and research is specifically about public transit, you may want to include all the routes.\nThe process of selecting some, but not all of the data that you have available, reduces clutter on your maps and can make them easier to read.\nWhat data to include depends on your objectives, audience, and story.\n\n\n\nSimilar to selection is cartographic generalization, this is when mapmakers simplify real-world details to make maps clearer and less cluttered, especially at smaller scales.\nFor example, a coastline with tons of tiny twists and inlets might get smoothed out‚Äîkeeping the overall shape but removing unnecessary complexity. This helps the map stay readable without losing its key features. It‚Äôs like sketching a quick but accurate version of a photo instead of drawing every single pixel.\n\n\n\nShoreline of eastern Canada at two different levels of simplification\n\n\nAnother example would be that if you had a dataset of sports and recreation facilities in a city with data on the type of activity each is predominately used by (e.g.¬†baseball, football, tennis, etc.). We can choose to have a different colour or symbol for each of these types, or generalize these to have them all look the same (e.g.¬†the same shade of green).",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbols-and-layer-styling",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#symbols-and-layer-styling",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Once we‚Äôve decided what we want to include on our map, we have to decide how we want to style each layer. In some mapping tools, like QGIS, style options are often called Symbology\nThese are the common graphic styling options available for vector data layers. If you‚Äôve worked with other design software, especially vector graphic software, many of these will be familiar.\nPoints:\n\nSymbol type (e.g.¬†circle, square, etc.)\nSize\nFill colour\nStroke colour\nStroke width\nOpacity\n\nLines:\n\nWidth\nColour\nStroke style (e.g.¬†solid, dashed, etc.)\nOpacity\n\nPolygons:\n\nFill colour\nFill pattern (solid, hatching, etc.)\nStroke colour\nStroke width\nStroke style (e.g.¬†solid, dashed, etc.)\nOpacity\n\nNote that there are other options as well, the above are just the most commonly used.\n\n\nSingle-rule styling is when we want all features in a dataset to look the same, regardless of how they may different in terms of their attributes\nIn the example in our notebook on introducing spatial data and GIS, we created a simple map of Toronto where we had two vector datasets 1) ward administrative boundaries (polygons) and 2) public libraries (points).\nEach of these layers is styled via a single rule. For example, all public libraries are shown as blue squares, even though some libraries may be larger than others or have longer opening hours.\n\n\n\nScreenshot of QGIS with single-rule based styling for two layers\n\n\n\n\n\nA power using tools like QGIS and Python or others like them, is that we can visualize spatial data based on a set of rules relating to their associated attribute data. For example ‚Ä¶\n\nColour census tract data based on median income\nSize business point data based on their number of employees\nShade different land-cover (wetlands, forest, farmland) with different textures for each category\nStyle street network data based on quality and safety of cycling infrastructure\nMany more!\n\nMany of these options we use visual variables like size, hue, saturation, orientation, etc. that we showed in our data visualization notebook, but apply these directly to spatial data on a map.\nLet‚Äôs look at a couple examples in QGIS!\n\n\n\nLand cover?\n\n\n\nColouring some points libraries by number of books\nHow we decide to group (i.e.¬†classify or bin) or data, as well as what colours we pick can",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#layers-and-hierarchy",
    "href": "notebooks/urban-data-visualization/maps-and-spatial-data-visualization/maps-and-spatial-data-visualization.html#layers-and-hierarchy",
    "title": "Maps and visualizing spatial data",
    "section": "",
    "text": "Maps are often the product of multiple layers, each with their own defined styling. Thinking about the ordering of the layers and their styling relative to each other goes a long way in making an effective map.\nWell designed maps, especially thematic maps, often have a visual hierarchy of background and foreground - where the background provides geographic reference (e.g.¬†streets, bodies of water), while the foreground are the key data that we want to show.\nThese are two examples of maps relating to the 2023 wildfires in Yellowknife where we tried to design them to a have strong visual hierarchy of the key data (e.g.¬†fire area, evacuation routes) relative to their geographic reference backgrounds. Both these maps are quite simple from data analytics standpoint, but were part of a story highlighting the scale and impact of the fires.\n\n\n\nMap of the 2023 Yellowknife wildfires\n\n\n\n\n\nScale and location of evacuation resulting from the 2023 Yellowknife wildfires",
    "crumbs": [
      "Urban Data Visualization",
      "Maps and visualizing spatial data"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html",
    "title": "Programming with Python and computational notebooks",
    "section": "",
    "text": "Coding, or computer programming, is the act of giving a computer written instructions for a task or set of tasks. According to Wikipedia, it ‚Äúinvolves designing and implementing algorithms, step-by-step specifications of procedures, by writing code in one or more programming languages.‚Äù\nProgramming languages, just like Python and R and JavaScript can be thought of similarly to languages that people speak and write, like Spanish or Arabic ‚Äì each language has its own set of rules and different syntax, but (most) ideas can be translated from one language to another. Instead of using language to communicate with other people, you write code to give directions to a computer. In data analysis, these directions could be to summarize data in a table, create a chart or a map, or generate a statistical model, for example.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-programming",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-programming",
    "title": "Programming with Python and computational notebooks",
    "section": "Why learn programming?",
    "text": "Why learn programming?\nLearning programming gives you more flexibility and control when working with urban and spatial data. It helps you handle larger datasets, repeat analyses efficiently, and create custom maps or tools that aren‚Äôt possible point-and-click software like Excel and GIS. It‚Äôs a practical skill that can really open up what you‚Äôre able to do in this field.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-python",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#why-learn-python",
    "title": "Programming with Python and computational notebooks",
    "section": "Why learn Python?",
    "text": "Why learn Python?\nPython is widely used programming language, especially in urban and spatial analysis because it‚Äôs easy to learn relative to other languages and has a strong ecosystem of libraries for working with data, maps, and models. Plus, it‚Äôs open source and has a huge community, which means lots of learning resources and support.\nThis notebook introduces the Python programming language and Jupyter notebooks. It will cover:\n\nA brief overview of programming, Python, integrated development environments (IDEs), and computational notebooks\nHow to run a Python (.py) script in the terminal\nHow to run code using Jupyter Notebook (.ipynb) in Visual Studio Code\nThe basics of coding in Python, including:\n\nVariables\nSimple math\nLists and dictionaries\nIf statements\nFor and while loops\nFunctions\n\n\nWhile we‚Äôll mostly be looking at Python, the same concepts are applicable across many other languages and software.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#how-does-python-work",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#how-does-python-work",
    "title": "Programming with Python and computational notebooks",
    "section": "How does Python work?",
    "text": "How does Python work?\nWhen you write something in English or another language, you need to choose an environment to write in. For example, if you‚Äôre writing an email, you might use Gmail or Outlook. If you‚Äôre writing a report, you might use Google Docs or Microsoft Word. Or maybe you prefer writing on parchment with a quill pen!\nPython is a programming language that lets you write instructions in a plain text file, often called a script, which the Python interpreter on your computer reads and runs line by line. You don‚Äôt need to compile your code like in some other languages‚Äîjust write it, save it as a .py file, and run it directly.\nFor example, I have a file on my computer called my-script.py, a very simple script which simply prints ‚ÄúHello, world!‚Äù\nprint(\"Hello, world!\")\nWhen running this code in your command line should result in something like the following.\nyour-computer-name:~$ python my-script.py \nHello, world!\nThe command line is often referred to as the shell, terminal, console, prompt or various other names.\nIf you are new to this, check out this tutorial for more in-depth instructions on command line Python",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#tools-for-writing-code",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#tools-for-writing-code",
    "title": "Programming with Python and computational notebooks",
    "section": "Tools for writing code",
    "text": "Tools for writing code\nWhen coding, you also have to choose an environment to write in. This is typically called a ‚Äúcode editor‚Äù or ‚Äúintegrated development environment‚Äù (IDE). The simplest editor would simply be the notepad or text editor on your computer. But there are many IDEs to choose from that make coding much easier, Some of the most popular ones for Python are Visual Studio Code (‚ÄúVS Code‚Äù) and PyCharm.\nA main benefit for using a code editor is that it highlights different parts of your code in different colours or styles making it much easier to read.\n\n\n\nScreenshot of part of a Python script in VS Code",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#computational-notebooks",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#computational-notebooks",
    "title": "Programming with Python and computational notebooks",
    "section": "Computational notebooks",
    "text": "Computational notebooks\nThere are many different ways to execute code, or tell the computer to perform the instructions you‚Äôve written. If you write your code in a file that‚Äôs saved with the .py file extension, you can run it all at once, and the computer will follow all of your instructions, one line at a time. This works well for scripts that are complete and do not need to be run in separate chunks.\nIf you are exploring, analyzing, or visualizing data, it is sometimes easier to work in a computational notebook. Computational notebooks are coding environments that not only allow you to write code, but also let you write explanations and show the outputs of your analysis, very similar to pre-digital formats.\n\n\n\nNotebook by Galileo\n\n\nJupyter Notebooks (named after Galileo‚Äôs work!) are often used for data analysis in Python. When using a Jupyter Notebook, you can run code chunk-by-chunk and see the output right below each chunk. For example, you can write a chunk of code that manipulates a dataframe and then look at the first few rows of the dataframe right below the code. The page you are reading now was written in a Jupyter notebook! :)\n\n\n\nExample of a Jupyter notebook that is plotting a map\n\n\nYou can run Jupyter Notebooks in VS Code via the Jupyter Notebook ‚Äúextension‚Äù. You can also work with notebooks via Jupyter Lab (a web-based environment for Jupyter Notebooks) or Sublime Text.\nTo get started in VS Code, you can follow these instructions. To create a notebook, you just need to create a file with the .ipynb extension in whichever directory you want to save it in. To write and run code in the notebook, you will use ‚Äúcells‚Äù that contain chunks of information.\nWhen you run an individual cell, you are telling the computer to follow the instructions you‚Äôve provided in only that cell, ignoring any other cells that are in the notebook. Be careful of the order in which you ‚Äúexecute‚Äù the cells. For example, if you run cell B before running cell A, it doesn‚Äôt matter if cell B is located below cell A ‚Äì the computer will still follow the instructions in B first and A second.",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#python-basics",
    "href": "notebooks/urban-data-analytics/intro-to-python-and-jupyter/intro-to-python-and-jupyter.html#python-basics",
    "title": "Programming with Python and computational notebooks",
    "section": "Python basics",
    "text": "Python basics\nNow that you have Jupyter Notebook set up, let‚Äôs code! Below, we‚Äôll cover some of the basic building blocks of Python.\nOpen up a fresh .ipynb file and you can get started building bits of code for each of the topics below\n\nVariables\nA variable is like a labeled entity that stores information (numbers, text, etc.). You can create a variable, give it a name, and assign it a value.\nNote that in the below code, some of the lines are written with a # at the beginning - these are comments. Putting a # in front of a line of code tells the computer not to execute it. You should use comments often to explain what your code is doing, either for someone else who might need to understand it or for your future self.\n\n# Assign a value to a variable\nname = \"Alice\"\nage = 30\n\nIn the above cell, we created two variables, one called name and another called age. The name variable is a string because it is a sequence of characters. The computer knows this is a string because we enclosed the text, Alice, in quotes. Single or double quotes both work here.\nThe age variable is an integer because it is a numeric value without decimals. You can see the data type with type([name of object]) like below:\n\ntype(age)\n\nint\n\n\nIf we print the variable, it will show us the variable‚Äôs value:\n\nprint(age)\n\n30\n\n\nWe can also re-assign variables, which will change their value. Now when we print the value of age, it will show 31 instead of 30:\n\n# Re-assign `age` variable with a new value\nage = 31\nprint(age)\n\n31\n\n\nRemember that the computer interprets your code in the order you run the cells, not in the order of the cells in the notebook. For example, if you ran the above cell that assigns a value of 31 to the age variable before running the cell that assigns the value of 30 to age, the computer would store the value of 31.\n\n\nSimple math\nPython can do simple math, like a calculator:\n\n4 + 3\n\n7\n\n\n\n10/3\n\n3.3333333333333335\n\n\nYou can also use the math module to access more advanced functions, like taking the square root. To use this module, you have to import it first:\n\nimport math # import module\nmath.sqrt(25)\n\n5.0\n\n\n\n\nLists\nA list is a collection of elements which can be accessed by their position. Python uses something called zero-based indexing, which means the first element of a sequence has an index (position) of 0 instead of 1.\nIn the below example, fruits is a variable whose type is a list.\n\n# Assign list to variable called 'fruits'\nfruits = [\"apple\", \"banana\", \"cherry\"]\ntype(fruits)\n\nlist\n\n\n\n# Access first item in list\nprint(fruits[0])\n\napple\n\n\n\n# Access second item in list\nprint(fruits[1])\n\nbanana\n\n\nItems can be appended to lists:\n\n# Add \"orange\" to the list\nfruits.append(\"orange\")\nprint(fruits)\n\n['apple', 'banana', 'cherry', 'orange']\n\n\nWe can check the length of the new list to see how many elements it has:\n\nlen(fruits)\n\n4\n\n\nLearn more about lists here.\n\n\nDictionaries\nA dictionary is a type of object that stores information in pairs: each ‚Äúentry‚Äù in the dictionary has both a key and a value. In the example below, person is a dictionary that contains characteristics ‚Äì specifically the name and age ‚Äì of a person.\n\nperson = {\"name\": \"Alice\", \"age\": 30}\n\nWe can access the value associated with the name of the person:\n\nprint(person[\"name\"])\n\nAlice\n\n\nWe can also add a new key-value pair to the dictionary that represents, in this case, the person‚Äôs job:\n\nperson[\"job\"] = \"Engineer\"\nprint(person)\n\n{'name': 'Alice', 'age': 30, 'job': 'Engineer'}\n\n\nLearn more about dictionaries here.\n\n\nIf statements\nIf statements let your code make decisions. You check a condition (e.g., whether age &gt;= 18), and run different code depending on whether it‚Äôs true or false.\n\nage = 18\n\nif age &gt;= 18:\n    print(\"You're an adult!\")\nelse:\n    print(\"You're a minor.\")\n\nYou're an adult!\n\n\nLearn more about if statements here.\n\n\nFor loops\nA for loop repeats code for each item in a list or range. For example:\n\nfor fruit in [\"apple\", \"banana\", \"cherry\"]:\n    print(fruit)\n\napple\nbanana\ncherry\n\n\nLearn more about for loops here.\n\n\nWhile loops\nA while loop repeats code as long as a condition is true. In the below example, we start with 0 and keep adding 1 until we get to 3, after which we stop counting:\n\ncount = 0\nwhile count &lt;= 3:\n    print(\"Counting:\", count)\n    count += 1\n\nCounting: 0\nCounting: 1\nCounting: 2\nCounting: 3\n\n\nLearn more about while loops here.\n\n\nFunctions\nA function is a reusable block of code that performs a task. You ‚Äúdefine‚Äù it (write the code that performs the task) once and ‚Äúcall‚Äù it (run that pre-defined code) whenever you want. In the below example, we define the function greet so that when it is called, it prints ‚ÄúHello, [name]!‚Äù where name is an argument (also known as a parameter) that‚Äôs passed into the function.\n\n# Define the function\ndef greet(name):\n    print(f\"Hello, {name}!\")\n\n# Call the function\ngreet(\"Alice\")\n\nHello, Alice!\n\n\nNot all functions need arguments. For example:\n\n# Define the function\ndef howdy():\n    print(\"Howdy!\")\n\n# Call the function\nhowdy()\n\nHowdy!\n\n\nSome functions have more than one argument. For example:\n\n# Define the function\ndef add_numbers(a, b):\n    c = a + b\n    print(c)\n\n# Call the function\nadd_numbers(8, 7)\n\n15\n\n\nWhile all of the example functions listed above result in something being printed out, most functions do more than that. For example, a single function can filter a dataset based on a set of values, manipulate the resulting dataset, and create a plot.\nLearn more about functions here.\n\n\nLibraries\nLibraries are collections of related functions that do things like analyze data, draw charts, or work with maps.\nMany libraries like math for a variety of mathematical operations, os for interacting with files on your computer, and random for generating random numbers, typically come automatically installed with Python.\nThere are many popular freely available libraries available‚Äîlike pandas for working with data tables, matplotlib for plotting, and geopandas for spatial data‚Äîthat can help you do a lot with just a few lines of code.\nTo include functions that are part of a library, we use import at the top of our script. For example, lets import a function for a random number generator, and use it create our own d20 dice rolling function.\n\nimport random\n\n# Function to roll a 20-sided dice\ndef roll_d20():\n    return random.randint(1, 20)\n\n# Simulate rolling the dice\nroll = roll_d20()\nprint(roll)\n\n14",
    "crumbs": [
      "Urban Data Analytics",
      "Programming with Python and computational notebooks"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html",
    "href": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html",
    "title": "Statistical Foundations",
    "section": "",
    "text": "So far, we‚Äôve learned how to handle data - but we also need to know how to understand it and analyze it in convincing ways. Statistics is the core of this - it can reveal what is typical or an outlier, what relationships exist between different variables, and whether assumptions we have are likely or not.\nIn order to do this, we are going to use the Python libraries numpy and scipy. Both of these libraries offer greater mathematical precision and access to a wide variety of statistical methods and tests, which we can apply to a data set loaded in pandas. For the sake of this tutorial, we also use matplotlib to show some intuitive visualizations - but you don‚Äôt need to worry about tinkering with these yet.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nFor this tutorial, we are going to use election and census data that was adapted for a project published by the School of Cities looking at immigrant vote patterns in the GTA. The data we are using is adapted from the 2025 Ontario election and the 2021 Canadian census. The data frame has the following columns: - riding_name: the name of the riding - num_pop_tot: the number of people in the riding - num_imm_tot: the number of immigrants in the riding - num_imm_new: the number of new immigrants in the riding - avg_hou_inc: the average household income in the riding - num_vm_tot: the number of visible minority individuals in the riding - cons_pct: the vote percent for the Progressive Conservative party - lib_pct: the vote percent for the Liberal Party - ndp_pct: the vote percent for the New Democratic Party (NDP) - oth_pct: the vote percent for other parties\n\ndf = pd.read_csv('./data/ont-ed_stats_2025.csv')\ndf.head()\n\n\n\n\n\n\n\n\nriding_name\nnum_pop_tot\nnum_imm_tot\nnum_imm_new\navg_hou_inc\nnum_vm_tot\ncons_pct\nlib_pct\nndp_pct\noth_pct\n\n\n\n\n0\nBrampton Centre\n102309.0\n48275.0\n14985.0\n103369.0\n68880.0\n51.85\n33.92\n8.77\n5.45\n\n\n1\nScarborough‚ÄîRouge Park\n102256.0\n52637.0\n8294.0\n120745.0\n77090.0\n52.98\n34.75\n10.38\n1.89\n\n\n2\nScarborough North\n94688.0\n60423.0\n11513.0\n103151.0\n87647.0\n52.98\n34.75\n10.38\n1.89\n\n\n3\nAjax\n115392.0\n47716.0\n8721.0\n137570.0\n74759.0\n44.16\n44.96\n7.01\n3.87\n\n\n4\nBeaches‚ÄîEast York\n106811.0\n33588.0\n9031.0\n129975.0\n39034.0\n21.38\n51.17\n22.94\n4.51",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical Foundations"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#descriptive-statistics-the-shape-of-the-data",
    "href": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#descriptive-statistics-the-shape-of-the-data",
    "title": "Statistical Foundations",
    "section": "Descriptive statistics: the shape of the data",
    "text": "Descriptive statistics: the shape of the data\n\nCentral tendency\nWhat is ‚Äútypical‚Äù in a set of data? This is at the heart of the notion of the central tendency in statistics. There are three common ways to measure this, with a accompanying functions. - Mean (np.mean()): the average of all values - Median (np.median()): the ‚Äúmiddle‚Äù value if you order them, ie. at the 50th percentile - Mode (stats.mode()): the most common value\nLet‚Äôs compute some of these central tendency statistics for the NDP, and compare them to the distribution of vote shares in all ridings.\n\nparty_var = 'ndp_pct'\n\nmean = np.mean(df[party_var])\nmedian = np.median(df[party_var])\nmode = stats.mode(df[party_var])\n\nprint(f\"Mean: {mean:.2f}\")\nprint(f\"Median: {median:.2f}\")\nprint(f\"Mode: {mode.mode} (appears {mode.count} times)\")\n\nMean: 14.11\nMedian: 6.88\nMode: 3.95 (appears 2 times)\n\n\n\nplt.figure(figsize=(8, 5))\nplt.hist(df[party_var], bins=20, color='orange', edgecolor='black')\n\nplt.axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:.2f}%')\nplt.axvline(median, color='blue', linestyle='--', linewidth=2, label=f'Median: {median:.2f}%')\nplt.axvline(mode.mode, color='green', linestyle='--', linewidth=2, label=f'Mode: {mode.mode:.2f}%')\n\nplt.title('Distribution of Vote Percentages by Riding')\nplt.xlabel('Vote Percentage (%)')\nplt.ylabel('Number of Ridings')\nplt.grid(axis='y', alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nNotice the difference between the mean and the median. While the mean accounts for all of the data directly, it is also sensitive to outliers and can be pulled in one direction or another.\nTry setting the variable party_var to different parties to see how the matter of mean vs.¬†median plays out for different distributions.\n\n\nDispersion\nSimilar to how we ask what is typical in a set of data, it‚Äôs also important to ask how that data varies. This is what we refer to when we talk about the dispersion of a distribution, and there are four common measures (with functions): - Standard deviation (np.std()): How spread out data points are from the mean, in the same units as the original data. - Variance (np.var()): The average squared deviation from the mean, representing how wildly individual values differ from the average or overall unevenness. - Range (np.max() - np.min()): The difference between the maximum and minimum values, showing the total spread of the dataset. - Interquartile range (np.percentile(..., [25, 75])): The range of the middle 50% of data (Q3‚ÄìQ1), reducing sensitivity to outliers.\nLet‚Äôs take a look at how the number of immigrants across different ridings varies, and compare it with the mean and median.\n\ncol_var = 'num_imm_tot'\n\nmean = np.mean(df[col_var])\nmedian = np.median(df[col_var])\nstd_dev = np.std(df[col_var], ddof=1)\nvariance = np.var(df[col_var])\ndata_range = np.max(df[col_var]) - np.min(df[col_var])\nq1, q3 = np.percentile(df[col_var], [25, 75])\niqr = q3 - q1\n\nprint(f\"Mean: {mean:.2f}\")\nprint(f\"Median: {median:.2f}\")\nprint('===')\nprint(f\"Std Dev: {std_dev:.2f}\")\nprint(f\"Variance: {variance:.2f}\")\nprint(f\"Range: {data_range:.2f}\")\nprint(f\"IQR: {iqr:.2f}\")\n\nMean: 52330.33\nMedian: 54255.00\n===\nStd Dev: 13630.44\nVariance: 182348372.93\nRange: 62964.00\nIQR: 18211.50\n\n\n\nplt.figure(figsize=(10, 6))\nax = plt.gca()\n\n# Histogram\nn, bins, patches = plt.hist(df[col_var], bins=20, color='#2ca02c', edgecolor='black', alpha=0.7)\n\n# Central tendency and dispersion\nplt.axvline(mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean:,.0f}')\nplt.axvline(median, color='blue', linestyle='--', linewidth=2, label=f'Median: {median:,.0f}')\n\nplt.axvspan(mean - std_dev, mean + std_dev, color='red', alpha=0.1, label=f'¬±1 Std Dev: {std_dev:,.0f}')\nplt.axvspan(q1, q3, color='blue', alpha=0.1, label=f'IQR: {iqr:,.0f}')\n\n# Annotations\nplt.title('Distribution of Immigrant Population by Riding', pad=20, fontsize=14)\nplt.xlabel('Number of Immigrants', fontsize=12)\nplt.ylabel('Number of Ridings', fontsize=12)\nplt.grid(axis='y', alpha=0.2)\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTake a look at how different measures of dispersion capture different parts of the data. Once again, set col_var to a different variable or two and examine the different measures of dispersion and how they play out.",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical Foundations"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#bivariate-analysis-finding-similarities-in-the-data",
    "href": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#bivariate-analysis-finding-similarities-in-the-data",
    "title": "Statistical Foundations",
    "section": "Bivariate analysis: finding similarities in the data",
    "text": "Bivariate analysis: finding similarities in the data\n\nCorrelation\nCorrelation measures how closely two variables move together, ranging from -1 (as X increases, Y decreases), to +1 (as X increases, Y increases). A value near 0 indicates no linear association. While it is acceptable to use the word ‚Äòrelationship‚Äô here instead of ‚Äòassociation‚Äô, it‚Äôs worth noting that a strong correlation is not enough to show causation ‚Äì namely, if we had a correlation value of 1, we can‚Äôt definitively say that Y increases because X increases, only that they increase together.\nWe‚Äôll be looking at a very common measure called Pearson correlation. It measures how closely two variables follow a straight-line relationship, ideal for normally distributed data with linear trends (e.g., height vs.¬†weight). While we won‚Äôt look at it today, it‚Äôs worth independently looking into ‚ÄúSpearman Rank Correlation‚Äù as well - which can capture nonlinear trends.\nWe‚Äôll look at the example of vote share for the Progressive Conservatives and the number of immigrants in a riding below. In the project that this data was drawn from, we used correlation to show how immigrant voters in the GTA are shifting conservative over time.\n\ncensus_var = 'num_imm_tot'\nparty_var = 'cons_pct'\n\nr, p_val = stats.pearsonr(df[census_var], df[party_var])\nprint(f\"Pearson corr: {r:.2f} (p-value: {p_val:.3f})\")\n\nPearson corr: 0.40 (p-value: 0.003)\n\n\nHere we can see a correlation between the two variables we have chosen. Try setting census_var and party_var to different column values and see if there is a correlation between those as well, or not!",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical Foundations"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#linear-regression-relationships-between-variables",
    "href": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#linear-regression-relationships-between-variables",
    "title": "Statistical Foundations",
    "section": "Linear regression: relationships between variables",
    "text": "Linear regression: relationships between variables\n\nSimple regression\nLinear Regression identifies the straight-line relationship between two variables, allowing you to predict outcomes (e.g., voting percentages) based on another factor (e.g., immigrant population). It calculates a ‚Äúbest-fit‚Äù line that minimizes the distance between all data points and the line itself, summarized by the equation y = slope  x + intercept*.\n\ncensus_var = 'num_imm_tot'\nparty_var = 'cons_pct'\n\nx = df[census_var].values  # Independent variable\ny = df[party_var].values     # Dependent variable\n\n# Fit regression\nslope, intercept, r_value, p_value, _ = stats.linregress(x, y)\nr_squared = r_value ** 2\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.scatter(x, y, color='blue', alpha=0.5, label='Actual Data')\nplt.plot(x, intercept + slope * x, 'r-', label=f'Regression Line: $R¬≤$={r_squared:.2f}\\nslope={slope:.5f}')\nplt.xlabel('Immigrant Population')\nplt.ylabel('Conservative Vote %')\nplt.title('Predicting Votes from Demographics', pad=15)\nplt.legend()\nplt.grid(alpha=0.2)\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(f\"Model Equation: {party_var} = {intercept:.2f} + {slope:.5f} * {census_var}\")\nprint(f\"R¬≤ = {r_squared:.3f} (Explains {r_squared*100:.1f}% of variance)\")\nprint(f\"p-value = {p_value:.4f} {'(Significant)' if p_value &lt; 0.05 else '(Not significant)'}\")\n\nModel Equation: cons_pct = 23.99 + 0.00039 * num_imm_tot\nR¬≤ = 0.161 (Explains 16.1% of variance)\np-value = 0.0026 (Significant)\n\n\nNot all regression lines are made equal, and R¬≤ measures how well the regression line fits the data, ranging from 0 (no fit) to 1 (perfect fit). It answers: ‚ÄúWhat percentage of variation in voting patterns can be explained by immigrant population?‚Äù. In the above example, we can say that 16.1% of the differences in conservative votes across ridings are predictable from immigrant numbers‚Äîthe rest is due to other factors.\nNow try the following two exercises to examine outcomes: - Change the x-variable to avg_hou_inc. Does wealth predict conservative votes better than immigrant population? - Add ndp_pct as y-variable. Is the relationship positive or negative?\n\n\nOther kinds of regression\nWhile linear regression is ideal for modeling straight-line relationships, real-world data often requires more flexible approaches. Below are key alternatives with use cases relevant to political/demographic data, along with their Python implementations: - Multiple Linear Regression: Modeling how multiple demographic factors jointly influence voting patterns (e.g., predicting Conservative vote share using both immigrant population and average income). - Polynomial Regression: Modeling curved relationships (e.g., voter turnout vs.¬†age, where middle-aged groups vote more than very young or elderly). - Logistic Regression: Predicting binary outcomes (e.g., whether a riding will vote Conservative (1) or not (0) based on income thresholds). - Ridge/Lasso Regression: Handling multicollinearity (e.g., when immigrant population and visible minority numbers are strongly correlated).\nIn case you want to do these more complex kinds of regression, the typical go to library is scikit-learn.",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical Foundations"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#hypothesis-testing-making-conclusions",
    "href": "notebooks/urban-data-analytics/statistical-foundations/statistical-foundations.html#hypothesis-testing-making-conclusions",
    "title": "Statistical Foundations",
    "section": "Hypothesis testing: making conclusions",
    "text": "Hypothesis testing: making conclusions\nHypothesis testing evaluates whether observed patterns in data are statistically significant or likely due to random chance. A t-test compares the means of two groups (e.g., Conservative vote share in high- vs.¬†low-income ridings) to determine if their difference is meaningful.\nThe t-statistic measures the size of the difference relative to the variability in the data‚Äîthink of it like a ‚Äúsignal-to-noise ratio‚Äù where if the group difference (signal) stands out from natural variation (noise). Larger absolute values (e.g., |t| &gt; 2) suggest stronger evidence against no difference (a strong ‚Äúsignal‚Äù), and the sign indicates direction (e.g., positive = Group A &gt; Group B).\nThe p-value then calculates how likely we‚Äôd see this t-statistic if no true difference existed. If p &lt; 0.05, we reject the null hypothesis.\nThere‚Äôs two different t-tests we‚Äôll illustrate today. - One-sample: Compares data to a fixed number - Two-sample: Compares two datasets to each other.\n\nOne-Sample t-test\nOur data set only includes ridings within the Greater Toronto Area (GTA) - but more than half of the ridings exist in the rest of Ontario. In the 2025 election, the Conservatives won 43% of the vote - does the average vote share for Conservatives in the GTA vary from Ontario in full?\n\nparty_var = 'cons_pct'\ntotal_party_vote = 43\n\nsample_data = df[party_var]\nt_stat, p_value = stats.ttest_1samp(sample_data, popmean=total_party_vote)\n\nprint(f\"t-statistic: {t_stat:.2f}, p-value: {p_value:.4f}\")\nprint(\"Significantly different!\" if p_value &lt; 0.05 else \"No significant difference.\")\n\nt-statistic: 0.76, p-value: 0.4497\nNo significant difference.\n\n\n\nplt.figure(figsize=(8, 4))\nplt.hist(sample_data, bins=15, color='skyblue', edgecolor='black', alpha=0.7)\nplt.axvline(total_party_vote, color='red', linestyle='--', label=f'Vote share in full election ({total_party_vote}%)')\nplt.xlabel('Vote %')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTwo-Sample t-test\nWe‚Äôve talked a lot about immigrant ridings as a whole, but we can also examine different subsets of them. For example: Do high-immigrant ridings vote differently than low-immigrant ridings?\n\ncensus_var = 'num_imm_tot'\nparty_var = 'cons_pct'\n\n# Split data into two groups (median split)\nmedian_data = df[census_var].median()\nhigh_data = df[df[census_var] &gt; median_data][party_var]\nlow_data = df[df[census_var] &lt;= median_data][party_var]\n\n# Independent t-test\nt_stat, p_value = stats.ttest_ind(high_data, low_data)\n\nprint(f\"t-statistic: {t_stat:.2f}, p-value: {p_value:.4f}\")\nprint(\"Significantly different!\" if p_value &lt; 0.05 else \"No significant difference.\")\n\nt-statistic: 2.52, p-value: 0.0149\nSignificantly different!\n\n\n\nplt.figure(figsize=(8, 4))\nplt.hist(high_data, bins=15, alpha=0.5, label=f'High-{census_var} ridings')\nplt.hist(low_data, bins=15, alpha=0.5, label=f'Low-{census_var} ridings')\nplt.xlabel('Vote %')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFinally, substitute out the census and party variables for different versions and run this analysis. Assess whether there are significant differences in the new hypothesis that you are testing.",
    "crumbs": [
      "Urban Data Analytics",
      "Statistical Foundations"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "A lot of urban datasets are directly linked to specific places, e.g.¬†addresses, streets, neighbourhoods, political or administrative boundaries, etc.\nData that include place-based information are often called spatial, geographic, or geospatial data Geographic Information Systems (GIS) are tools and software for analyzing, processing, and visualizing spatial data.\nThis page will cover the basics of spatial data and how we can view and interact with this data in the software QGIS (you will need to download QGIS to work on the hands-on part of this tutorial).\n\n\nA spatial dataset is a combination of‚Ä¶\n\nattribute data (the what)\nlocation data and spatial dimensions (the where)\n\nSpatial data, this combination of attribute and location data, can be organized and represented in a number of different formats.\nFor example, a city can be represented on a map via a single point with a label (e.g.¬†based on latitude and longitude coordinates). Or a city can be represented as a polygon, based on on it‚Äôs administrative boundary\nImportantly, there are always uncertainty about the level of accuracy, precision, and resolution of spatial data. Spatial data are abstractions of reality, and thus have some loss of information when used for visualization and analysis. Any analysis can only be as good as the available data.\nThe two most common forms of spatial data are vector data and raster data.\n\n\n\nExamples of spatial data\n\n\n\n\nVector data uses geographic coordinates, or a series of coordinates, to create points, lines, and polygons representing real-world features.\ne.g.¬†in the map below (a screenshot of OpenStreetMap) lines are used to represent roads and rail, points for retail, polygons for parks and buildings, etc.\n\n\n\nScreenshot of OpenStreetMap\n\n\nSpatial data can be stored as columns in traditional table-based formats (e.g.¬†.csv), but there are also a wide range of data formats specific to spatial data. These are some of the most common:\n\nGeoJSON .geojson\nGeoPacakge .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nHere is an example of a single point location of a city stored in a .geojson file. .geojson is a standardized data schema .json format for spatial data.\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [-80.992, 46.490]\n            },\n            \"properties\": {\n                \"city_name\": \"Sudbury\"\n            }\n        }\n    ]\n}\n\n\n\nRaster data represents space as a continuous grid with equal cell sizes. Each cell contains a value pertaining to the type of feature it represents. These values can be quantitative (e.g.¬†elevation) or categorical (e.g.¬†type of land use). Common examples of raster data include Digital Elevation Models (DEMs), satellite imagery, and scanned images like historical maps.\ne.g.¬†the map below shows a DEM for Toronto at two different scales.\n\n\n\nDEM of Toronto\n\n\n\n\n\n\nWe use Geographic Information Systems (GIS) to analyze, manipulate, and visualize spatial data.\nWhy is GIS useful?\n\nexplore spatial patterns and relationships in data\ncreate maps for publications\ngenerate new data, either ‚Äúby hand‚Äù or via spatial relationships from other data (e.g.¬†through spatial queries)\nperform spatial analysis (i.e.¬†statistical methods applied to spatial data)\n\nGIS is often thought of as more than just a tool or piece of software. It can refer to all aspects of managing and analyzing digital spatially referenced data.\nThe power of GIS software and tools is the ability to work with data stored in different layers (e.g.¬†a layer for roads, another for buildings, and so on) in conjunction with each other. These layers can be visualized and analyzed relative to each other based on their spatial relationships.\n\n\n\nSimple AI generated schematic of layers\n\n\nGIS software usually links to data stored elsewhere on a computer, rather than in a project file. If the source location of the data (i.e.¬†which folder it‚Äôs in) changes, then this will have to be updated in the GIS project. If data are edited in GIS, it will update the data in its source location.\nThe open-source QGIS and the proprietary ESRI ArcGIS are the two most used desktop GIS software. ESRI‚Äôs suite of tools are often used by larger corporate and government organizations while QGIS is more used by small consultants and freelancers, non-profits, and academia. Many spatial data processing, visualization, and analyses steps also have equivalents in Python, R, SQL and other programming languages via specific libraries\nWe‚Äôll be working with QGIS and Python, because they free and work across multiple operating systems, and are commonly used across different areas of work and research. But pretty much everything we‚Äôll show can be accomplished across different software options, the buttons and steps will just be a bit different.\nLearning the core analytical and visualization concepts and ideas are much more important than exactly where to click or what specific functions to run.\n\n\n\nA CRS is a framework for describing the geographic position of location-based data. It tells your GIS software where your data are located on the Earth. Without a CRS, your data would just be a bunch of numbers without an ability to place them on a map and relate them to other data.\nThere are thousands of CRSs. Each CRS has specific attributes about its position and one main set of spatial units (e.g.¬†a CRS will measure location in degrees longitude/latitude, metres, miles, etc.).\nProbably the most common CRS is WGS84, which uses longitude/latitude to link data to the Earth‚Äôs surface.\nWhen doing spatial analyses and processing where we are comparing two or more datasets to each other, it is important that they are in the same CRS.\n\n\nCRS are often paired with map projections. This is sometimes called a Projected CRS.\nThe earth is a 3D globe, but most maps are represented on 2D surfaces (e.g.¬†on a screen, piece of paper). Map projections are mathematical model to flatten the earth to create a 2D view.\nEvery projection distorts shape, area, distance, or direction in some way or another. Different map projections distort the Earth in different ways. Here are some examples! For more check out this projection transition tool\n\n\n\nQGIS screenshot without data\n\n\nFor doing analysis and an urban scale, it is important that we choose a map projection that conserves area and distances in both X and Y directions (i.e.¬†space is not being distorted in one particular direction more than others)\nFor example, these are two aerial images of Toronto, the left uses a local Mercator projection which does not distort data at a local scale, while the right is a rectangular projection where degrees latitude are equal shown at the same scale as degrees longitude.\nA general recommendation for urban-scale analyses is to use a Universal Transverse Mercator (UTM) projected CRS, which conserves area, distance, and direction along bands of the earth. This is a good tool for finding which UTM zone your region is in.\n\n\n\n\nLet‚Äôs use QGIS to quickly view a few spatial datasets. When you open up QGIS, it should look something like this:\n\n\n\nQGIS screenshot without data\n\n\nThe Browser panel is used for finding and loading data on your computer or from external sources, each line and symbol is for a different data source. The Layer panel will list all the datasets that are loaded into your project. The big white space will populate with data once it is loaded. The buttons at the top include a variety of options for loading data, saving your project, exploring your map, and various common processing and analytical tools.\n\n\nLet‚Äôs begin by adding in a basemap. Basemaps are often used to provide geographic reference for other data that we load into QGIS.\nWe‚Äôll add a basemap of OpenStreetMap, a free crowd-sourced map of the world, available as XYZ raster tiles. To do this, go to Layer - Data Source Manager or simply click on the Data Source Manager button highlighted below. When you open the Data Source Manager you have options for a wide variety of data types to add to your map.\n\n\n\nData source manager location in QGIS\n\n\n\n\n\nAdding an XYZ tile\n\n\nHere is the URL to paste into QGIS for adding an OpenStreetMap basemap: https://tile.openstreetmap.org/{z}/{x}/{y}.png.\nWhen you click ‚ÄòAdd‚Äô it should be added to the map. You can use the navigation buttons or just scroll on your mouse to zoom to your city.\n\n\n\nQGIS with OpenStreetMap\n\n\nLet‚Äôs now add some vector data to the map. We‚Äôve pre-downloaded two datasets from the City of Toronto‚Äôs Open Data Portal - City Wards (polygons) - Library Locations (points)\nThese can be added via the Data Source Manager - Vector, or simply via drag-and-drop from the folder of where they are located on your computer.\n\n\n\nQGIS with Toronto data\n\n\n\n\n\nThe data added to the project will be listed in the Layers panel. The order of the layers determine the order of which they stack on top of each other on the map. In the example above, the libraries are listed at the top of the other two layers, and it thus appears on top of the other two layers on the map.\nTo change the layer order, you can drag individual layers to be above or below any other layer in the Layers panel.\nMost vector data has associated attribute data, e.g.¬†the number of a ward, the name of a library, etc. To view this data, right-click on a layer and then click on Open Attribute Table. In GIS, column names are often called ‚ÄúFields‚Äù\n\n\n\nWhen you load vector data like this into QGIS, the layers have default styling (e.g.¬†colours, sizes, line-widths, etc.).\nThere are tonnes of options in QGIS to change these initial styles. To do so, right-click on a layer, go to Properties, and then Symbology.\nThere are lots of options in here to change the colours, size, and symbols.\nFor example, in the screenshot below, the sizes of the libraries and the line-widths of the wards are increased a bit, the wards are given a transparent fill, and the library symbols are converted to squares.\n\n\n\nQGIS with Toronto data\n\n\nTip! you get extra options if you click on the sub-component of the symbol. e.g.¬†in this example I clicked on Simple Fill to find additional styling options like pattern-based fill styles.\n\n\n\nQGIS symbology example\n\n\nWe‚Äôll explore data-driven styling, where colours or other style options are based on data values, in a future tutorial.\n\n\n\nYou can change the project CRS in QGIS by going to Project - Properties - CRS.\n\n\n\nYou can export and create copies of any of your data layers. This may be useful if you want to change the file format, the CRS, subset the attribute table, or simply just make a copy of the data.\nTo do this, go to Export - Save Features As, and you can adjust the Format, CRS, and other options as needed.\n\n\n\nYou can save a project in QGIS by going to Project and then Save or Save As.\nIt will get saved to a .qgis file. These files save styling, filters, layer orders, map scale and location, and layout settings.\nHowever, .qgis files do not save datasets directly in the file, only paths and links to where the data is stored, either on your computer or from a URL like the OpenStreetMap basemap. Saving only links to data is common in other tools and software.\nTherefore, if you share your project, you‚Äôll need to also share any local datasets it is linking to.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#spatial-data",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#spatial-data",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "A spatial dataset is a combination of‚Ä¶\n\nattribute data (the what)\nlocation data and spatial dimensions (the where)\n\nSpatial data, this combination of attribute and location data, can be organized and represented in a number of different formats.\nFor example, a city can be represented on a map via a single point with a label (e.g.¬†based on latitude and longitude coordinates). Or a city can be represented as a polygon, based on on it‚Äôs administrative boundary\nImportantly, there are always uncertainty about the level of accuracy, precision, and resolution of spatial data. Spatial data are abstractions of reality, and thus have some loss of information when used for visualization and analysis. Any analysis can only be as good as the available data.\nThe two most common forms of spatial data are vector data and raster data.\n\n\n\nExamples of spatial data\n\n\n\n\nVector data uses geographic coordinates, or a series of coordinates, to create points, lines, and polygons representing real-world features.\ne.g.¬†in the map below (a screenshot of OpenStreetMap) lines are used to represent roads and rail, points for retail, polygons for parks and buildings, etc.\n\n\n\nScreenshot of OpenStreetMap\n\n\nSpatial data can be stored as columns in traditional table-based formats (e.g.¬†.csv), but there are also a wide range of data formats specific to spatial data. These are some of the most common:\n\nGeoJSON .geojson\nGeoPacakge .gpkg\nShapefile .shapfile\nGeodatabase .gdb\n\nHere is an example of a single point location of a city stored in a .geojson file. .geojson is a standardized data schema .json format for spatial data.\n{\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"geometry\": {\n                \"type\": \"Point\",\n                \"coordinates\": [-80.992, 46.490]\n            },\n            \"properties\": {\n                \"city_name\": \"Sudbury\"\n            }\n        }\n    ]\n}\n\n\n\nRaster data represents space as a continuous grid with equal cell sizes. Each cell contains a value pertaining to the type of feature it represents. These values can be quantitative (e.g.¬†elevation) or categorical (e.g.¬†type of land use). Common examples of raster data include Digital Elevation Models (DEMs), satellite imagery, and scanned images like historical maps.\ne.g.¬†the map below shows a DEM for Toronto at two different scales.\n\n\n\nDEM of Toronto",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#geographic-information-systems-gis",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#geographic-information-systems-gis",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "We use Geographic Information Systems (GIS) to analyze, manipulate, and visualize spatial data.\nWhy is GIS useful?\n\nexplore spatial patterns and relationships in data\ncreate maps for publications\ngenerate new data, either ‚Äúby hand‚Äù or via spatial relationships from other data (e.g.¬†through spatial queries)\nperform spatial analysis (i.e.¬†statistical methods applied to spatial data)\n\nGIS is often thought of as more than just a tool or piece of software. It can refer to all aspects of managing and analyzing digital spatially referenced data.\nThe power of GIS software and tools is the ability to work with data stored in different layers (e.g.¬†a layer for roads, another for buildings, and so on) in conjunction with each other. These layers can be visualized and analyzed relative to each other based on their spatial relationships.\n\n\n\nSimple AI generated schematic of layers\n\n\nGIS software usually links to data stored elsewhere on a computer, rather than in a project file. If the source location of the data (i.e.¬†which folder it‚Äôs in) changes, then this will have to be updated in the GIS project. If data are edited in GIS, it will update the data in its source location.\nThe open-source QGIS and the proprietary ESRI ArcGIS are the two most used desktop GIS software. ESRI‚Äôs suite of tools are often used by larger corporate and government organizations while QGIS is more used by small consultants and freelancers, non-profits, and academia. Many spatial data processing, visualization, and analyses steps also have equivalents in Python, R, SQL and other programming languages via specific libraries\nWe‚Äôll be working with QGIS and Python, because they free and work across multiple operating systems, and are commonly used across different areas of work and research. But pretty much everything we‚Äôll show can be accomplished across different software options, the buttons and steps will just be a bit different.\nLearning the core analytical and visualization concepts and ideas are much more important than exactly where to click or what specific functions to run.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#coordinate-reference-systems-crs-and-map-projections",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#coordinate-reference-systems-crs-and-map-projections",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "A CRS is a framework for describing the geographic position of location-based data. It tells your GIS software where your data are located on the Earth. Without a CRS, your data would just be a bunch of numbers without an ability to place them on a map and relate them to other data.\nThere are thousands of CRSs. Each CRS has specific attributes about its position and one main set of spatial units (e.g.¬†a CRS will measure location in degrees longitude/latitude, metres, miles, etc.).\nProbably the most common CRS is WGS84, which uses longitude/latitude to link data to the Earth‚Äôs surface.\nWhen doing spatial analyses and processing where we are comparing two or more datasets to each other, it is important that they are in the same CRS.\n\n\nCRS are often paired with map projections. This is sometimes called a Projected CRS.\nThe earth is a 3D globe, but most maps are represented on 2D surfaces (e.g.¬†on a screen, piece of paper). Map projections are mathematical model to flatten the earth to create a 2D view.\nEvery projection distorts shape, area, distance, or direction in some way or another. Different map projections distort the Earth in different ways. Here are some examples! For more check out this projection transition tool\n\n\n\nQGIS screenshot without data\n\n\nFor doing analysis and an urban scale, it is important that we choose a map projection that conserves area and distances in both X and Y directions (i.e.¬†space is not being distorted in one particular direction more than others)\nFor example, these are two aerial images of Toronto, the left uses a local Mercator projection which does not distort data at a local scale, while the right is a rectangular projection where degrees latitude are equal shown at the same scale as degrees longitude.\nA general recommendation for urban-scale analyses is to use a Universal Transverse Mercator (UTM) projected CRS, which conserves area, distance, and direction along bands of the earth. This is a good tool for finding which UTM zone your region is in.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#working-with-spatial-data-in-qgis",
    "href": "notebooks/urban-data-analytics/spatial-data-and-gis/spatial-data-and-gis.html#working-with-spatial-data-in-qgis",
    "title": "Spatial data and GIS",
    "section": "",
    "text": "Let‚Äôs use QGIS to quickly view a few spatial datasets. When you open up QGIS, it should look something like this:\n\n\n\nQGIS screenshot without data\n\n\nThe Browser panel is used for finding and loading data on your computer or from external sources, each line and symbol is for a different data source. The Layer panel will list all the datasets that are loaded into your project. The big white space will populate with data once it is loaded. The buttons at the top include a variety of options for loading data, saving your project, exploring your map, and various common processing and analytical tools.\n\n\nLet‚Äôs begin by adding in a basemap. Basemaps are often used to provide geographic reference for other data that we load into QGIS.\nWe‚Äôll add a basemap of OpenStreetMap, a free crowd-sourced map of the world, available as XYZ raster tiles. To do this, go to Layer - Data Source Manager or simply click on the Data Source Manager button highlighted below. When you open the Data Source Manager you have options for a wide variety of data types to add to your map.\n\n\n\nData source manager location in QGIS\n\n\n\n\n\nAdding an XYZ tile\n\n\nHere is the URL to paste into QGIS for adding an OpenStreetMap basemap: https://tile.openstreetmap.org/{z}/{x}/{y}.png.\nWhen you click ‚ÄòAdd‚Äô it should be added to the map. You can use the navigation buttons or just scroll on your mouse to zoom to your city.\n\n\n\nQGIS with OpenStreetMap\n\n\nLet‚Äôs now add some vector data to the map. We‚Äôve pre-downloaded two datasets from the City of Toronto‚Äôs Open Data Portal - City Wards (polygons) - Library Locations (points)\nThese can be added via the Data Source Manager - Vector, or simply via drag-and-drop from the folder of where they are located on your computer.\n\n\n\nQGIS with Toronto data\n\n\n\n\n\nThe data added to the project will be listed in the Layers panel. The order of the layers determine the order of which they stack on top of each other on the map. In the example above, the libraries are listed at the top of the other two layers, and it thus appears on top of the other two layers on the map.\nTo change the layer order, you can drag individual layers to be above or below any other layer in the Layers panel.\nMost vector data has associated attribute data, e.g.¬†the number of a ward, the name of a library, etc. To view this data, right-click on a layer and then click on Open Attribute Table. In GIS, column names are often called ‚ÄúFields‚Äù\n\n\n\nWhen you load vector data like this into QGIS, the layers have default styling (e.g.¬†colours, sizes, line-widths, etc.).\nThere are tonnes of options in QGIS to change these initial styles. To do so, right-click on a layer, go to Properties, and then Symbology.\nThere are lots of options in here to change the colours, size, and symbols.\nFor example, in the screenshot below, the sizes of the libraries and the line-widths of the wards are increased a bit, the wards are given a transparent fill, and the library symbols are converted to squares.\n\n\n\nQGIS with Toronto data\n\n\nTip! you get extra options if you click on the sub-component of the symbol. e.g.¬†in this example I clicked on Simple Fill to find additional styling options like pattern-based fill styles.\n\n\n\nQGIS symbology example\n\n\nWe‚Äôll explore data-driven styling, where colours or other style options are based on data values, in a future tutorial.\n\n\n\nYou can change the project CRS in QGIS by going to Project - Properties - CRS.\n\n\n\nYou can export and create copies of any of your data layers. This may be useful if you want to change the file format, the CRS, subset the attribute table, or simply just make a copy of the data.\nTo do this, go to Export - Save Features As, and you can adjust the Format, CRS, and other options as needed.\n\n\n\nYou can save a project in QGIS by going to Project and then Save or Save As.\nIt will get saved to a .qgis file. These files save styling, filters, layer orders, map scale and location, and layout settings.\nHowever, .qgis files do not save datasets directly in the file, only paths and links to where the data is stored, either on your computer or from a URL like the OpenStreetMap basemap. Saving only links to data is common in other tools and software.\nTherefore, if you share your project, you‚Äôll need to also share any local datasets it is linking to.",
    "crumbs": [
      "Urban Data Analytics",
      "Spatial data and GIS"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/index.html",
    "href": "notebooks/urban-data-analytics/index.html",
    "title": "Urban data analysis",
    "section": "",
    "text": "Urban data analysis",
    "crumbs": [
      "Urban Data Analytics"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html",
    "href": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html",
    "title": "Introduction to Geopandas",
    "section": "",
    "text": "GeoPandas is a library for working with spatial (typically geographic) data. It extends the functionality of pandas to support spatial data types and operations, making it easier to analyze, visualize, and manipulate spatial data. Many of the tasks that are typically done within a GIS can be done via geopandas.\n\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#loading-exploring-and-geometric-data-types",
    "href": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#loading-exploring-and-geometric-data-types",
    "title": "Introduction to Geopandas",
    "section": "Loading, Exploring, and Geometric Data Types",
    "text": "Loading, Exploring, and Geometric Data Types\nGeospatial data represents real-world features using three primary geometric types: - Points: Single (x,y) coordinates for discrete locations like transit stops or landmarks. - Lines: Connected sequences of points forming paths, such as roads or rivers. - Polygons: Closed shapes defining areas like census tracts or property boundaries.\n\ntransit_stops = gpd.read_file(\"data/ttc_stops.geojson\")\ntransit_routes = gpd.read_file(\"data/ttc_routes.geojson\")\ncensus_tracts = gpd.read_file(\"data/toronto_census_tract_2021.shp\")\ncensus_data = pd.read_csv(\"data/census_tract_data_sample.csv\")\n\nIn geopandas, we typically load data with a more agnostic read_file() function. For this workshop, we‚Äôre going to use four sources of data: - transit_stops: each of the stops for the TTC - transit_routes: each of the lines for the TTC - census_tracts: The geometry of Canadian census tracts within Toronto - census_data: the data of Canadian census tracts within Toronto\nLet‚Äôs take a look at the first layer, the transit stops. We have two columns with text, and a third with geometry data.\n\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nNAME\ngeometry\n\n\n\n\n0\nAvenue\nEglinton Crosstown LRT\nPOINT (-79.40851 43.70460)\n\n\n1\nForest Hill\nEglinton Crosstown LRT\nPOINT (-79.42556 43.70102)\n\n\n2\nLeaside\nEglinton Crosstown LRT\nPOINT (-79.37715 43.71105)\n\n\n3\nSloane\nEglinton Crosstown LRT\nPOINT (-79.31352 43.72597)\n\n\n4\nBirchmount\nEglinton Crosstown LRT\nPOINT (-79.27791 43.73006)\n\n\n...\n...\n...\n...\n\n\n101\nVaughan Metropolitan Centre\nToronto-York Spadina Subway Extension\nPOINT (-79.52727 43.79351)\n\n\n102\nSheppard-Yonge\nSheppard Subway\nPOINT (-79.41092 43.76151)\n\n\n103\nSpadina\nBloor-Danforth Subway\nPOINT (-79.40397 43.66728)\n\n\n104\nSt. George\nBloor-Danforth Subway\nPOINT (-79.39930 43.66827)\n\n\n105\nBloor-Yonge\nBloor-Danforth Subway\nPOINT (-79.38572 43.67100)\n\n\n\n\n106 rows √ó 3 columns\n\n\n\nThe data can be manipulated like a regular pandas data frame. For example, if we want to filter out all stations on the ‚ÄúEglinton Crosstown LRT‚Äù line, since it hasn‚Äôt been completed yet (at the time of writing), we can do so as follows:\n\ntransit_stops = transit_stops.loc[transit_stops[\"NAME\"] != \"Eglinton Crosstown LRT\"]\ntransit_stops\n\n\n\n\n\n\n\n\nLOCATION_N\nNAME\ngeometry\n\n\n\n\n25\nKipling\nBloor-Danforth Subway\nPOINT (-79.53628 43.63694)\n\n\n26\nIslington\nBloor-Danforth Subway\nPOINT (-79.52459 43.64532)\n\n\n27\nRoyal York\nBloor-Danforth Subway\nPOINT (-79.51129 43.64811)\n\n\n28\nOld Mill\nBloor-Danforth Subway\nPOINT (-79.49509 43.65007)\n\n\n29\nJane\nBloor-Danforth Subway\nPOINT (-79.48446 43.64979)\n\n\n...\n...\n...\n...\n\n\n101\nVaughan Metropolitan Centre\nToronto-York Spadina Subway Extension\nPOINT (-79.52727 43.79351)\n\n\n102\nSheppard-Yonge\nSheppard Subway\nPOINT (-79.41092 43.76151)\n\n\n103\nSpadina\nBloor-Danforth Subway\nPOINT (-79.40397 43.66728)\n\n\n104\nSt. George\nBloor-Danforth Subway\nPOINT (-79.39930 43.66827)\n\n\n105\nBloor-Yonge\nBloor-Danforth Subway\nPOINT (-79.38572 43.67100)\n\n\n\n\n81 rows √ó 3 columns\n\n\n\nWe can do the same for the transit lines data. Here the data are coded as a MULTILINESTRING, essentially a combination of lines that combine into one object. There are also MULTIPOLYGON geometry types\n\ntransit_routes = transit_routes.loc[transit_routes[\"NAME\"] != \"Eglinton Crosstown LRT\"]\ntransit_routes\n\n\n\n\n\n\n\n\nSTATUS\nTECHNOLOGY\nNAME\ngeometry\n\n\n\n\n0\nExisting\nSubway\nSheppard Subway\nMULTILINESTRING ((-79.41092 43.76151, -79.4096...\n\n\n1\nExisting\nSubway\nYonge-University-Spadina Subway\nMULTILINESTRING ((-79.46247 43.75043, -79.4621...\n\n\n2\nExisting\nSubway\nSpadina Subway Extension\nMULTILINESTRING ((-79.52727 43.79351, -79.5261...\n\n\n4\nExisting\nSubway\nScarborough RT\nMULTILINESTRING ((-79.26453 43.73226, -79.2632...\n\n\n5\nExisting\nSubway\nBloor Subway\nMULTILINESTRING ((-79.26453 43.73226, -79.2669...\n\n\n\n\n\n\n\nBefore we go on, it‚Äôs important to have an idea of the metadata of geometric files that we work with. There‚Äôs two key parts to this. - CRS (Coordinate Reference System): The crs attribute defines a geodataset‚Äôs spatial ‚Äúcoordinate system‚Äù (e.g., latitude/longitude, meters-based projections). We can use it to ensure layers align‚Äîfor example, combining Toronto census tracts (EPSG:3347) with a Web Mercator basemap (EPSG:3857). - Total Bounds: The total_bounds attribute returns the min/max coordinates (xmin, ymin, xmax, ymax) of your data‚Äôs extent. It‚Äôs useful for setting map zoom levels or clipping other datasets to the same area‚Äîlike focusing a transit map on Toronto‚Äôs downtown core.\n\ntransit_stops.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ntransit_stops.total_bounds\n\narray([-79.53627668,  43.63693527, -79.25160004,  43.79350523])"
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#basic-static-plots",
    "href": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#basic-static-plots",
    "title": "Introduction to Geopandas",
    "section": "Basic Static Plots",
    "text": "Basic Static Plots\nWe explore geometry simply by plotting using .plot(). We can do this for any row, or the entire GeoDataFrame\n\ntransit_stops.plot()\n\n\n\n\n\n\n\n\nThis is the default plot, but we can tweak the colours, add multiple layers, and change some of the layout options using matplotlib, probably the most commonly used map. Here‚Äôs a very simple schematic of rapid transit in Toronto (circa 2021)\n\nfig, ax = plt.subplots(ncols = 1, figsize=(3, 3))\n\ntransit_stops.plot(\n    color=\"Black\",\n    markersize = 6,\n    ax = ax\n)\n\ntransit_routes.plot(\n    linewidth = 1,\n    color=\"Black\",\n    ax = ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nLet‚Äôs take a look at the census tract data. Here we have polygon geometries.\n\ncensus_tracts\n\n\n\n\n\n\n\n\nid\nctuid\ndguid\nctname\nlandarea\npruid\ngeometry\n\n\n\n\n0\n487\n5350128.04\n2021S05075350128.04\n0128.04\n0.1620\n35\nPOLYGON ((629437.750 4839364.950, 629247.561 4...\n\n\n1\n502\n5350363.06\n2021S05075350363.06\n0363.06\n0.8210\n35\nPOLYGON ((640741.738 4848050.419, 640723.345 4...\n\n\n2\n506\n5350363.07\n2021S05075350363.07\n0363.07\n2.2422\n35\nPOLYGON ((642782.718 4849973.938, 642781.180 4...\n\n\n3\n508\n5350378.23\n2021S05075350378.23\n0378.23\n1.5314\n35\nPOLYGON ((639248.900 4849901.332, 639248.900 4...\n\n\n4\n509\n5350378.24\n2021S05075350378.24\n0378.24\n2.5129\n35\nPOLYGON ((639952.255 4850407.204, 639952.255 4...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n580\n5861\n5350210.04\n2021S05075350210.04\n0210.04\n0.4751\n35\nMULTIPOLYGON (((623047.314 4831182.748, 623047...\n\n\n581\n5862\n5350062.03\n2021S05075350062.03\n0062.03\n0.4638\n35\nPOLYGON ((629776.795 4835352.843, 629766.377 4...\n\n\n582\n5863\n5350062.04\n2021S05075350062.04\n0062.04\n0.1215\n35\nPOLYGON ((630319.668 4835517.832, 630149.660 4...\n\n\n583\n5864\n5350017.01\n2021S05075350017.01\n0017.01\n0.8026\n35\nPOLYGON ((633075.947 4834744.346, 633089.159 4...\n\n\n584\n5865\n5350017.02\n2021S05075350017.02\n0017.02\n0.5681\n35\nPOLYGON ((631852.697 4833570.180, 631843.913 4...\n\n\n\n\n585 rows √ó 7 columns\n\n\n\n\nfig, ax = plt.subplots(figsize=(5, 5))\n\ncensus_tracts.plot(\n    edgecolor=\"Black\",\n    color=\"White\",\n    linewidth=0.5,\n    ax = ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nLet‚Äôs try to make a choropleth map. We‚Äôll have to join in the tabular data in census_data\n\ncensus_tracts_data = census_tracts.merge(census_data, how='left', on='ctuid')\n\n\ncensus_tracts_data[\"population density\"] = census_tracts_data[\"population_2021\"] / census_tracts_data[\"landarea\"]\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\ncensus_tracts_data.to_crs(4326).plot(\n    column = \"population density\",\n    edgecolor=\"White\",\n    cmap = 'YlOrRd', \n    k = 7,\n    scheme = \"Quantiles\", \n    linewidth=0.5,\n    legend = True,\n    zorder=1,\n    legend_kwds = {\n        \"loc\": \"lower right\",\n        \"fontsize\": 7,\n        \"title\": \"Population Density\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    },\n    ax=ax\n).set_axis_off()\n\n\n\n\n\n\n\n\nWe can choose a different variable and make a map of median income. Notice that we‚Äôve also switched the color parameter cmap; just as in matplotlib generally, we can style the map as we please.\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\ncensus_tracts_data.plot(\n    column = \"median_aftertax_hhld_income_2020\",\n    edgecolor=\"Black\",\n    cmap = 'coolwarm_r', \n    k = 4,\n    scheme = \"Quantiles\", \n    linewidth=0.5,\n    legend = True,\n    legend_kwds = {\n        \"loc\": \"lower right\",\n        \"fontsize\": 7,\n        \"title\": \"Median Income\\n(after tax)\",\n        \"alignment\": \"left\",\n        \"reverse\": True\n    },\n    ax=ax\n).set_axis_off()"
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#interactive-exploration",
    "href": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#interactive-exploration",
    "title": "Introduction to Geopandas",
    "section": "Interactive Exploration",
    "text": "Interactive Exploration\nGeoPandas‚Äô explore() function generates an interactive Leaflet map (like Google Maps) from your geodata. We can use it to better understand the data we are working with and how it might be viewed from the user side on a web application (e.g., Svelte).\nYou‚Äôll need to install a couple libraries in order for this to work - matplotlib, folium, and mapclassify. This can be done in the environment that you‚Äôre working in with a command like pip install folium matplotlib mapclassify.\n\ntransit_stops.explore(\n    column='LOCATION_N',  # Popup labels\n    tiles=\"CartoDB Positron\", \n    marker_kwds={\"radius\": 5}\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#layering-multiple-datasets",
    "href": "notebooks/urban-data-analytics/intro-geopandas/intro-geopandas.html#layering-multiple-datasets",
    "title": "Introduction to Geopandas",
    "section": "Layering Multiple Datasets",
    "text": "Layering Multiple Datasets\nLayering lets you combine different geographic datasets (like roads on top of neighborhoods) to reveal spatial relationships. In GeoPandas, each plot() call adds a new visual layer, with zorder controlling which appears on top. These plots are highly customizable, as we see below.\n\n# Initialize the canvas (all layers will share this axis)\nfig, ax = plt.subplots(figsize=(7, 7))  \n\n# Layer 1: Transit routes (black lines on top, zorder=3)\ntransit_routes.to_crs(4326).plot(\n    linewidth=1,        \n    color=\"Black\",      \n    ax=ax,              # Draw on our shared axis\n    zorder=3            # Top layer (overlaps others)\n)\n\n# Layer 2: Transit stops (black dots below routes, zorder=2)\ntransit_stops.to_crs(4326).plot(\n    color=\"Black\",      \n    markersize=16,      \n    ax=ax,              # Same shared axis\n    zorder=2            # Middle layer (above tracts)\n)\n\n# Layer 3: Census tracts (colored by density, zorder=1)\ncensus_tracts_data.to_crs(4326).plot(\n    column=\"population density\",  \n    edgecolor=\"White\",            \n    cmap='YlOrRd',                \n    k=7,                          \n    scheme=\"Quantiles\",           \n    linewidth=0.5,                \n    legend=True,                  \n    zorder=1,                     # Bottom layer\n    legend_kwds={\n        \"loc\": \"lower right\",     \n        \"fontsize\": 7,            \n        \"title\": \"Population Density\",  \n        \"alignment\": \"left\",      \n        \"reverse\": True           \n    },\n    ax=ax\n).set_axis_off()  # Hide distracting x/y axes"
  },
  {
    "objectID": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html",
    "href": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "This page covers 1) common metrics and indicators used in urban analysis, 2) spatial units and measures of aggregation often used for urban analysis, and 3) limitations and biases for working with urban datasets to keep in mind when working with urban data.\n\n\nThe table below lists examples of variables/metrics that are used in urban analyses, grouped by topic.\n\n\n\nTopic\nCommon metrics\n\n\n\n\nPeople / socioeconomics\n- Population density (e.g., population per square kilometer)- Median income- Race/ethnicity- Displacement risk- Social vulnerability index- Population change- Inequality metrics (e.g., Gini index - US & Canada)- Crime rate- Education level (e.g., % with bachelor‚Äôs degree, master‚Äôs degree)- Dissimilarity index of racial segregation- Voting patterns (e.g., in Toronto)\n\n\nHousing\n- % owners/renters- Median rent & home value- Vacancy rate- Cost burden- Core housing need- Number of units built or permits issued (e.g., Canadian ADU analysis; market-rate housing in the Bay Area)\n\n\nLand use\n- Walk score- Entropy index for land use mix- Change in land cover (or forest)- Zoning (e.g., zoning policy changes; residential zoning in Canadian cities)\n\n\nEconomics / employment\nJob density (e.g., Longitudinal Employer-Household Dynamics in the US; Canadian Employer-Employee Dynamics Database in Canada)- Commuting patterns- Venture capital investment by city- Sales by sector downtown- Downtown recovery post-pandemic (e.g., trends; Urban Activity Atlas)\n\n\nTransportation\n- Commute mode (e.g., % of people by census tract who commute via public transit vs car)- Public transit accessibility (e.g., ‚Äúwalkshed‚Äù around transit stations; % of population within given distance of rail station; population near individual transit stations)- E-bike trip distance- Bike share trips in Toronto- Traffic violations\n\n\nEnvironment\n- Emissions/air pollution)- Urban heat (e.g., heat exposure in Toronto)- Greenness)- Park access (e.g., distance to nearest park)- Flood maps (e.g., Toronto; FEMA; Canada)- Tree canopy coverage\n\n\nHealth\n- Life expectancy- Prevalence of chronic diseases (e.g., diabetes, asthma)\n\n\n\n\n\n\nUrban data is often linked to specific places. This is often called spatial or geographic data.\nWhen analyzing spatial data, our analysis is often at the level of specific spatial units or unit of aggregations. Sometimes our data is directly collected at these units, while sometimes it is useful to aggregate large datasets to these units to help analyses and visualizations. Below are spatial units and types of encoding that are often used for collecting and analyzing urban data.\nIn our notebook on spatial data and GIS, we go into details on how different spatial data is structured, and how we can begin to view, explore, and analyze different data in GIS.\n\n\nPolitical boundaries that delineate jurisdictions for different levels of government, from national down to local levels. Countries, provinces or states, counties, municipalities, electoral districts or city wards, and within cities, neighbourhood planning areas, are all examples of commonly used spatial units.\n\n\n\nFederal electoral district in Saskatoon\n\n\n\n\n\nNational censuses aggregate data to a variety of spatial units ranging in size, many are the same as administrative and political boundaries, as well as many smaller-geography boundaries that are super useful for urban- and neighbourhood-scale maps and analyses. Census tracts (usually in the range of 2,500 and 8,000 persons) and Dissemination Areas (400 to 700 persons) are two scales that are often used. Check out (Statistics Canada documentation) or see our notebook on Canadian census data for more information.\n\n\n\nCommon census boundaries in Toronto\n\n\n\n\n\nA grid is repetitive tesselation spread across the surface of a map. Grids are used in spatial analysis when existing boundaries are unavailable, unsuitable, or when evenly sized, uniform areas are required. Geohashes, which uniquely identify specific regions according to their latitude and longitude everywhere on Earth, are one type of commonly used grid. Grids do not have to 4-sided.\n\n\n\nGeohashes in Qu√©bec City (source)\n\n\nTriangular and hexagonal grids are often used for some studies. Hexagon‚Äôs are often recommended since they are the regular polygon with the most sides (i.e.¬†can closest represent a circle), that can tesselate without any gaps.\n\n\n\nScreenshot of a map showing density of activity in Glasgow\n\n\n\n\n\nWhile streets are often added to maps to provide geographic context, streets can also be their own unit of analysis. For example, traffic flow could be measured on street segments throughout a city. In the image below, streets are coloured by how many parking tickets that they have.\n\n\n\nParking tickets in Toronto (source)\n\n\n\n\n\nSome urban data is measured or collected at the address level. For example, address of businesses, non-profits, or community facilities. To map them and compare with other spatial data, addresses are often geocoded, where their names are converted into geographic coordinates (latitude and longitude). See our Spatial data and GIS tutorial for more.\n\n\n\nGeocoded addresses of businesses in Mississauga from the Canadian Urban Institutes Measuring Main Streets project\n\n\n\n\n\n\nWhen collecting and analyzing data, it is important to verify the quality of the dataset. Some data is incomplete or has missing values, which can bias the results, especially if data from certain categories is missing disproportionately. For example, if income data is missing more often for lower-income individuals, the results may overestimate average income and under-represent vulnerable populations.\nThese are a few important sources of bias or limitations when working with spatial data that are super important to be aware of when working with data linked to places.\n\nSelf-reporting bias which is when individuals report inaccurate information about themselves in a survey. This can be intentional (e.g., under-reporting income or over-reporting education) or unintentional (e.g., forgetting details). This can lead to biases in the final dataset and any subsequent analysis.\nEcological fallacy is the phenomenon of drawing conclusions about individuals based on the group they belong to. For example, one might infer that everyone in a census tract with an overall high median income is wealthy. Although the median income is high, there may be low income residents who live in the tract who are not close to the median.\n[Edge effects] in spatial analysis refer to the limitations or distortions that occur at the boundaries of a study area. They can bias results or reduce accuracy, especially when spatial patterns or processes extend beyond the area being analyzed.. For example, let‚Äôs say you were mapping access to healthy food in a city. Your map may show that one corner of your city does not have a grocery store, leading to a conclusion of it being a food dessert. But if you didn‚Äôt consider grocery stores just outside the edge or boundary of your city adjacent to this corner, this may not be the case.\nModifiable areal unit problem (MAUP) is another source of bias when working with spatial data. It is a form of statistical bias that results from the fact that changing the scale or shape of aggregation units leads to different results. Gerrymandering is a classic example of intentional MAUP to obtain specific voting outcomes. Check out the graphic below, we can see the results of how different spatial units are arranged would impact the overall results of an election. Overall, it is important to think critically when working with different spatial units to avoid misrepresenting data or cherry-picking results.\n\n\n\n\nGerrymandering example",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#common-metrics-and-indicators-for-urban-analyses",
    "href": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#common-metrics-and-indicators-for-urban-analyses",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "The table below lists examples of variables/metrics that are used in urban analyses, grouped by topic.\n\n\n\nTopic\nCommon metrics\n\n\n\n\nPeople / socioeconomics\n- Population density (e.g., population per square kilometer)- Median income- Race/ethnicity- Displacement risk- Social vulnerability index- Population change- Inequality metrics (e.g., Gini index - US & Canada)- Crime rate- Education level (e.g., % with bachelor‚Äôs degree, master‚Äôs degree)- Dissimilarity index of racial segregation- Voting patterns (e.g., in Toronto)\n\n\nHousing\n- % owners/renters- Median rent & home value- Vacancy rate- Cost burden- Core housing need- Number of units built or permits issued (e.g., Canadian ADU analysis; market-rate housing in the Bay Area)\n\n\nLand use\n- Walk score- Entropy index for land use mix- Change in land cover (or forest)- Zoning (e.g., zoning policy changes; residential zoning in Canadian cities)\n\n\nEconomics / employment\nJob density (e.g., Longitudinal Employer-Household Dynamics in the US; Canadian Employer-Employee Dynamics Database in Canada)- Commuting patterns- Venture capital investment by city- Sales by sector downtown- Downtown recovery post-pandemic (e.g., trends; Urban Activity Atlas)\n\n\nTransportation\n- Commute mode (e.g., % of people by census tract who commute via public transit vs car)- Public transit accessibility (e.g., ‚Äúwalkshed‚Äù around transit stations; % of population within given distance of rail station; population near individual transit stations)- E-bike trip distance- Bike share trips in Toronto- Traffic violations\n\n\nEnvironment\n- Emissions/air pollution)- Urban heat (e.g., heat exposure in Toronto)- Greenness)- Park access (e.g., distance to nearest park)- Flood maps (e.g., Toronto; FEMA; Canada)- Tree canopy coverage\n\n\nHealth\n- Life expectancy- Prevalence of chronic diseases (e.g., diabetes, asthma)",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#common-spatial-units-and-measures-of-aggregation",
    "href": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#common-spatial-units-and-measures-of-aggregation",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "Urban data is often linked to specific places. This is often called spatial or geographic data.\nWhen analyzing spatial data, our analysis is often at the level of specific spatial units or unit of aggregations. Sometimes our data is directly collected at these units, while sometimes it is useful to aggregate large datasets to these units to help analyses and visualizations. Below are spatial units and types of encoding that are often used for collecting and analyzing urban data.\nIn our notebook on spatial data and GIS, we go into details on how different spatial data is structured, and how we can begin to view, explore, and analyze different data in GIS.\n\n\nPolitical boundaries that delineate jurisdictions for different levels of government, from national down to local levels. Countries, provinces or states, counties, municipalities, electoral districts or city wards, and within cities, neighbourhood planning areas, are all examples of commonly used spatial units.\n\n\n\nFederal electoral district in Saskatoon\n\n\n\n\n\nNational censuses aggregate data to a variety of spatial units ranging in size, many are the same as administrative and political boundaries, as well as many smaller-geography boundaries that are super useful for urban- and neighbourhood-scale maps and analyses. Census tracts (usually in the range of 2,500 and 8,000 persons) and Dissemination Areas (400 to 700 persons) are two scales that are often used. Check out (Statistics Canada documentation) or see our notebook on Canadian census data for more information.\n\n\n\nCommon census boundaries in Toronto\n\n\n\n\n\nA grid is repetitive tesselation spread across the surface of a map. Grids are used in spatial analysis when existing boundaries are unavailable, unsuitable, or when evenly sized, uniform areas are required. Geohashes, which uniquely identify specific regions according to their latitude and longitude everywhere on Earth, are one type of commonly used grid. Grids do not have to 4-sided.\n\n\n\nGeohashes in Qu√©bec City (source)\n\n\nTriangular and hexagonal grids are often used for some studies. Hexagon‚Äôs are often recommended since they are the regular polygon with the most sides (i.e.¬†can closest represent a circle), that can tesselate without any gaps.\n\n\n\nScreenshot of a map showing density of activity in Glasgow\n\n\n\n\n\nWhile streets are often added to maps to provide geographic context, streets can also be their own unit of analysis. For example, traffic flow could be measured on street segments throughout a city. In the image below, streets are coloured by how many parking tickets that they have.\n\n\n\nParking tickets in Toronto (source)\n\n\n\n\n\nSome urban data is measured or collected at the address level. For example, address of businesses, non-profits, or community facilities. To map them and compare with other spatial data, addresses are often geocoded, where their names are converted into geographic coordinates (latitude and longitude). See our Spatial data and GIS tutorial for more.\n\n\n\nGeocoded addresses of businesses in Mississauga from the Canadian Urban Institutes Measuring Main Streets project",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#biases-and-limitations-of-spatial-data",
    "href": "notebooks/urban-data-analytics/measuring-the-city/measuring-the-city.html#biases-and-limitations-of-spatial-data",
    "title": "Measuring the city: metrics and indicators",
    "section": "",
    "text": "When collecting and analyzing data, it is important to verify the quality of the dataset. Some data is incomplete or has missing values, which can bias the results, especially if data from certain categories is missing disproportionately. For example, if income data is missing more often for lower-income individuals, the results may overestimate average income and under-represent vulnerable populations.\nThese are a few important sources of bias or limitations when working with spatial data that are super important to be aware of when working with data linked to places.\n\nSelf-reporting bias which is when individuals report inaccurate information about themselves in a survey. This can be intentional (e.g., under-reporting income or over-reporting education) or unintentional (e.g., forgetting details). This can lead to biases in the final dataset and any subsequent analysis.\nEcological fallacy is the phenomenon of drawing conclusions about individuals based on the group they belong to. For example, one might infer that everyone in a census tract with an overall high median income is wealthy. Although the median income is high, there may be low income residents who live in the tract who are not close to the median.\n[Edge effects] in spatial analysis refer to the limitations or distortions that occur at the boundaries of a study area. They can bias results or reduce accuracy, especially when spatial patterns or processes extend beyond the area being analyzed.. For example, let‚Äôs say you were mapping access to healthy food in a city. Your map may show that one corner of your city does not have a grocery store, leading to a conclusion of it being a food dessert. But if you didn‚Äôt consider grocery stores just outside the edge or boundary of your city adjacent to this corner, this may not be the case.\nModifiable areal unit problem (MAUP) is another source of bias when working with spatial data. It is a form of statistical bias that results from the fact that changing the scale or shape of aggregation units leads to different results. Gerrymandering is a classic example of intentional MAUP to obtain specific voting outcomes. Check out the graphic below, we can see the results of how different spatial units are arranged would impact the overall results of an election. Overall, it is important to think critically when working with different spatial units to avoid misrepresenting data or cherry-picking results.\n\n\n\n\nGerrymandering example",
    "crumbs": [
      "Urban Data Analytics",
      "Measuring the city: metrics and indicators"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html",
    "title": "Data visualization",
    "section": "",
    "text": "Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools help people understand patterns, trends, and outliers in data. At its core, data visualization translates abstract numbers into something visible and intuitive, helping our audiences better understand what the data is telling us.\nWe live in a world increasingly shaped by data ‚Äì from climate change and public health to business performance and social media behavior ‚Äì however, raw numbers alone are difficult to interpret. Visualizations help bridge that gap by making data more accessible, understandable, and actionable. It‚Äôs not just about making things look good, it‚Äôs about making data make sense.\n‚ÄúThe simple graph has brought more information to the data analyst‚Äôs mind than any other device. It specializes in providing indications of unexpected phenomena.‚Äù ‚Äì John Tukey\n\n\nWe visualize data to help us understand it better. When we turn numbers into pictures like charts or graphs, it becomes easier to see patterns, trends, or problems. Broadly speaking, visualization can serve two major goals:\n\nExploration ‚Äì Using charts to help make sense of the data. It helps you find interesting things or answer questions\nCommunication ‚Äì Using visuals to help other people understand what you found in the data\n\nThese goals are not mutually exclusive. The best data work is iterative. We often begin by visualizing data to explore patterns, test ideas, and uncover insights. Once we have made sense of the data ourselves, we can use visualizations to communicate key findings, tell a compelling story, or make a case for action. Whether the goal is understanding, persuasion, or advocacy, good data visualization helps bridge the gap between raw information and meaningful insight.\nVisualizing data is ultimately about communication and striking the right balance between analysis, design, and narrative. To share data in a clear and meaningful way, it helps to think through this data storytelling framework:\n\n\n\nFramework for data storytelling\n\n\n\nAudience & Objectives: Who is this for? What do they need to know or do with the information?\nData Analysis: What does the data tell us? What patterns or relationships matter?\nInformation Design: How should this data be visually structured for clarity?\nWriting & Narrative: What story does the data tell? What‚Äôs your argument or takeaway?\n\nThis framework helps situate visualization as part of a broader storytelling or decision-making process. Whether you are creating a data story with multiple visuals or a single chart for a report, it is important to keep all of these things in mind. The purpose behind your visualization determines everything from the chart type you use to the level of detail, annotation, and tone you adopt.\n\n\n\nBefore we share our findings with others, we often need to make sense of the data ourselves. This is where exploratory data analysis comes in. This is the process of visually investigating datasets to uncover trends, spot anomalies, test hypotheses, and surface insights that might otherwise be hidden in raw numbers or tables.\nVisualization during this stage can reveal unexpected patterns and outliers, correlations and clusters, distribution shapes, and data quality issues.\nIt also helps uncover relationships that summary statistics alone might obscure. The exploratory process is typically iterative and often messy, with charts not needing to be beautiful, but instead, informative. Quick plots, heatmaps, and scatterplots are all useful tools in this phase, enabling a deeper understanding of the data before any formal analysis or presentation.\nEven datasets with identical statistical properties can convey vastly different stories when visualized. A perfect example of this is Anscombe‚Äôs Quartet (Figure 2), a set of four datasets that have nearly identical means, variances, and correlation coefficients, yet their graphical representations reveal dramatic differences. This illustrates the importance of visualizing data, rather than solely relying on summary statistics, as the true patterns and relationships may be hidden within the numbers themselves.\n\n\n\nAnsecombe‚Äôs Quartet, for datasets with almost identical summary statistics, but widely different patterns when charted\n\n\n\n\n\nOnce we‚Äôve explored the data and identified key findings, the next step is to share these insights with others. Data visualization becomes a powerful tool for communication, helping us inform, engage, and persuade different audiences. In this context, visualization is not just about showing data, it‚Äôs about shaping understanding and sometimes even inspiring action.\nData stories combine charts and text to walk the audience through a narrative arc. These stories often include:\n\nContext ‚Äì What‚Äôs the issue or question?\nData ‚Äì What patterns or evidence are we seeing?\nInsight ‚Äì Why does it matter?\nAction ‚Äì What should happen next?\n\nGood data visualizations and stories often zoom in and out to show both the big picture and key details while guiding the audience with clear visual hierarchy and annotations.\nIn advocacy contexts, data visualizations are used to support arguments, influence policy, and raise awareness. The visual design here should support the clarity and urgency of the message while still being transparent and truthful.\n\n\nNot all data visualizations are created for the same audience or goal. Some are meant to be read in detail (like charts in a policy report), while others are intended to create an emotional impact at a glance (like a billboard or social media graphic).\nFor example, a line chart depicting global temperatures over time provides nuance and precision, making it an excellent choice for scientists and analysts.\n\n\n\nGlobal Temperatures Over Time (Source: NASA/GISS)\n\n\nAlternatively, warming stripes, a minimalist visualization uses a simple color gradient to represent temperature changes. This approach communicates the same pattern but in a more striking and emotionally impactful way, making it highly effective for public engagement and advocacy.\n\n\n\nWarming strips\n\n\n\n\n\n\nA well-designed visualization isn‚Äôt just about making data look good ‚Äì it‚Äôs about making it understandable, accurate, and accessible. This often requires a base understanding of how we encode data visually, how the human brain processes that information, and how to make charts function well across different audiences and mediums.\n\n\nAt the heart of every chart or graph is visual encoding: the transformation of data into visual elements (Figure 5). Each variable in your dataset must be ‚Äúmapped‚Äù to a visual channel, often called a visual variable. The effectiveness of these visual encodings depends on what type of data you‚Äôre working with.\n\n\n\nBertin‚Äôs Visual Variables via Axis Maps\n\n\n\n\n\nEffective data visualization leverages our brain‚Äôs innate ability to rapidly process certain visual elements. This phenomenon, known as preattentive processing, allows viewers to quickly and effortlessly identify certain patterns, contrasts, and structures without conscious effort.\nThis allows us to instantly spot outliers, notice trends and clusters, and recognize visual hierarchies.\nKey preattentive features include color, form, orientation, and size. For example, we can quickly distinguish a red circle among blue circles (color) and identify a square amidst a group of circles (form). These features are processed so swiftly and effortlessly that they can influence our perception and understanding of visual information without conscious effort.\n\n\n\nExample of preattentive processing. Source: www.csc2.ncsu.edu/faculty/healey/PP/\n\n\nEven when designing a table for a slide deck or a report, you can use the theory of preattentive processing to pick a visual variable to focus your readers attention on specific data points that are important to your story.\n\n\n\nExample of preattentive processing to highlight certain numbers in a table via different shading. Source: Better Data Visualizations (Schwabish, 2021)\n\n\nVia preattentive processing, we can guide viewers‚Äô attention efficiently, enhancing comprehension and reducing cognitive load.\n\n\n\n**Perceptual Rankings* help explain why some encodings are easier to interpret than others. Not all visual variables are equally effective.\nYou may have noticed that in the scatter-plot example in the previous section, that it‚Äôs generally easier to pick out a red circle among a group of blue circles rather than a blue square among a group of blue circles.\nResearch has established a hierarchy of visual channels based on their accuracy and ease of interpretation:\n\n\n\nPerceptual Rankings (Source: Visualization Analysis and Design by Tamara Munzner)\n\n\nUnderstanding these perceptual principles ensures that visualizations communicate information clearly and intuitively, aligning with how viewers naturally process visual stimuli.\nFor example, based on this research, position on a common scale is easier to interpret than size or area for quantitative data. Comparing volumes is also relatively difficult. This leads to two recommendations‚Ä¶\n\nAvoid pie charts (especially for more 3 categories) since they can be less effective than alternatives (e.g.¬†bar charts)\nAvoid 3D charts in most cases because they tend to over-complicate, be difficult to read, and add extra visual clutter compared to 2D alternatives\n\n\n\n\nYour data visualizations should be readable by everyone, which means thinking beyond aesthetics and into the realm of inclusive design.\nUse color vision deficiency-safe palettes, such as those available on ColorBrewer, and avoid relying solely on color to distinguish elements.\nIncorporate texture, thickness, shape, or labels where possible. Additionally, ensure sufficient contrast between the foreground and background, as light grey text on white or red-on-green combinations are problematic for readability. The greater the contrast between the foreground and background, the easier it is to read. For text, keep in mind that font size plays a significant role: smaller fonts require even higher contrast between the text color and background to maintain readability.\n\n\n\n\nStrong visualizations depend not just on the data and encodings, but on thoughtful framing. The supporting elements of a chart provide clarity and context.\n\n\n\nA strong visualization can make complex data accessible; however, without care, the message can get lost. Great visualizations balance clarity, precision, and aesthetics. This section offers practical tips and guiding principles to elevate your data visualizations.\nNote that these are general recommendations and rules of thumb, not rules that you must follow 100% of the time! data visualization is a combination of technical, design, and artistic skills, and there are often exceptions to the rules :)\n\n\nGuide the viewer‚Äôs eye by creating a hierarchy between background and foreground. Your key message should be the focus and highlighted while everything else (axes, grid-lines, background elements) should support and not compete with it.\n\nBold or highlight key data points.\nUse size, contrast, and color to signal importance.\nDe-emphasize secondary elements like gridlines, minor tick marks, or axis labels\n\nThis is very similar to journalistic styles of writing, ‚Äúdon‚Äôt burry the lead‚Äù and ‚Äúbottom line up-front‚Äù\nWhen guiding your readers through a story with a series of visualizations, sometimes it is useful to follow a Data Visualization Sandwich metaphor\nLet‚Äôs look at an example, this is a map of the United States from an article by the Guardian on how the USA is facing above average rises in temperatures. This map acts as the Patty ‚Äì it draws the attention of the reader and introduces the topic of the article.\n\nThe article then takes a deeper look at specific counties that have experienced significant temperature increases via a table\n\nFinally, this visual, which could be considered a patty or bun, helps illustrate the temperatures across U.S. states. It also includes a ‚Äútopping‚Äù ‚Äì an annotation over California that highlights how the entire population has experienced a temperature increase since 1895.\n\n\n\n\nCoined by Edward Tufte, the data-ink ratio refers to the proportion of visual elements that represent actual data, rather than decoration, relative to all ‚Äòink‚Äô on the chart.\nIn other words, this is about reducing clutter and focusing graphical elements of a chart on the data.\n\n\n\nLeft: Low data-ink ratio / Right: High data-ink ratio\n\n\nHere are a few recommendations for reducing clutter and increasing data-ink ratios:\n\nRemove non-essential ‚Äúchart-junk‚Äù (3D effects, background shading, unnecessary gridlines, borders).\nUse direct labels instead of relying on legends.\nChoose simple chart types unless complexity is truly needed.\nUse subtle formatting of reference information like gridlines to keep the audience focused on the data itself.\nAvoid overuse of colors, labels, and gridlines.\nDon‚Äôt always need to visualize every possible variable, focus on the story you‚Äôre trying to tell.\nGroup or collapse less important data to simplify interpretation.\nWhitespace is your friend\n\n\n\n\n\n\n\nIf you‚Äôre creating visualizations within an organization or for a specific campaign, it‚Äôs important to follow established brand guidelines.\nThis includes using approved typefaces, colors, and logos, and aligning your visuals with the tone and messaging of the broader content.\nMaintaining visual consistency across all charts, graphics, and reports reinforces brand identity, leads to more cohesive products, and builds recognition with your audience.\n\n\n\nGreat charts don‚Äôt always speak for themselves. Use clear titles, subtitles, and axis labels. Add callouts or annotations to highlight patterns or anomalies. You can guide the reader to your takeaway rather than leaving it to interpretation.\n\n\n\nExample of how simple text labels and titles can help tell an effective visual story (Source: Wall Street Journal 2015)\n\n\n\n\n\nMatch the visualization to the type of data and the message you‚Äôre trying to convey.\nand 3D charts in most cases because they tend to overcomplicate\nor be difficult to accurately perceive. Below are some uses for various basic charts.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#why-visualize-data",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#why-visualize-data",
    "title": "Data visualization",
    "section": "",
    "text": "We visualize data to help us understand it better. When we turn numbers into pictures like charts or graphs, it becomes easier to see patterns, trends, or problems. Broadly speaking, visualization can serve two major goals:\n\nExploration ‚Äì Using charts to help make sense of the data. It helps you find interesting things or answer questions\nCommunication ‚Äì Using visuals to help other people understand what you found in the data\n\nThese goals are not mutually exclusive. The best data work is iterative. We often begin by visualizing data to explore patterns, test ideas, and uncover insights. Once we have made sense of the data ourselves, we can use visualizations to communicate key findings, tell a compelling story, or make a case for action. Whether the goal is understanding, persuasion, or advocacy, good data visualization helps bridge the gap between raw information and meaningful insight.\nVisualizing data is ultimately about communication and striking the right balance between analysis, design, and narrative. To share data in a clear and meaningful way, it helps to think through this data storytelling framework:\n\n\n\nFramework for data storytelling\n\n\n\nAudience & Objectives: Who is this for? What do they need to know or do with the information?\nData Analysis: What does the data tell us? What patterns or relationships matter?\nInformation Design: How should this data be visually structured for clarity?\nWriting & Narrative: What story does the data tell? What‚Äôs your argument or takeaway?\n\nThis framework helps situate visualization as part of a broader storytelling or decision-making process. Whether you are creating a data story with multiple visuals or a single chart for a report, it is important to keep all of these things in mind. The purpose behind your visualization determines everything from the chart type you use to the level of detail, annotation, and tone you adopt.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-exploratory-analysis",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-exploratory-analysis",
    "title": "Data visualization",
    "section": "",
    "text": "Before we share our findings with others, we often need to make sense of the data ourselves. This is where exploratory data analysis comes in. This is the process of visually investigating datasets to uncover trends, spot anomalies, test hypotheses, and surface insights that might otherwise be hidden in raw numbers or tables.\nVisualization during this stage can reveal unexpected patterns and outliers, correlations and clusters, distribution shapes, and data quality issues.\nIt also helps uncover relationships that summary statistics alone might obscure. The exploratory process is typically iterative and often messy, with charts not needing to be beautiful, but instead, informative. Quick plots, heatmaps, and scatterplots are all useful tools in this phase, enabling a deeper understanding of the data before any formal analysis or presentation.\nEven datasets with identical statistical properties can convey vastly different stories when visualized. A perfect example of this is Anscombe‚Äôs Quartet (Figure 2), a set of four datasets that have nearly identical means, variances, and correlation coefficients, yet their graphical representations reveal dramatic differences. This illustrates the importance of visualizing data, rather than solely relying on summary statistics, as the true patterns and relationships may be hidden within the numbers themselves.\n\n\n\nAnsecombe‚Äôs Quartet, for datasets with almost identical summary statistics, but widely different patterns when charted",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-communication-and-storytelling",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-for-communication-and-storytelling",
    "title": "Data visualization",
    "section": "",
    "text": "Once we‚Äôve explored the data and identified key findings, the next step is to share these insights with others. Data visualization becomes a powerful tool for communication, helping us inform, engage, and persuade different audiences. In this context, visualization is not just about showing data, it‚Äôs about shaping understanding and sometimes even inspiring action.\nData stories combine charts and text to walk the audience through a narrative arc. These stories often include:\n\nContext ‚Äì What‚Äôs the issue or question?\nData ‚Äì What patterns or evidence are we seeing?\nInsight ‚Äì Why does it matter?\nAction ‚Äì What should happen next?\n\nGood data visualizations and stories often zoom in and out to show both the big picture and key details while guiding the audience with clear visual hierarchy and annotations.\nIn advocacy contexts, data visualizations are used to support arguments, influence policy, and raise awareness. The visual design here should support the clarity and urgency of the message while still being transparent and truthful.\n\n\nNot all data visualizations are created for the same audience or goal. Some are meant to be read in detail (like charts in a policy report), while others are intended to create an emotional impact at a glance (like a billboard or social media graphic).\nFor example, a line chart depicting global temperatures over time provides nuance and precision, making it an excellent choice for scientists and analysts.\n\n\n\nGlobal Temperatures Over Time (Source: NASA/GISS)\n\n\nAlternatively, warming stripes, a minimalist visualization uses a simple color gradient to represent temperature changes. This approach communicates the same pattern but in a more striking and emotionally impactful way, making it highly effective for public engagement and advocacy.\n\n\n\nWarming strips",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-theory",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#data-visualization-theory",
    "title": "Data visualization",
    "section": "",
    "text": "A well-designed visualization isn‚Äôt just about making data look good ‚Äì it‚Äôs about making it understandable, accurate, and accessible. This often requires a base understanding of how we encode data visually, how the human brain processes that information, and how to make charts function well across different audiences and mediums.\n\n\nAt the heart of every chart or graph is visual encoding: the transformation of data into visual elements (Figure 5). Each variable in your dataset must be ‚Äúmapped‚Äù to a visual channel, often called a visual variable. The effectiveness of these visual encodings depends on what type of data you‚Äôre working with.\n\n\n\nBertin‚Äôs Visual Variables via Axis Maps\n\n\n\n\n\nEffective data visualization leverages our brain‚Äôs innate ability to rapidly process certain visual elements. This phenomenon, known as preattentive processing, allows viewers to quickly and effortlessly identify certain patterns, contrasts, and structures without conscious effort.\nThis allows us to instantly spot outliers, notice trends and clusters, and recognize visual hierarchies.\nKey preattentive features include color, form, orientation, and size. For example, we can quickly distinguish a red circle among blue circles (color) and identify a square amidst a group of circles (form). These features are processed so swiftly and effortlessly that they can influence our perception and understanding of visual information without conscious effort.\n\n\n\nExample of preattentive processing. Source: www.csc2.ncsu.edu/faculty/healey/PP/\n\n\nEven when designing a table for a slide deck or a report, you can use the theory of preattentive processing to pick a visual variable to focus your readers attention on specific data points that are important to your story.\n\n\n\nExample of preattentive processing to highlight certain numbers in a table via different shading. Source: Better Data Visualizations (Schwabish, 2021)\n\n\nVia preattentive processing, we can guide viewers‚Äô attention efficiently, enhancing comprehension and reducing cognitive load.\n\n\n\n**Perceptual Rankings* help explain why some encodings are easier to interpret than others. Not all visual variables are equally effective.\nYou may have noticed that in the scatter-plot example in the previous section, that it‚Äôs generally easier to pick out a red circle among a group of blue circles rather than a blue square among a group of blue circles.\nResearch has established a hierarchy of visual channels based on their accuracy and ease of interpretation:\n\n\n\nPerceptual Rankings (Source: Visualization Analysis and Design by Tamara Munzner)\n\n\nUnderstanding these perceptual principles ensures that visualizations communicate information clearly and intuitively, aligning with how viewers naturally process visual stimuli.\nFor example, based on this research, position on a common scale is easier to interpret than size or area for quantitative data. Comparing volumes is also relatively difficult. This leads to two recommendations‚Ä¶\n\nAvoid pie charts (especially for more 3 categories) since they can be less effective than alternatives (e.g.¬†bar charts)\nAvoid 3D charts in most cases because they tend to over-complicate, be difficult to read, and add extra visual clutter compared to 2D alternatives\n\n\n\n\nYour data visualizations should be readable by everyone, which means thinking beyond aesthetics and into the realm of inclusive design.\nUse color vision deficiency-safe palettes, such as those available on ColorBrewer, and avoid relying solely on color to distinguish elements.\nIncorporate texture, thickness, shape, or labels where possible. Additionally, ensure sufficient contrast between the foreground and background, as light grey text on white or red-on-green combinations are problematic for readability. The greater the contrast between the foreground and background, the easier it is to read. For text, keep in mind that font size plays a significant role: smaller fonts require even higher contrast between the text color and background to maintain readability.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#chart-components",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#chart-components",
    "title": "Data visualization",
    "section": "",
    "text": "Strong visualizations depend not just on the data and encodings, but on thoughtful framing. The supporting elements of a chart provide clarity and context.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  },
  {
    "objectID": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#practical-tips-for-effective-data-visualization",
    "href": "notebooks/urban-data-visualization/data-visualization/data-visualization.html#practical-tips-for-effective-data-visualization",
    "title": "Data visualization",
    "section": "",
    "text": "A strong visualization can make complex data accessible; however, without care, the message can get lost. Great visualizations balance clarity, precision, and aesthetics. This section offers practical tips and guiding principles to elevate your data visualizations.\nNote that these are general recommendations and rules of thumb, not rules that you must follow 100% of the time! data visualization is a combination of technical, design, and artistic skills, and there are often exceptions to the rules :)\n\n\nGuide the viewer‚Äôs eye by creating a hierarchy between background and foreground. Your key message should be the focus and highlighted while everything else (axes, grid-lines, background elements) should support and not compete with it.\n\nBold or highlight key data points.\nUse size, contrast, and color to signal importance.\nDe-emphasize secondary elements like gridlines, minor tick marks, or axis labels\n\nThis is very similar to journalistic styles of writing, ‚Äúdon‚Äôt burry the lead‚Äù and ‚Äúbottom line up-front‚Äù\nWhen guiding your readers through a story with a series of visualizations, sometimes it is useful to follow a Data Visualization Sandwich metaphor\nLet‚Äôs look at an example, this is a map of the United States from an article by the Guardian on how the USA is facing above average rises in temperatures. This map acts as the Patty ‚Äì it draws the attention of the reader and introduces the topic of the article.\n\nThe article then takes a deeper look at specific counties that have experienced significant temperature increases via a table\n\nFinally, this visual, which could be considered a patty or bun, helps illustrate the temperatures across U.S. states. It also includes a ‚Äútopping‚Äù ‚Äì an annotation over California that highlights how the entire population has experienced a temperature increase since 1895.\n\n\n\n\nCoined by Edward Tufte, the data-ink ratio refers to the proportion of visual elements that represent actual data, rather than decoration, relative to all ‚Äòink‚Äô on the chart.\nIn other words, this is about reducing clutter and focusing graphical elements of a chart on the data.\n\n\n\nLeft: Low data-ink ratio / Right: High data-ink ratio\n\n\nHere are a few recommendations for reducing clutter and increasing data-ink ratios:\n\nRemove non-essential ‚Äúchart-junk‚Äù (3D effects, background shading, unnecessary gridlines, borders).\nUse direct labels instead of relying on legends.\nChoose simple chart types unless complexity is truly needed.\nUse subtle formatting of reference information like gridlines to keep the audience focused on the data itself.\nAvoid overuse of colors, labels, and gridlines.\nDon‚Äôt always need to visualize every possible variable, focus on the story you‚Äôre trying to tell.\nGroup or collapse less important data to simplify interpretation.\nWhitespace is your friend\n\n\n\n\n\n\n\nIf you‚Äôre creating visualizations within an organization or for a specific campaign, it‚Äôs important to follow established brand guidelines.\nThis includes using approved typefaces, colors, and logos, and aligning your visuals with the tone and messaging of the broader content.\nMaintaining visual consistency across all charts, graphics, and reports reinforces brand identity, leads to more cohesive products, and builds recognition with your audience.\n\n\n\nGreat charts don‚Äôt always speak for themselves. Use clear titles, subtitles, and axis labels. Add callouts or annotations to highlight patterns or anomalies. You can guide the reader to your takeaway rather than leaving it to interpretation.\n\n\n\nExample of how simple text labels and titles can help tell an effective visual story (Source: Wall Street Journal 2015)\n\n\n\n\n\nMatch the visualization to the type of data and the message you‚Äôre trying to convey.\nand 3D charts in most cases because they tend to overcomplicate\nor be difficult to accurately perceive. Below are some uses for various basic charts.",
    "crumbs": [
      "Urban Data Visualization",
      "Data visualization"
    ]
  }
]